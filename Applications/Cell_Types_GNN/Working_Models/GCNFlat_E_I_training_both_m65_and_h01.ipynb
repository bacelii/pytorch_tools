{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Implementation fo DiffPool\n",
    "graph coarsening manner\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "#sys.path.append(\"/meshAfterParty/meshAfterParty\")\n",
    "sys.path.append(\"/datasci_tools/datasci_tools\")\n",
    "sys.path.append(\"/machine_learning_tools/machine_learning_tools/\")\n",
    "sys.path.append(\"/pytorch_tools/pytorch_tools/\")\n",
    "sys.path.append(\"/neuron_morphology_tools/neuron_morphology_tools/\")\n",
    "sys.path.append(\"/meshAfterParty/meshAfterParty/\")\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/.ipynb_checkpoints'),\n",
       " PosixPath('../data/df_cell_type_fine_h01.pbz2'),\n",
       " PosixPath('../data/df_morphometrics_h01.pbz2')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"../data/\")\n",
    "list(data_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasci_tools modules\n",
    "import system_utils as su\n",
    "import pandas_utils as pu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_utils as nu\n",
    "import networkx_utils as xu\n",
    "from tqdm_utils import tqdm\n",
    "\n",
    "#neuron_morphology_tools modules\n",
    "import neuron_nx_io as nxio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import transforms\n",
    "\n",
    "# for the dataset object\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import DenseDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "#pytorch_tools modules\n",
    "import preprocessing_utils as pret\n",
    "import geometric_models as gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Choosing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "with_skeleton = True\n",
    "\n",
    "features_to_delete = [\n",
    "    \"mesh_volume\",\n",
    "    \"apical_label\",\n",
    "    \"basal_label\",\n",
    "]\n",
    "\n",
    "if not with_skeleton:\n",
    "    features_to_delete +=[\n",
    "        \"skeleton_vector_downstream_phi\",      \n",
    "        \"skeleton_vector_downstream_theta\",    \n",
    "        \"skeleton_vector_upstream_phi\",        \n",
    "        \"skeleton_vector_upstream_theta\",  \n",
    "    ]\n",
    "\n",
    "features_to_keep = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading the Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "def load_data(\n",
    "    gnn_task,\n",
    "    data_file,#\"df_cell_type_fine.pbz2\"\n",
    "    data_path = Path(\"./data/m65_full/\"),\n",
    "    data_df = None,\n",
    "    label_name = None,\n",
    "    graph_label = \"cell_type_fine_label\", \n",
    "    dense_adj = False,\n",
    "    directed = False,\n",
    "    features_to_remove = [\n",
    "        \"mesh_volume\",\n",
    "        \"apical_label\",\n",
    "        \"basal_label\",\n",
    "    ],\n",
    "    with_skeleton=True,\n",
    "    device = \"cpu\",\n",
    "    \n",
    "    #for the standardization\n",
    "    df_standardization = None,\n",
    "    \n",
    "    cell_type_map = None,\n",
    "    \n",
    "    processed_data_folder_name = None,\n",
    "    \n",
    "    max_nodes = 300,\n",
    "    \n",
    "    #--------- processing the dataset ----\n",
    "    clean_prior_dataset = False,\n",
    "    data_source = None,\n",
    "    verbose = True,\n",
    "    \n",
    "    return_cell_type_map = False,\n",
    "    \n",
    "    \n",
    "    ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Purpose: Will load the data for processing using the GNN models\n",
    "    \n",
    "    \"\"\"\n",
    "    if with_skeleton:\n",
    "        gnn_task_name = f\"{gnn_task}_with_skeleton\"\n",
    "        features_to_delete = features_to_remove\n",
    "    else:\n",
    "        gnn_task_name = f\"{gnn_task}\"\n",
    "        \n",
    "        features_to_delete = features_to_remove + [\n",
    "        \"skeleton_vector_downstream_phi\",      \n",
    "        \"skeleton_vector_downstream_theta\",    \n",
    "        \"skeleton_vector_upstream_phi\",        \n",
    "        \"skeleton_vector_upstream_theta\",  \n",
    "        ]\n",
    "            \n",
    "    if processed_data_folder_name is None:\n",
    "        if dense_adj:\n",
    "            processed_data_folder = data_path / Path(f\"{gnn_task_name}\")#_processed_dense\")\n",
    "        elif directed:\n",
    "            processed_data_folder = data_path / Path(f\"{gnn_task_name}_directed\")#_processed_dense\")\n",
    "        else:\n",
    "            processed_data_folder = data_path / Path(f\"{gnn_task_name}_no_dense\")#_processed_dense\")\n",
    "    else:\n",
    "        processed_data_folder = data_path / Path(f\"{processed_data_folder_name}\")\n",
    "\n",
    "        \n",
    "    #1) Load the data\n",
    "    if verbose:\n",
    "        print(f\"Starting to load data\")\n",
    "        st = time.time()\n",
    "        \n",
    "    if data_df is None:\n",
    "        data_filepath = Path(data_path) / Path(data_file)\n",
    "        data_df = su.decompress_pickle(data_filepath)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Finished loading data: {time.time() - st}\")\n",
    "    \n",
    "    #2) Getting the means and standard deviations if not already computed\n",
    "    if df_standardization is None:\n",
    "        if verbose:\n",
    "            print(f\"Started calculating normalization\")\n",
    "        all_batch_df = pd.concat([nxio.feature_df_from_gnn_info(\n",
    "            k[0],\n",
    "            return_data_labels_split = False) for k in data_df[gnn_task].to_list()])\n",
    "\n",
    "        if label_name is not None:\n",
    "            all_batch_df = all_batch_df[[k for k in \n",
    "                    all_batch_df.columns if k not in nu.convert_to_array_like(label_name)]]\n",
    "        else:\n",
    "            all_batch_df = all_batch_df\n",
    "\n",
    "        # will use these to normalize the data\n",
    "        col_means = all_batch_df.mean(axis=0).to_numpy()\n",
    "        col_stds = all_batch_df.std(axis=0).to_numpy()\n",
    "        df_standardization = pd.DataFrame(np.array([col_means,col_stds]),\n",
    "             index=[\"norm_mean\",\"norm_std\"],\n",
    "            columns=all_batch_df.columns)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Finished calculating normalization: {time.time() - st}\")\n",
    "        \n",
    "        #max_nodes = np.max(all_batch_df.index.to_numpy()) + 1\n",
    "        \n",
    "#         all_batch_df_norm = pu.normalize_df(all_batch_df,\n",
    "#                 column_means=df_standardization[all_batch_df.columns].loc[\"norm_mean\",:],\n",
    "#                 column_stds =df_standardization[all_batch_df.columns].loc[\"norm_std\",:])\n",
    "    try:\n",
    "        col_means = df_standardization.loc[\"norm_mean\",:].to_numpy()\n",
    "    except:\n",
    "        col_means = df_standardization.iloc[0,:].to_numpy()\n",
    "    \n",
    "    try:\n",
    "        col_stds = df_standardization.loc[\"norm_std\",:].to_numpy()\n",
    "    except:\n",
    "        col_stds = df_standardization.iloc[1,:].to_numpy()\n",
    "\n",
    "    \n",
    "    #3) Creating the Dataclass\n",
    "    if cell_type_map is None:\n",
    "        total_labels,label_counts = np.unique((data_df.query(f\"{graph_label}=={graph_label}\")[\n",
    "        graph_label]).to_numpy(),return_counts = True)\n",
    "        cell_type_map = {k:i+1 for i,k in enumerate(total_labels)}\n",
    "        cell_type_map[None] = 0\n",
    "    \n",
    "    \n",
    "    # ---------- Creating the dataset --------------------\n",
    "    \n",
    "    # --------- Functions for loading custom dataset -----\n",
    "    def pytorch_data_from_gnn_info(\n",
    "        gnn_info,\n",
    "        y = None,\n",
    "        verbose = False,\n",
    "        normalize = True,\n",
    "        features_to_delete=None,\n",
    "        features_to_keep = None,\n",
    "        data_name = None,\n",
    "        data_source = None,\n",
    "        ): \n",
    "        \"\"\"\n",
    "        Purpose: To convert our data format into pytorch Data object\n",
    "\n",
    "        Pseudocode: \n",
    "        1) Create the edgelist (turn into tensor)\n",
    "        2) Get the \n",
    "        \"\"\"\n",
    "        edgelist = torch.tensor(xu.edgelist_from_adjacency_matrix(\n",
    "            array = gnn_info[\"adjacency\"],\n",
    "            verbose = False,\n",
    "        ).T,dtype=torch.long)\n",
    "\n",
    "        x,y_raw = nxio.feature_df_from_gnn_info(\n",
    "            gnn_info,\n",
    "            return_data_labels_split = True)\n",
    "        if y is None:\n",
    "            y = y_raw\n",
    "\n",
    "        if not type(y) == str:\n",
    "            y = None\n",
    "\n",
    "        y_int = np.array(cell_type_map[y] ).reshape(1,-1)\n",
    "\n",
    "        if normalize:\n",
    "            x = (x-col_means)/col_stds\n",
    "\n",
    "        # --- keeping or not keeping sertain features\n",
    "        gnn_features = gnn_info[\"features\"]\n",
    "\n",
    "        keep_idx = np.arange(len(gnn_features))\n",
    "        if features_to_delete is not None:\n",
    "            curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                           if k not in features_to_delete])\n",
    "            keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "            if verbose:\n",
    "                print(f\"keep_idx AFTER DELETE= {keep_idx}\")\n",
    "        if features_to_keep is not None:\n",
    "            curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                           if k in features_to_keep])\n",
    "            keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "            if verbose:\n",
    "                print(f\"keep_idx AFTER KEEP = {keep_idx}\")\n",
    "\n",
    "        x = x[:,keep_idx]\n",
    "\n",
    "        x = torch.tensor(x,dtype=torch.float)\n",
    "        y = torch.tensor(y_int,dtype=torch.long)\n",
    "\n",
    "        if len(y) > 1:\n",
    "            raise Exception(f\"y = {y}\")\n",
    "\n",
    "        if y.shape[0] != 1 or y.shape[1] != 1:\n",
    "            raise Exception(f\"y = {y}\")\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"x.shape = {x.shape},y.shape ={y.shape}\")\n",
    "        \n",
    "        data_dict = dict(x=x,y=y,edge_index=edgelist)\n",
    "        if data_name is not None:\n",
    "            data_dict[\"data_name\"] = data_name\n",
    "            \n",
    "        if data_source is not None:\n",
    "            data_dict[\"data_source\"] = data_source\n",
    "            \n",
    "        \n",
    "        data = Data(**data_dict)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    class CellTypeDataset(InMemoryDataset):\n",
    "        def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "            super().__init__(root, transform, pre_transform, pre_filter)\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "        @property\n",
    "        def raw_file_names(self):\n",
    "            #return ['some_file_1', 'some_file_2', ...]\n",
    "            return [str(data_filepath.absolute())]\n",
    "\n",
    "        @property\n",
    "        def processed_file_names(self):\n",
    "            return ['data.pt']\n",
    "\n",
    "        # def download(self):\n",
    "        #     # Download to `self.raw_dir`.\n",
    "        #     download_url(url, self.raw_dir)\n",
    "        #     ...\n",
    "\n",
    "        def process(self):\n",
    "            # Read data into huge `Data` list.\n",
    "            #data_list = [...]\n",
    "\n",
    "    #         if data_df is None:\n",
    "    #             data_df = su.decompress_pickle(self.raw_file_names[0])\n",
    "\n",
    "\n",
    "            data_list = []\n",
    "            for k,y,segment_id,split_index in tqdm(zip(\n",
    "                data_df[gnn_task].to_list(),\n",
    "                data_df[graph_label].to_list(),\n",
    "                data_df[\"segment_id\"],\n",
    "                data_df[\"split_index\"])):\n",
    "                \n",
    "                if len(k) > 0:\n",
    "                    data_list.append(pytorch_data_from_gnn_info(\n",
    "                        k[0],\n",
    "                        y=y,\n",
    "                        features_to_delete=features_to_delete,\n",
    "                        features_to_keep = features_to_keep,\n",
    "                        data_name = f\"{segment_id}_{split_index}\",\n",
    "                        data_source = data_source,\n",
    "                        verbose = False))\n",
    "\n",
    "            if self.pre_filter is not None:\n",
    "                data_list_final = []\n",
    "                for data in data_list:\n",
    "                    try:\n",
    "                        if self.pre_filter(data):\n",
    "                            data_list_final.append(data)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                data_list = data_list_final\n",
    "\n",
    "            for j,d in enumerate(data_list):\n",
    "                if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                    raise Exception(f\"{j}\")\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data_list_final = []\n",
    "                for j,data in enumerate(data_list):\n",
    "                    try:\n",
    "                        curr_t = self.pre_transform(data)\n",
    "                        if curr_t.y.shape[0] != 1 or curr_t.y.shape[1] != 1:\n",
    "                            raise Exception(f\"{j}, data = {curr_t}\")\n",
    "                        data_list_final.append(curr_t)\n",
    "                    except:\n",
    "                        continue\n",
    "                data_list = data_list_final\n",
    "\n",
    "            for j,d in enumerate(data_list):\n",
    "                if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                    raise Exception(f\"{j}, data = {d}\")\n",
    "\n",
    "            data, slices = self.collate(data_list)\n",
    "            torch.save((data, slices), self.processed_paths[0])\n",
    "            \n",
    "    # --- creating the folder for the dataset --\n",
    "    if clean_prior_dataset:\n",
    "        try:\n",
    "            su.rm_dir(processed_data_folder)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    processed_data_folder.mkdir(exist_ok = True)\n",
    "    \n",
    "    \n",
    "    # a) Processing Filteres\n",
    "    class MyFilter(object):\n",
    "        def __call__(self, data):\n",
    "            return data.num_nodes <= max_nodes\n",
    "\n",
    "    if dense_adj:\n",
    "        #gets the maximum number of nodes in any of the graphs\n",
    "        transform_list = [\n",
    "            transforms.ToUndirected(),\n",
    "            T.ToDense(max_nodes),\n",
    "            #transforms.NormalizeFeatures(),\n",
    "            ]\n",
    "        re_filter = MyFilter()\n",
    "    elif directed:\n",
    "        transform_list = []\n",
    "        pre_filter = None\n",
    "    else:\n",
    "        transform_list = [\n",
    "            transforms.ToUndirected(),]\n",
    "\n",
    "        pre_filter = None\n",
    "\n",
    "\n",
    "    transform_norm = transforms.Compose(transform_list)\n",
    "    \n",
    "    \n",
    "    # b) Creating the Dataset\n",
    "    dataset = CellTypeDataset(\n",
    "            processed_data_folder.absolute(),\n",
    "            pre_transform = transform_norm,\n",
    "            pre_filter = pre_filter,\n",
    "            )\n",
    "    \n",
    "    if return_cell_type_map:\n",
    "        return dataset,cell_type_map\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Running the preprocessing --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardization = None#pu.csv_to_df(\"../data/cell_type_normalization_df.csv\")\n",
    "cell_type_map = None,#su.decompress_pickle(\"../data/cell_type_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_task = \"cell_type_fine\"\n",
    "label_name = None\n",
    "graph_label = \"cell_type_fine_label\"\n",
    "data_file = \"df_cell_type_fine.pbz2\"\n",
    "dense_adj = False\n",
    "directed = False\n",
    "\n",
    "data_path = Path(\"../data/m65_full/\")\n",
    "data_source = \"m65\"\n",
    "processed_data_folder_name = f\"{gnn_task}_{data_source}\"\n",
    "\n",
    "m65_dataset = load_data(\n",
    "    gnn_task = gnn_task,\n",
    "    data_file = data_file,\n",
    "    data_path = data_path,\n",
    "    label_name = label_name,\n",
    "    graph_label = graph_label, \n",
    "    dense_adj = False,\n",
    "    directed = False,\n",
    "    features_to_remove = [\n",
    "        \"mesh_volume\",\n",
    "        \"apical_label\",\n",
    "        \"basal_label\",\n",
    "    ],\n",
    "    with_skeleton=True,\n",
    "    device = \"cpu\",\n",
    "    \n",
    "    #for the standardization\n",
    "    df_standardization = df_standardization,\n",
    "    cell_type_map = cell_type_map,\n",
    "    \n",
    "    processed_data_folder_name = processed_data_folder_name,\n",
    "    \n",
    "    max_nodes = 300,\n",
    "    clean_prior_dataset = True,  \n",
    "    data_source = data_source,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m65_dataset[1000].x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- a) creating the h01 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cell_type_map.update({\"Unsure E\":32,\"Unsure I\":33})\n",
    "cell_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= Path(\"../data/h01_full\")\n",
    "gnn_task = \"cell_type_fine\"\n",
    "label_name = None\n",
    "graph_label = \"cell_type_fine_label\"\n",
    "data_file = \"df_cell_type_fine_h01.pbz2\"\n",
    "dense_adj = False\n",
    "directed = False\n",
    "data_source = \"h01\"\n",
    "\n",
    "\n",
    "processed_data_folder_name = f\"{gnn_task}_h01\"\n",
    "\n",
    "df_standardization = pu.csv_to_df(\"../cell_type_normalization_df.csv\")\n",
    "\n",
    "\n",
    "h01_dataset = load_data(\n",
    "    gnn_task = gnn_task,\n",
    "    data_file = data_file,\n",
    "    data_path = data_path,\n",
    "    graph_label = graph_label, \n",
    "    dense_adj = False,\n",
    "    directed = False,\n",
    "    \n",
    "    #for the standardization\n",
    "    df_standardization = df_standardization,\n",
    "    cell_type_map=cell_type_map,\n",
    "    \n",
    "    processed_data_folder_name = processed_data_folder_name,\n",
    "    clean_prior_dataset = True,\n",
    "    \n",
    "    data_source = data_source,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num_node_features = m65_dataset.num_node_features\n",
    "dataset_num_classes = m65_dataset.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /pytorch_tools/Applications/Cell_Types_GNN/tensorboard/GCNFlat --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geometric_models as gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils as gu\n",
    "architecture_kwargs_global = dict(\n",
    "    n_hidden_channels = 32, \n",
    "    global_pool_type=\"mean\",\n",
    "    n_layers = 2\n",
    ")\n",
    "\n",
    "architecture_kwargs_curr = dict(\n",
    "    n_hidden_channels = 64,\n",
    "    global_pool_type = \"mean\",\n",
    "    n_layers = 2)\n",
    "\n",
    "architecture_kwargs = gu.merge_dicts([architecture_kwargs_global,architecture_kwargs_curr])\n",
    "architecture_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GCNFlat\"\n",
    "checkpoint_dir = Path(\"../model_checkpoints\")\n",
    "checkpoint_dir = checkpoint_dir / Path(f\"{model_name}\")\n",
    "\n",
    "winning_name = (f\"{model_name}_\" + \"_\".join([f\"{k}_{v}\" for k,v in architecture_kwargs.items()]) +\n",
    "                \"_lr_0.01_with_skeleton_True\")\n",
    "\n",
    "\n",
    "epoch = 95\n",
    "winning_dir = checkpoint_dir / Path(f\"{winning_name}_checkpoints\") \n",
    "winning_filepath = winning_dir / Path(f\"{winning_name}_epoch_{epoch}\")\n",
    "winning_filepath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(gm,model_name)(\n",
    "    dataset_num_node_features=dataset_num_node_features,\n",
    "    dataset_num_classes=dataset_num_classes,\n",
    "    **architecture_kwargs,\n",
    "    #use_bn=False\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(winning_filepath)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[list(model.parameters())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = m65_dataset + h01_dataset\n",
    "print(f\"m65_dataset = {len(m65_dataset)}\")\n",
    "print(f\"h01_dataset = {len(h01_dataset)}\")\n",
    "print(f\"dataset = {len(dataset)}\")\n",
    "# mask_m65 = np.zeros(len(dataset))\n",
    "# mask_m65[:len(m65_dataset)] = 1\n",
    "\n",
    "# mask_h01 = np.zeros(len(dataset))\n",
    "# mask_h01[len(m65_dataset)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "all_data_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle = False)\n",
    "\n",
    "all_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model.eval()\n",
    "embeddings = []\n",
    "labels = []\n",
    "data_names = []\n",
    "data_sources = []\n",
    "for data in tqdm(all_data_loader):#train_loader:  # Iterate in batches over the training dataset.\n",
    "    data = data.to(device)\n",
    "    if model_name == \"DiffPool\":\n",
    "            out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "            #y_true = data.y.reshape(-1,3)\n",
    "    elif model_name == \"TreeLSTM\":\n",
    "        n = data.x.shape[0]\n",
    "        h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        out = model(\n",
    "            data,\n",
    "            h = h,\n",
    "            c = c,\n",
    "            embeddings = data.x\n",
    "            )\n",
    "    else:\n",
    "        out = model(data)\n",
    "\n",
    "    out_array = out.detach().cpu().numpy()\n",
    "    out_labels = data.y.numpy().reshape(-1)\n",
    "    #print(f\"out_array.shape = {out_array.shape}, out_labels.shape = {out_labels.shape}\")\n",
    "    \n",
    "#     if out_array.shape[0] != out_labels.shape[0]:\n",
    "#         raise Exception(\"\")\n",
    "    \n",
    "    embeddings.append(out_array)\n",
    "    labels.append(out_labels)\n",
    "    data_names.append(data.data_name)\n",
    "    data_sources.append(data.data_source)\n",
    "    \n",
    "    \n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.hstack(labels)\n",
    "data_names = np.hstack(data_names)\n",
    "data_sources = np.hstack(data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.DataFrame(embeddings)\n",
    "embedding_df[\"cell_type\"] = labels\n",
    "embedding_df[\"cell_type_predicted\"] = np.argmax(embeddings,axis=1)\n",
    "embedding_df[\"data_name\"] = data_names\n",
    "embedding_df[\"data_source\"] = data_sources\n",
    "\n",
    "import general_utils as gu\n",
    "decoder_map = dict([(v,k) if k is not None else (v,\"Unknown\") for k,v in cell_type_map.items()])\n",
    "\n",
    "import pandas_utils as pu\n",
    "embedding_df[\"cell_type\"] = pu.new_column_from_dict_mapping(\n",
    "    embedding_df,\n",
    "    decoder_map,\n",
    "    column_name = \"cell_type\"\n",
    ")\n",
    "\n",
    "def e_i_label(row):\n",
    "    ct = row[\"cell_type\"]\n",
    "    if ct is None:\n",
    "        return ct\n",
    "    \n",
    "    return ctu.e_i_label_from_cell_type_fine(ct)\n",
    "\n",
    "embedding_df[\"e_i\"] = pu.new_column_from_row_function(\n",
    "    embedding_df,\n",
    "    e_i_label,\n",
    ")\n",
    "\n",
    "embedding_df[\"cell_type_predicted\"] = pu.new_column_from_dict_mapping(\n",
    "    embedding_df,\n",
    "    decoder_map,\n",
    "    column_name = \"cell_type_predicted\"\n",
    ")\n",
    "\n",
    "\n",
    "def e_i_label_predicted(row):\n",
    "    ct = row[\"cell_type_predicted\"]\n",
    "    if ct is None:\n",
    "        return ct\n",
    "    \n",
    "    return ctu.e_i_label_from_cell_type_fine(ct)\n",
    "\n",
    "embedding_df[\"e_i_predicted\"] = pu.new_column_from_row_function(\n",
    "    embedding_df,\n",
    "    e_i_label_predicted,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Doing the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string_utils as stru\n",
    "embed_cols = [k for k in embedding_df.columns if \"int\" in str(type(k))]#stru.is_int(k)]\n",
    "np.array(embed_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Plotting the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the mask of m65 vs h01\n",
    "extract the data the data names\n",
    "Collect the X value and the y values\n",
    "\n",
    "-> there might be certain masks want to apply\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datajoint_utils as du\n",
    "import cell_type_utils as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import visualizations_ml as vml\n",
    "n_components = 3\n",
    "import dimensionality_reduction_ml as dru\n",
    "import pandas_ml as pdml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df_known = embedding_df.query(\"(cell_type != 'Unknown') and (cell_type != 'Unsure')\").reset_index(drop=True)\n",
    "embedding_df_known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) UMAP on embedding (0.5 min dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"UMAP\"\n",
    "kwargs = dict(n_components =2,min_dist = 0.5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_before_trans = np.ones(len(X_trans)).astype(\"bool\")\n",
    "data_source = \"h01\"\n",
    "\n",
    "if data_source is not None:\n",
    "    df_input = embedding_df_known.query(f\"data_source == '{data_source}'\").reset_index(drop=True)#.#.query(\"\")\n",
    "else:\n",
    "    df_input = embedding_df_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = dru.dimensionality_reduction_by_method(\n",
    "        method=method,\n",
    "        X = df_input[embed_cols].to_numpy().astype(\"float\"),\n",
    "        **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_cols = [f\"{method}_{k}\" for k in range(X_trans.shape[1])]\n",
    "df_input = pd.concat([df_input,pd.DataFrame(X_trans,columns = trans_cols)],axis = 1)\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_input\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vml.plot_df_scatter_classification(\n",
    "                X = df_plot[trans_cols].to_numpy().astype(\"float\"),\n",
    "                y = df_plot[\"cell_type\"].to_numpy(),\n",
    "                target_to_color = ctu.cell_type_fine_color_map,\n",
    "                ndim = len(trans_cols),\n",
    "                title=method,\n",
    "                use_labels_as_text_to_plot=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling the Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataInterfaceH01 import data_interface as hdju_h01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"h01\"\n",
    "embedding_df_known.query(\"data_source == 'h01'\").query(\"cell_type_predicted=='Martinotti'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df_known.query(\"data_source == 'h01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding where disagrees\n",
    "embedding_df_known.query(\"data_source == 'h01'\").query(\"e_i != e_i_predicted\").query(\"cell_type=='Unsure E'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import trimesh_utils as tu\n",
    "import human_utils as hu\n",
    "tu = reload(tu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Wrong ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Excitatory's that are wrong are mostly the aspiny\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdju_h01.plot_axon_dendrite_skeletons(\"46836446896_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the 5 closest cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"m65\"\n",
    "data_source_df = embedding_df_known.query(f\"data_source == '{data_source}'\").query(\"cell_type=='BC'\")\n",
    "data_source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name = \"864691134884769786_0\" # IT cell\n",
    "\n",
    "X = data_source_df[embed_cols]\n",
    "data_point = data_source_df.query(f\"data_name=='{node_name}'\")[embed_cols].to_numpy()\n",
    "data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dataInterfaceMinnie65 import data_interface as hdju_m65\n",
    "import gnn_embedding_utils as gnneu\n",
    "\n",
    "node_name = \"864691136925825354_0\"\n",
    "node_name = \"864691135272313361_0\"\n",
    "\n",
    "gnneu.closest_neighbors_in_embedding_df(\n",
    "    df = data_source_df,\n",
    "    data_name = node_name,\n",
    "    n_neighbors = 5,\n",
    "    verbose = True,\n",
    "    plot = True,\n",
    "    plotting_func = hdju_m65.plot_axon_dendrite_skeletons,\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
