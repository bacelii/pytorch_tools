{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Implementation fo DiffPool\n",
    "graph coarsening manner\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "#sys.path.append(\"/meshAfterParty/meshAfterParty\")\n",
    "sys.path.append(\"/python_tools/python_tools\")\n",
    "sys.path.append(\"/machine_learning_tools/machine_learning_tools/\")\n",
    "sys.path.append(\"/pytorch_tools/pytorch_tools/\")\n",
    "sys.path.append(\"/neuron_morphology_tools/neuron_morphology_tools/\")\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/m65_full/df_morphometrics.pbz2'),\n",
       " PosixPath('data/m65_full/cell_type_fine_with_skeleton_no_dense'),\n",
       " PosixPath('data/m65_full/df_cell_type_fine.pbz2')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"./data/m65_full/\")\n",
    "list(data_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python_tools modules\n",
    "import system_utils as su\n",
    "import pandas_utils as pu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_utils as nu\n",
    "import networkx_utils as xu\n",
    "from tqdm_utils import tqdm\n",
    "\n",
    "#neuron_morphology_tools modules\n",
    "import neuron_nx_io as nxio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import transforms\n",
    "\n",
    "# for the dataset object\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import DenseDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_tools modules\n",
    "import preprocessing_utils as pret\n",
    "import geometric_models as gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Choosing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "with_skeleton = True\n",
    "\n",
    "features_to_delete = [\n",
    "    \"mesh_volume\",\n",
    "    \"apical_label\",\n",
    "    \"basal_label\",\n",
    "]\n",
    "\n",
    "if not with_skeleton:\n",
    "    features_to_delete +=[\n",
    "        \"skeleton_vector_downstream_phi\",      \n",
    "        \"skeleton_vector_downstream_theta\",    \n",
    "        \"skeleton_vector_upstream_phi\",        \n",
    "        \"skeleton_vector_upstream_theta\",  \n",
    "    ]\n",
    "\n",
    "features_to_keep = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading the Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_task = \"cell_type_fine\"\n",
    "label_name = None\n",
    "graph_label = \"cell_type_fine_label\"\n",
    "data_file = \"df_cell_type_fine.pbz2\"\n",
    "dense_adj = getattr(model_class,\"dense_adj\",False)\n",
    "directed = getattr(model_class,\"directed\",False)\n",
    "print(f\"dense_adj= {dense_adj}, directed = {directed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>split_index</th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>external_layer</th>\n",
       "      <th>external_visual_area</th>\n",
       "      <th>cell_type_fine</th>\n",
       "      <th>cell_type_fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691134277239760</td>\n",
       "      <td>0</td>\n",
       "      <td>89719</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864691134339067925</td>\n",
       "      <td>0</td>\n",
       "      <td>624899</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>AL</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864691134366116139</td>\n",
       "      <td>0</td>\n",
       "      <td>476756</td>\n",
       "      <td>WHITE_MATTER</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_6'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>864691134378215335</td>\n",
       "      <td>0</td>\n",
       "      <td>3799</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864691134527727930</td>\n",
       "      <td>0</td>\n",
       "      <td>631380</td>\n",
       "      <td>WHITE_MATTER</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_0'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60448</th>\n",
       "      <td>864691137197334593</td>\n",
       "      <td>0</td>\n",
       "      <td>376218</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_5',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60449</th>\n",
       "      <td>864691137197344065</td>\n",
       "      <td>0</td>\n",
       "      <td>191436</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_4', 'L0_6',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60450</th>\n",
       "      <td>864691137197345345</td>\n",
       "      <td>0</td>\n",
       "      <td>584463</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_13', 'L4_5', 'L0_8', 'L2_5'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60451</th>\n",
       "      <td>864691137197353281</td>\n",
       "      <td>0</td>\n",
       "      <td>591241</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L4_6', 'L1_10', 'L3_4', 'L0_10...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60452</th>\n",
       "      <td>864691137197364801</td>\n",
       "      <td>0</td>\n",
       "      <td>488097</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_9', 'L0_8', 'L0_10', 'L0_6'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60453 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               segment_id  split_index  nucleus_id external_layer  \\\n",
       "0      864691134277239760            0       89719        LAYER_6   \n",
       "1      864691134339067925            0      624899        LAYER_6   \n",
       "2      864691134366116139            0      476756   WHITE_MATTER   \n",
       "3      864691134378215335            0        3799      LAYER_2/3   \n",
       "4      864691134527727930            0      631380   WHITE_MATTER   \n",
       "...                   ...          ...         ...            ...   \n",
       "60448  864691137197334593            0      376218        LAYER_6   \n",
       "60449  864691137197344065            0      191436      LAYER_2/3   \n",
       "60450  864691137197345345            0      584463      LAYER_2/3   \n",
       "60451  864691137197353281            0      591241        LAYER_5   \n",
       "60452  864691137197364801            0      488097      LAYER_2/3   \n",
       "\n",
       "      external_visual_area                                     cell_type_fine  \\\n",
       "0                       V1  [{'nodelist': ['L0_0'], 'features': ['mesh_vol...   \n",
       "1                       AL  [{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...   \n",
       "2                       RL  [{'nodelist': ['L0_6'], 'features': ['mesh_vol...   \n",
       "3                       V1  [{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...   \n",
       "4                       RL  [{'nodelist': ['L0_0'], 'features': ['mesh_vol...   \n",
       "...                    ...                                                ...   \n",
       "60448                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_5',...   \n",
       "60449                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_4', 'L0_6',...   \n",
       "60450                   RL  [{'nodelist': ['L0_13', 'L4_5', 'L0_8', 'L2_5'...   \n",
       "60451                   RL  [{'nodelist': ['L4_6', 'L1_10', 'L3_4', 'L0_10...   \n",
       "60452                   RL  [{'nodelist': ['L0_9', 'L0_8', 'L0_10', 'L0_6'...   \n",
       "\n",
       "      cell_type_fine_label  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "...                    ...  \n",
       "60448                  NaN  \n",
       "60449                  NaN  \n",
       "60450                  NaN  \n",
       "60451                  NaN  \n",
       "60452                  NaN  \n",
       "\n",
       "[60453 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filepath = Path(data_path) / Path(data_file)\n",
    "\n",
    "data_df = su.decompress_pickle(data_filepath)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote the cell_type_fine is the column\\nthat has all of the graph data stored\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note the cell_type_fine is the column\n",
    "that has all of the graph data stored\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>split_index</th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>external_layer</th>\n",
       "      <th>external_visual_area</th>\n",
       "      <th>cell_type_fine</th>\n",
       "      <th>cell_type_fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>864691134884748026</td>\n",
       "      <td>0</td>\n",
       "      <td>366181</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_10', 'L0_11', 'L0_12', 'L0_...</td>\n",
       "      <td>4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>864691134884761338</td>\n",
       "      <td>0</td>\n",
       "      <td>458241</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>23P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>864691134884769786</td>\n",
       "      <td>0</td>\n",
       "      <td>592718</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_18', 'L0_11', 'L0_17', 'L0_...</td>\n",
       "      <td>5P_IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>864691134884879610</td>\n",
       "      <td>0</td>\n",
       "      <td>304873</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L2_4', 'L1_1', 'L0_3', 'L1_6',...</td>\n",
       "      <td>IT_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>864691134884945146</td>\n",
       "      <td>0</td>\n",
       "      <td>63499</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>23P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60416</th>\n",
       "      <td>864691137197239105</td>\n",
       "      <td>0</td>\n",
       "      <td>262000</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60417</th>\n",
       "      <td>864691137197241665</td>\n",
       "      <td>0</td>\n",
       "      <td>308938</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L1_4', 'L0_18', 'L4_1', 'L0_16...</td>\n",
       "      <td>Unsure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60433</th>\n",
       "      <td>864691137197306177</td>\n",
       "      <td>0</td>\n",
       "      <td>304611</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_3', 'L0_6', 'L0_0',...</td>\n",
       "      <td>5P_NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60442</th>\n",
       "      <td>864691137197321281</td>\n",
       "      <td>0</td>\n",
       "      <td>434601</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_3', 'L3_3', 'L2_2', 'L2_3',...</td>\n",
       "      <td>6P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60445</th>\n",
       "      <td>864691137197329985</td>\n",
       "      <td>0</td>\n",
       "      <td>260468</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_3', 'L1_79', 'L1_82', 'L0_1...</td>\n",
       "      <td>BPC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3338 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               segment_id  split_index  nucleus_id external_layer  \\\n",
       "115    864691134884748026            0      366181        LAYER_5   \n",
       "147    864691134884761338            0      458241        LAYER_4   \n",
       "170    864691134884769786            0      592718        LAYER_5   \n",
       "205    864691134884879610            0      304873        LAYER_5   \n",
       "213    864691134884945146            0       63499      LAYER_2/3   \n",
       "...                   ...          ...         ...            ...   \n",
       "60416  864691137197239105            0      262000        LAYER_4   \n",
       "60417  864691137197241665            0      308938        LAYER_6   \n",
       "60433  864691137197306177            0      304611        LAYER_5   \n",
       "60442  864691137197321281            0      434601        LAYER_5   \n",
       "60445  864691137197329985            0      260468        LAYER_4   \n",
       "\n",
       "      external_visual_area                                     cell_type_fine  \\\n",
       "115                     V1  [{'nodelist': ['L0_10', 'L0_11', 'L0_12', 'L0_...   \n",
       "147                     RL  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "170                     RL  [{'nodelist': ['L0_18', 'L0_11', 'L0_17', 'L0_...   \n",
       "205                     V1  [{'nodelist': ['L2_4', 'L1_1', 'L0_3', 'L1_6',...   \n",
       "213                     V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "...                    ...                                                ...   \n",
       "60416                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "60417                   V1  [{'nodelist': ['L1_4', 'L0_18', 'L4_1', 'L0_16...   \n",
       "60433                   V1  [{'nodelist': ['L0_1', 'L0_3', 'L0_6', 'L0_0',...   \n",
       "60442                   RL  [{'nodelist': ['L0_3', 'L3_3', 'L2_2', 'L2_3',...   \n",
       "60445                   V1  [{'nodelist': ['L0_3', 'L1_79', 'L1_82', 'L0_1...   \n",
       "\n",
       "      cell_type_fine_label  \n",
       "115                     4P  \n",
       "147                    23P  \n",
       "170                  5P_IT  \n",
       "205               IT_short  \n",
       "213                    23P  \n",
       "...                    ...  \n",
       "60416                   4P  \n",
       "60417               Unsure  \n",
       "60433                5P_NP  \n",
       "60442                   6P  \n",
       "60445                  BPC  \n",
       "\n",
       "[3338 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.query(\"cell_type_fine_label == cell_type_fine_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodelist': array(['L0_1', 'L0_0', 'L0_2'], dtype=object),\n",
       " 'features': array(['mesh_volume', 'n_spines', 'n_synapses_head', 'n_synapses_neck',\n",
       "        'n_synapses_post', 'n_synapses_pre', 'skeletal_length',\n",
       "        'total_spine_volume', 'width_upstream', 'width_downstream',\n",
       "        'apical_label', 'basal_label', 'skeleton_vector_downstream_phi',\n",
       "        'skeleton_vector_downstream_theta', 'skeleton_vector_upstream_phi',\n",
       "        'skeleton_vector_upstream_theta', 'width_no_spine'], dtype=object),\n",
       " 'adjacency': array([[0, 1, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]),\n",
       " 'feature_matrix': array([[ 3.71170715e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.43345470e+03,  0.00000000e+00,  5.91837642e+01,\n",
       "          5.91837642e+01,  0.00000000e+00,  1.00000000e+00,\n",
       "         -6.22637047e-01,  2.00297982e+00, -6.22637047e-01,\n",
       "          2.00297982e+00,  5.91837642e+01],\n",
       "        [ 2.99289539e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          6.76722467e+03,  0.00000000e+00,  4.71136497e+02,\n",
       "          8.69454592e+02,  0.00000000e+00,  1.00000000e+00,\n",
       "          3.01881171e+00,  1.61006762e+00, -2.79826200e+00,\n",
       "          2.06957198e+00,  7.32983276e+02],\n",
       "        [ 8.07215086e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          4.79074407e+03,  0.00000000e+00,  1.97902300e+02,\n",
       "          4.48326427e+02,  0.00000000e+00,  1.00000000e+00,\n",
       "          1.19580679e+00,  7.15696534e-01,  1.01784622e+00,\n",
       "          7.42017670e-01,  3.82383945e+02]]),\n",
       " 'label_name': None,\n",
       " 'graph_label': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = data_df[[\"cell_type_fine\"]].iloc[1].to_list()[0][0]\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating the Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- a) Getting Means and Std Dev for Normalization --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mesh_volume                         1.0\n",
       "n_spines                            1.0\n",
       "n_synapses_head                     1.0\n",
       "n_synapses_neck                     1.0\n",
       "n_synapses_post                     1.0\n",
       "n_synapses_pre                      1.0\n",
       "skeletal_length                     1.0\n",
       "total_spine_volume                  1.0\n",
       "width_upstream                      1.0\n",
       "width_downstream                    1.0\n",
       "apical_label                        1.0\n",
       "basal_label                         1.0\n",
       "skeleton_vector_downstream_phi      1.0\n",
       "skeleton_vector_downstream_theta    1.0\n",
       "skeleton_vector_upstream_phi        1.0\n",
       "skeleton_vector_upstream_theta      1.0\n",
       "width_no_spine                      1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_batch_df = pd.concat([nxio.feature_df_from_gnn_info(\n",
    "    k[0],\n",
    "    return_data_labels_split = False) for k in data_df[gnn_task].to_list()])\n",
    "\n",
    "if label_name is not None:\n",
    "    all_batch_df = all_batch_df[[k for k in \n",
    "            all_batch_df.columns if k not in nu.convert_to_array_like(label_name)]]\n",
    "else:\n",
    "    all_batch_df = all_batch_df\n",
    "    \n",
    "# will use these to normalize the data\n",
    "col_means = all_batch_df.mean(axis=0).to_numpy()\n",
    "col_stds = all_batch_df.std(axis=0).to_numpy()\n",
    "\n",
    "all_batch_df_norm = pu.normalize_df(all_batch_df,column_means=col_means,\n",
    "                                 column_stds = col_stds)\n",
    "all_batch_df_norm.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- b) Creating the Dataset Class --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1P': 1,\n",
       " '23P': 2,\n",
       " '4P': 3,\n",
       " '5P_IT': 4,\n",
       " '5P_NP': 5,\n",
       " '5P_PT': 6,\n",
       " '6CT': 7,\n",
       " '6P': 8,\n",
       " '6P_CT': 9,\n",
       " '6P_IT': 10,\n",
       " '6P_U': 11,\n",
       " 'BC': 12,\n",
       " 'BPC': 13,\n",
       " 'I targeting non_bpc': 14,\n",
       " 'IT_big_tuft': 15,\n",
       " 'IT_short': 16,\n",
       " 'IT_small_tuft': 17,\n",
       " 'Martinotti': 18,\n",
       " 'NGC': 19,\n",
       " 'Pvalb': 20,\n",
       " 'SST': 21,\n",
       " 'Unsure': 22,\n",
       " 'VIP': 23,\n",
       " 'WM_P': 24,\n",
       " 'cb1 basket': 25,\n",
       " 'chandelier': 26,\n",
       " 'l1vip': 27,\n",
       " 'ndnf+npy_': 28,\n",
       " 'ngfc': 29,\n",
       " 'prox targeting': 30,\n",
       " 'small basket': 31,\n",
       " None: 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- mapping of the labels to integers --\n",
    "total_labels,label_counts = np.unique((data_df.query(f\"{graph_label}=={graph_label}\")[\n",
    "    graph_label]).to_numpy(),return_counts = True)\n",
    "cell_type_map = {k:i+1 for i,k in enumerate(total_labels)}\n",
    "cell_type_map[None] = 0\n",
    "cell_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 0.25, 0.3 , 0.5 , 1.  , 0.8 , 1.  , 0.8 , 1.  , 0.8 ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cell_type_map\n",
    "cell_type_fine_classifier_weights = {\n",
    "'23P': 0.25,#1294\n",
    "'4P': 0.3,#890\n",
    "'5P_IT': 0.5,#465\n",
    "'6P': 0.8,#342\n",
    "'6P_IT': 0.8,#263\n",
    "'5P_PT': 0.8,#224\n",
    "}\n",
    "\n",
    "\n",
    "class_idx = np.array(list(cell_type_map.values()) )\n",
    "class_labels = np.array(list(cell_type_map.keys()) )\n",
    "weights = np.array([cell_type_fine_classifier_weights.get(k,1) for k in class_labels])\n",
    "weights = weights[np.argsort(class_idx)]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_data_from_gnn_info(\n",
    "    gnn_info,\n",
    "    y = None,\n",
    "    verbose = False,\n",
    "    normalize = True,\n",
    "    features_to_delete=None,\n",
    "    features_to_keep = None\n",
    "    ): \n",
    "    \"\"\"\n",
    "    Purpose: To convert our data format into pytorch Data object\n",
    "\n",
    "    Pseudocode: \n",
    "    1) Create the edgelist (turn into tensor)\n",
    "    2) Get the \n",
    "    \"\"\"\n",
    "    edgelist = torch.tensor(xu.edgelist_from_adjacency_matrix(\n",
    "        array = gnn_info[\"adjacency\"],\n",
    "        verbose = False,\n",
    "    ).T,dtype=torch.long)\n",
    "\n",
    "    x,y_raw = nxio.feature_df_from_gnn_info(\n",
    "        gnn_info,\n",
    "        return_data_labels_split = True)\n",
    "    if y is None:\n",
    "        y = y_raw\n",
    "        \n",
    "    if not type(y) == str:\n",
    "        y = None\n",
    "        \n",
    "    y_int = np.array(cell_type_map[y] ).reshape(1,-1)\n",
    "    \n",
    "    if normalize:\n",
    "        x = (x-col_means)/col_stds\n",
    "    \n",
    "    # --- keeping or not keeping sertain features\n",
    "    gnn_features = gnn_info[\"features\"]\n",
    "\n",
    "    keep_idx = np.arange(len(gnn_features))\n",
    "    if features_to_delete is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k not in features_to_delete])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER DELETE= {keep_idx}\")\n",
    "    if features_to_keep is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k in features_to_keep])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER KEEP = {keep_idx}\")\n",
    "\n",
    "    x = x[:,keep_idx]\n",
    "\n",
    "    x = torch.tensor(x,dtype=torch.float)\n",
    "    y = torch.tensor(y_int,dtype=torch.long)\n",
    "    \n",
    "    if len(y) > 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "        \n",
    "    if y.shape[0] != 1 or y.shape[1] != 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"x.shape = {x.shape},y.shape ={y.shape}\")\n",
    "\n",
    "    data = Data(x=x,y=y,edge_index=edgelist)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellTypeDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        #return ['some_file_1', 'some_file_2', ...]\n",
    "        return [str(data_filepath.absolute())]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # def download(self):\n",
    "    #     # Download to `self.raw_dir`.\n",
    "    #     download_url(url, self.raw_dir)\n",
    "    #     ...\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        #data_list = [...]\n",
    "\n",
    "#         if data_df is None:\n",
    "#             data_df = su.decompress_pickle(self.raw_file_names[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        data_list = []\n",
    "        for k,y in tqdm(zip(\n",
    "            data_df[gnn_task].to_list(),\n",
    "            data_df[graph_label].to_list())):\n",
    "            \n",
    "            data_list.append(pytorch_data_from_gnn_info(\n",
    "                k[0],\n",
    "                y=y,\n",
    "                features_to_delete=features_to_delete,\n",
    "                features_to_keep = features_to_keep,\n",
    "                verbose = False))\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list_final = []\n",
    "            for data in data_list:\n",
    "                try:\n",
    "                    if self.pre_filter(data):\n",
    "                        data_list_final.append(data)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}\")\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list_final = []\n",
    "            for j,data in enumerate(data_list):\n",
    "                try:\n",
    "                    curr_t = self.pre_transform(data)\n",
    "                    if curr_t.y.shape[0] != 1 or curr_t.y.shape[1] != 1:\n",
    "                        raise Exception(f\"{j}, data = {curr_t}\")\n",
    "                    data_list_final.append(curr_t)\n",
    "                except:\n",
    "                    continue\n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}, data = {d}\")\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_skeleton:\n",
    "    gnn_task_name = f\"{gnn_task}_with_skeleton\"\n",
    "else:\n",
    "    gnn_task_name = f\"{gnn_task}\"\n",
    "\n",
    "if dense_adj:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}\")#_processed_dense\")\n",
    "elif directed:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_directed\")#_processed_dense\")\n",
    "else:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_no_dense\")#_processed_dense\")\n",
    "    \n",
    "# try:\n",
    "#     su.rm_dir(processed_data_folder)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "processed_data_folder.mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes = np.max(all_batch_df_norm.index.to_numpy()) + 1\n",
    "\n",
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return data.num_nodes <= max_nodes\n",
    "    \n",
    "if dense_adj:\n",
    "    #gets the maximum number of nodes in any of the graphs\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "        T.ToDense(max_nodes),\n",
    "        #transforms.NormalizeFeatures(),\n",
    "    ]\n",
    "    pre_filter = MyFilter()\n",
    "elif directed:\n",
    "    transform_list = []\n",
    "    pre_filter = None\n",
    "else:\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "    ]\n",
    "    \n",
    "    pre_filter = None\n",
    "    \n",
    "\n",
    "transform_norm = transforms.Compose(transform_list)\n",
    "dataset = CellTypeDataset(\n",
    "        processed_data_folder.absolute(),\n",
    "        pre_transform = transform_norm,\n",
    "        pre_filter = pre_filter,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j,d in enumerate(dataset):\n",
    "#     if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "#         raise Exception(f\"{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num_node_features = dataset.num_node_features\n",
    "dataset_num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: CellTypeDataset(58948):\n",
      "====================\n",
      "Number of graphs: 58948\n",
      "Number of features: 14\n",
      "Number of classes: 32\n",
      "\n",
      "Data(x=[3, 14], edge_index=[2, 4], y=[1, 1])\n",
      "=============================================================\n",
      "Number of nodes: 3\n",
      "Number of edges: 4\n",
      "Average node degree: 1.33\n"
     ]
    }
   ],
   "source": [
    "# looking at the dataset\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset_num_node_features}')\n",
    "print(f'Number of classes: {dataset_num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "# print(f'Has self-loops: {data.has_self_loops()}')\n",
    "# print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- c) Splitting the Data into Labeled and unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3338"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_mask = np.array(\n",
    "    [True if k.y[0][0] > 0 else False for k in dataset]\n",
    ").astype('int')\n",
    "dataset_labeled = dataset[np.where(labeled_mask)[0]]\n",
    "len(dataset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_curr = dataset_labeled\n",
    "torch.manual_seed(12345)\n",
    "dataset_curr = dataset_curr.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- d) Split Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: To turn percentages into raw lengths\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: To turn percentages into raw lengths\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size = 667.0 (0.2 %)\n",
      "validation size = 667.0 (0.2 %)\n",
      "train_size = 2004.0\n",
      "data_lengths_with_train = [2004  667  667]\n",
      "Number of training graphs: 2004\n",
      "Number of test graphs: 667\n",
      "Number of val graphs: 667\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "(train_dataset,\n",
    "val_dataset,\n",
    "test_dataset,) = pret.train_val_test_split(\n",
    "    dataset_curr,\n",
    "    return_dict=False,\n",
    "    verbose = True)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "print(f'Number of val graphs: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if wanted to try and add weights\n",
    "# y_train = np.array([int(data.y[0][0].numpy()) for data in train_dataset])\n",
    "# y_train_classes,y_train_count = np.unique(y_train,return_counts = True)\n",
    "# y_train_classes,y_train_count\n",
    "\n",
    "# sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "# weight = {i:1/}\n",
    "# samples_weight = np.array([weight[t] for t in y_train])\n",
    "# samples_weight = torch.from_numpy(samples_weight)\n",
    "# samples_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dense_adj:\n",
    "    data_loader_mod = DenseDataLoader\n",
    "else:\n",
    "    data_loader_mod = DataLoader\n",
    "\n",
    "\n",
    "train_loader = data_loader_mod(train_dataset, batch_size=batch_size,shuffle = False)\n",
    "test_loader = data_loader_mod(test_dataset, batch_size=batch_size,shuffle=False)\n",
    "val_loader = data_loader_mod(val_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3a; Picking the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GAT\"\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "import general_utils as gu\n",
    "architecture_kwargs_global = dict(\n",
    "    n_hidden_channels = 32, \n",
    "    #n_hidden_channels=64, \n",
    "    #first_heads=8, \n",
    "    #output_heads=1, \n",
    "    #dropout=0.6,\n",
    "    global_pool_type=\"mean\",\n",
    "    n_layers = 2,\n",
    "    first_heads = 8,\n",
    "    output_heads = 1,\n",
    "    classifier_flat = True\n",
    "    \n",
    ")\n",
    "\n",
    "optimizer_kwargs_global = dict(\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "all_run_kwargs = [\n",
    "    #dict(architecture_kwargs = dict(n_hidden_channels = 64),),\n",
    "    \n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 8)),\n",
    "    dict(),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 128)),\n",
    "    \n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 8)),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 32),),\n",
    "    #dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 64),),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 128),),\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_utils as evu\n",
    "import torch.nn.functional as F\n",
    "import model_utils as mdlu\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****------ Running Model Config 0 with following parameters ------****\n",
      "{'n_hidden_channels': 8, 'global_pool_type': 'mean', 'n_layers': 2, 'first_heads': 8, 'output_heads': 1, 'classifier_flat': True, 'lr': 0.01}\n",
      "GAT(\n",
      "  (conv0): GATConv(14, 8, heads=8)\n",
      "  (conv1): GATConv(64, 8, heads=1)\n",
      "  (classifier): ClassifierFlat(\n",
      "    (lin): Linear(in_features=8, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "tensorboard_file_name = GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True\n",
      "GAT(\n",
      "  (conv0): GATConv(14, 8, heads=8)\n",
      "  (conv1): GATConv(64, 8, heads=1)\n",
      "  (classifier): ClassifierFlat(\n",
      "    (lin): Linear(in_features=8, out_features=32, bias=True)\n",
      "  )\n",
      ")\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 4  5  6 14 20 22 23 31],y_pred_counts = [  75 1380    3   10   37   32  424   43]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6 14 20 22 23 31],y_pred_counts = [  1  32 442   1   3  10  15 152  11]\n",
      "Epoch: 000, loss = 3.7463791370391846\n",
      "   train metrics:  accuracy: 0.027944,\n",
      "   val metrics:  accuracy: 0.023988,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6 14 18 20 22 23 31],y_pred_counts = [   1  295 1077    1   10    1   66   22  517   14]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5 14 20 22 23 31],y_pred_counts = [  1  87 361   3  17  12 183   3]\n",
      "Epoch: 001, loss = 3.638627290725708\n",
      "   train metrics:  accuracy: 0.038922,\n",
      "   val metrics:  accuracy: 0.035982,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6 14 18 20 22 23 31],y_pred_counts = [  1 562 825   1   7   3  80  13 508   4]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5 14 18 20 22 23],y_pred_counts = [  1 181 268   2   1  25   7 182]\n",
      "Epoch: 002, loss = 3.411712169647217\n",
      "   train metrics:  accuracy: 0.051397,\n",
      "   val metrics:  accuracy: 0.058471,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 4  5  8 11 14 18 20 22 23],y_pred_counts = [864 599   7   1   6   3  70  16 438]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 14 18 20 22 23],y_pred_counts = [  1 275 200   2   3   1  26   6 153]\n",
      "Epoch: 003, loss = 3.4265453815460205\n",
      "   train metrics:  accuracy: 0.071856,\n",
      "   val metrics:  accuracy: 0.065967,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 11 14 18 20 22 23],y_pred_counts = [   4 1099  458   64    1    4    3   65   16  290]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 11 14 18 20 22 23],y_pred_counts = [  1 344 157  22   1   2   2  20   6 112]\n",
      "Epoch: 004, loss = 3.319230556488037\n",
      "   train metrics:  accuracy: 0.096806,\n",
      "   val metrics:  accuracy: 0.079460,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 11 14 18 20 22 23],y_pred_counts = [   3 1171  351  250    2    2    4   27   18  176]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 11 14 18 20 22 23],y_pred_counts = [  3 370 125  84   1   1   3   9   6  65]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_5\n",
      "Epoch: 005, loss = 3.4935548305511475\n",
      "   train metrics:  accuracy: 0.108782,\n",
      "   val metrics:  accuracy: 0.088456,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 14 18 20 22 23],y_pred_counts = [  12    2 1158  232  494    3    1    6    7   12   77]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 14 18 20 22 23],y_pred_counts = [  8   1 368  79 177   1   1   3   3   4  22]\n",
      "Epoch: 006, loss = 3.0069122314453125\n",
      "   train metrics:  accuracy: 0.122754,\n",
      "   val metrics:  accuracy: 0.100450,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18 20 22 23],y_pred_counts = [  25    4 1087  173  671    3   10    1    3   27]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18 22 23],y_pred_counts = [ 10   3 347  58 236   1   2   1   9]\n",
      "Epoch: 007, loss = 3.1705377101898193\n",
      "   train metrics:  accuracy: 0.126248,\n",
      "   val metrics:  accuracy: 0.110945,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18 22 23],y_pred_counts = [ 66   6 959 142 811   4   9   1   6]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18 22 23],y_pred_counts = [ 16   3 315  50 278   1   2   1   1]\n",
      "Epoch: 008, loss = 3.050868272781372\n",
      "   train metrics:  accuracy: 0.126747,\n",
      "   val metrics:  accuracy: 0.113943,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18 23],y_pred_counts = [122   7 849 124 888   2   9   3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [ 25   3 283  47 306   1   2]\n",
      "Epoch: 009, loss = 2.9906184673309326\n",
      "   train metrics:  accuracy: 0.131737,\n",
      "   val metrics:  accuracy: 0.118441,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18 23],y_pred_counts = [244   7 728 125 889   2   8   1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [ 65   3 236  49 311   1   2]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_10\n",
      "Epoch: 010, loss = 3.048480987548828\n",
      "   train metrics:  accuracy: 0.148703,\n",
      "   val metrics:  accuracy: 0.139430,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [366   9 559 129 932   2   7]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [101   5 184  50 324   1   2]\n",
      "Epoch: 011, loss = 3.092085123062134\n",
      "   train metrics:  accuracy: 0.171657,\n",
      "   val metrics:  accuracy: 0.155922,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [500  14 407 151 923   2   7]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [154   5 118  52 335   1   2]\n",
      "Epoch: 012, loss = 2.88435697555542\n",
      "   train metrics:  accuracy: 0.189122,\n",
      "   val metrics:  accuracy: 0.185907,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [596  32 279 161 927   2   7]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [188  11  77  56 332   1   2]\n",
      "Epoch: 013, loss = 2.773585557937622\n",
      "   train metrics:  accuracy: 0.202096,\n",
      "   val metrics:  accuracy: 0.193403,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 11 18],y_pred_counts = [660  58 206 167   1 905   1   6]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [202  19  66  56 321   1   2]\n",
      "Epoch: 014, loss = 2.6286418437957764\n",
      "   train metrics:  accuracy: 0.219561,\n",
      "   val metrics:  accuracy: 0.205397,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [706  81 195 181   2 832   7]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 11 18],y_pred_counts = [210  23  63  57 311   1   2]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_15\n",
      "Epoch: 015, loss = 2.985278844833374\n",
      "   train metrics:  accuracy: 0.235030,\n",
      "   val metrics:  accuracy: 0.211394,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [744 108 196 190   4 754   8]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [225  31  62  62 285   2]\n",
      "Epoch: 016, loss = 2.623690128326416\n",
      "   train metrics:  accuracy: 0.247505,\n",
      "   val metrics:  accuracy: 0.226387,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [779 133 212 201   4 667   8]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [242  38  62  68 254   3]\n",
      "Epoch: 017, loss = 2.65171217918396\n",
      "   train metrics:  accuracy: 0.259980,\n",
      "   val metrics:  accuracy: 0.236882,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [805 146 229 214   4 597   9]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [249  47  67  72 229   3]\n",
      "Epoch: 018, loss = 2.5183026790618896\n",
      "   train metrics:  accuracy: 0.273453,\n",
      "   val metrics:  accuracy: 0.245877,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [787 160 268 220   4 554  11]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [255  55  76  75 203   3]\n",
      "Epoch: 019, loss = 2.6287729740142822\n",
      "   train metrics:  accuracy: 0.273952,\n",
      "   val metrics:  accuracy: 0.250375,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [807 141 317 228   5 495  11]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [265  44  95  79 181   3]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_20\n",
      "Epoch: 020, loss = 2.6777100563049316\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.262369,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [834 100 389 244   6 420  11]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [266  32 123  87 156   3]\n",
      "Epoch: 021, loss = 2.4540460109710693\n",
      "   train metrics:  accuracy: 0.276946,\n",
      "   val metrics:  accuracy: 0.260870,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [836  57 457 246   6 385  17]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [267  19 146  90 142   3]\n",
      "Epoch: 022, loss = 2.5628750324249268\n",
      "   train metrics:  accuracy: 0.278443,\n",
      "   val metrics:  accuracy: 0.268366,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [822  34 522 248   6 349  23]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [258  12 171  89 133   4]\n",
      "Epoch: 023, loss = 2.562635898590088\n",
      "   train metrics:  accuracy: 0.281936,\n",
      "   val metrics:  accuracy: 0.268366,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [805  20 572 242   6 330  29]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [255   5 188  87 124   8]\n",
      "Epoch: 024, loss = 2.477200746536255\n",
      "   train metrics:  accuracy: 0.284930,\n",
      "   val metrics:  accuracy: 0.275862,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [807   8 608 218   6 316  41]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  8 18],y_pred_counts = [256   1 203  82 112  13]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_25\n",
      "Epoch: 025, loss = 2.5512936115264893\n",
      "   train metrics:  accuracy: 0.293413,\n",
      "   val metrics:  accuracy: 0.280360,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [792   2 637 203   7 308  55]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [253 213  78 107  16]\n",
      "Epoch: 026, loss = 2.4582364559173584\n",
      "   train metrics:  accuracy: 0.297405,\n",
      "   val metrics:  accuracy: 0.281859,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [817   1 623 179   7 305  72]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [258 210  61 110  28]\n",
      "Epoch: 027, loss = 2.5958824157714844\n",
      "   train metrics:  accuracy: 0.304391,\n",
      "   val metrics:  accuracy: 0.289355,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 18],y_pred_counts = [841   1 608 147   7 302  98]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [272 203  42 110  40]\n",
      "Epoch: 028, loss = 2.4851481914520264\n",
      "   train metrics:  accuracy: 0.315868,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 18],y_pred_counts = [863 591 121   7 306 116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [284 194  38 110  41]\n",
      "Epoch: 029, loss = 2.5091021060943604\n",
      "   train metrics:  accuracy: 0.319860,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 18],y_pred_counts = [877 582 101   7 308 129]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [290 189  30 110  48]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_30\n",
      "Epoch: 030, loss = 2.4280309677124023\n",
      "   train metrics:  accuracy: 0.322854,\n",
      "   val metrics:  accuracy: 0.311844,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18],y_pred_counts = [908 557  77   7 311   1 143]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 18],y_pred_counts = [300 180  23   1 109  54]\n",
      "Epoch: 031, loss = 2.2751049995422363\n",
      "   train metrics:  accuracy: 0.329341,\n",
      "   val metrics:  accuracy: 0.313343,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18],y_pred_counts = [954 509  62   9 317   1 152]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 18],y_pred_counts = [322 157  18   1 112  57]\n",
      "Epoch: 032, loss = 2.1013476848602295\n",
      "   train metrics:  accuracy: 0.333832,\n",
      "   val metrics:  accuracy: 0.314843,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18],y_pred_counts = [990 475  53   9 317   1 159]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 18],y_pred_counts = [336 146  13   1 110  61]\n",
      "Epoch: 033, loss = 2.3237802982330322\n",
      "   train metrics:  accuracy: 0.340319,\n",
      "   val metrics:  accuracy: 0.322339,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18 19],y_pred_counts = [1036  426   44    9  323    1  164    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [350 132  12 111  62]\n",
      "Epoch: 034, loss = 2.3272223472595215\n",
      "   train metrics:  accuracy: 0.347305,\n",
      "   val metrics:  accuracy: 0.323838,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18 19],y_pred_counts = [1097  366   35    8  326    1  170    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [367 114  11 112  63]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_35\n",
      "Epoch: 035, loss = 2.2474894523620605\n",
      "   train metrics:  accuracy: 0.349301,\n",
      "   val metrics:  accuracy: 0.334333,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18 19],y_pred_counts = [1110  350   30    9  334    1  169    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [370 108  10 115  64]\n",
      "Epoch: 036, loss = 2.509826183319092\n",
      "   train metrics:  accuracy: 0.349301,\n",
      "   val metrics:  accuracy: 0.337331,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18 19],y_pred_counts = [1104  343   27    8  351    1  169    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [367 106  10 120  64]\n",
      "Epoch: 037, loss = 2.359696388244629\n",
      "   train metrics:  accuracy: 0.347305,\n",
      "   val metrics:  accuracy: 0.332834,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18 19],y_pred_counts = [1085  352   25    9  360    3  169    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [364 108  10 121  64]\n",
      "Epoch: 038, loss = 2.335031509399414\n",
      "   train metrics:  accuracy: 0.343812,\n",
      "   val metrics:  accuracy: 0.329835,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8 15 18 19],y_pred_counts = [1071  358   24    8  370    4  168    1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [358 112  10 123  64]\n",
      "Epoch: 039, loss = 2.2515478134155273\n",
      "   train metrics:  accuracy: 0.342315,\n",
      "   val metrics:  accuracy: 0.326837,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  4  5  6  8 15 18 19],y_pred_counts = [   1 1050  372   20    9  376    5  170    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  8 18],y_pred_counts = [345 116   9 131  66]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_40\n",
      "Epoch: 040, loss = 2.336348533630371\n",
      "   train metrics:  accuracy: 0.338323,\n",
      "   val metrics:  accuracy: 0.317841,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8 15 18 19],y_pred_counts = [   1 1031    2  388   19   10  374    8  170    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8 15 18],y_pred_counts = [335   1 124   9   1 131   1  65]\n",
      "Epoch: 041, loss = 2.035409450531006\n",
      "   train metrics:  accuracy: 0.337824,\n",
      "   val metrics:  accuracy: 0.314843,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 14 15 18 19],y_pred_counts = [  1 967   2 450  20  11 367   2   1   9 173   1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8  9 14 15 18],y_pred_counts = [322   1 135   9   1 131   1   1   3  63]\n",
      "Epoch: 042, loss = 2.4334230422973633\n",
      "   train metrics:  accuracy: 0.331836,\n",
      "   val metrics:  accuracy: 0.316342,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [934   2 483  19  11 362   5   1   1  14 172]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [307   1 145   9   1 134   3   1   1   3  62]\n",
      "Epoch: 043, loss = 2.2243309020996094\n",
      "   train metrics:  accuracy: 0.324850,\n",
      "   val metrics:  accuracy: 0.313343,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 902   2 521  19  12 349   9   1   2  17 169]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8  9 14 15 18],y_pred_counts = [298 160   8   1 127   5   2   5  61]\n",
      "Epoch: 044, loss = 2.342165470123291\n",
      "   train metrics:  accuracy: 0.319860,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 866   2 562  19  15 336  14   1   4  17 167]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  4  5  6  8  9 12 14 15 18],y_pred_counts = [281 177   8   1 122  10   1   3   5  59]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_45\n",
      "Epoch: 045, loss = 2.178511142730713\n",
      "   train metrics:  accuracy: 0.317365,\n",
      "   val metrics:  accuracy: 0.298351,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  2 833   1 603  17  16 323  20   1  11  16 161]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [264   1 193   8   2 120  11   1   4   6  57]\n",
      "Epoch: 046, loss = 2.1889522075653076\n",
      "   train metrics:  accuracy: 0.306387,\n",
      "   val metrics:  accuracy: 0.290855,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  2 816   2 629  15  15 311  25   2  22  16 149]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [259   1 201   7   3 117  11   2   6   6  54]\n",
      "Epoch: 047, loss = 2.0189037322998047\n",
      "   train metrics:  accuracy: 0.302395,\n",
      "   val metrics:  accuracy: 0.287856,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 823   3 626  14  15 311  22   3  29  14 143]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 259   1 204   7   3 114  10   2   8   4  54]\n",
      "Epoch: 048, loss = 2.120206594467163\n",
      "   train metrics:  accuracy: 0.302395,\n",
      "   val metrics:  accuracy: 0.283358,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 815  15 617  14  17 317  19   6  34  10 139]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 256   4 201   7   2 120   9   2   8   2  55]\n",
      "Epoch: 049, loss = 2.208733558654785\n",
      "   train metrics:  accuracy: 0.301397,\n",
      "   val metrics:  accuracy: 0.278861,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 795  34 617  12  17 329  14   7  34   9 135]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 243  18 201   7   3 119   8   2   9   2  54]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_50\n",
      "Epoch: 050, loss = 2.0681192874908447\n",
      "   train metrics:  accuracy: 0.298902,\n",
      "   val metrics:  accuracy: 0.275862,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  2 768  57 621  11  17 332  12  12  36   7 129]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 233  26 202   6   2 123   8   2   9   1  54]\n",
      "Epoch: 051, loss = 2.035836935043335\n",
      "   train metrics:  accuracy: 0.300898,\n",
      "   val metrics:  accuracy: 0.266867,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  2 763  84 600  11  17 331  13  13  32   7 131]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  1 231  35 194   5   2 125   8   2   9   1  54]\n",
      "Epoch: 052, loss = 1.9298475980758667\n",
      "   train metrics:  accuracy: 0.310878,\n",
      "   val metrics:  accuracy: 0.259370,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  3 748  99 586  12  18 340  17  16  24   5 136]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  2 231  39 185   4   3 128   8   3   6  58]\n",
      "Epoch: 053, loss = 2.236969470977783\n",
      "   train metrics:  accuracy: 0.316866,\n",
      "   val metrics:  accuracy: 0.275862,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  4 759  99 574  13  17 338  20  20   9   3 148]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  2 233  38 184   4   2 129   9   3   4  59]\n",
      "Epoch: 054, loss = 2.2518980503082275\n",
      "   train metrics:  accuracy: 0.324351,\n",
      "   val metrics:  accuracy: 0.277361,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  4 798  83 550  13  19 335  22  23   6   3 148]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  3 251  34 169   4   2 129   9   4   4  58]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_55\n",
      "Epoch: 055, loss = 2.0121583938598633\n",
      "   train metrics:  accuracy: 0.329341,\n",
      "   val metrics:  accuracy: 0.295352,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  5 841  70 530  11  19 328  22  23   3   2 150]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  4 276  25 161   6   3 119   9   4   4  56]\n",
      "Epoch: 056, loss = 2.212968587875366\n",
      "   train metrics:  accuracy: 0.335329,\n",
      "   val metrics:  accuracy: 0.325337,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [  9 893  66 496   9  20 313  22  23   2   2 149]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  4 296  19 153   6   6 112   9   4   3  55]\n",
      "Epoch: 057, loss = 2.0243663787841797\n",
      "   train metrics:  accuracy: 0.342814,\n",
      "   val metrics:  accuracy: 0.337331,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 18 914  71 481   9  16 291  28  24   2   1 149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  5 301  22 150   6   7 105   9   4   3  55]\n",
      "Epoch: 058, loss = 2.0618252754211426\n",
      "   train metrics:  accuracy: 0.347804,\n",
      "   val metrics:  accuracy: 0.340330,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 24 920  74 479   9  11 276  35  24   3   1 148]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 10 305  24 147   4   4 101  10   4   3  55]\n",
      "Epoch: 059, loss = 2.0008091926574707\n",
      "   train metrics:  accuracy: 0.348303,\n",
      "   val metrics:  accuracy: 0.338831,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 34 946  79 439   9   8 264  49  27   6   1 142]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 12 315  24 135   5   4  95  15   5   4  53]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_60\n",
      "Epoch: 060, loss = 2.0814826488494873\n",
      "   train metrics:  accuracy: 0.352794,\n",
      "   val metrics:  accuracy: 0.335832,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 41 943  93 429  11   3 253  55  28  10   1 137]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 19 313  34 133   6   2  83  15   5   5  52]\n",
      "Epoch: 061, loss = 2.1251587867736816\n",
      "   train metrics:  accuracy: 0.350299,\n",
      "   val metrics:  accuracy: 0.331334,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 45 918 105 436  13   1 247  62  28  21   1 127]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  8  9 12 14 18],y_pred_counts = [ 21 304  35 139   3  81  18   8   5  53]\n",
      "Epoch: 062, loss = 1.9141981601715088\n",
      "   train metrics:  accuracy: 0.351297,\n",
      "   val metrics:  accuracy: 0.329835,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 50 872 123 456  14   1 242  66  29  26   1 124]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  8  9 12 14 18],y_pred_counts = [ 22 293  37 146   3  79  19   9   5  54]\n",
      "Epoch: 063, loss = 1.980749487876892\n",
      "   train metrics:  accuracy: 0.351297,\n",
      "   val metrics:  accuracy: 0.329835,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 53 810 124 500  15   1 256  65  30  29   1 120]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 23 274  37 158   5   1  81  19  11   8  50]\n",
      "Epoch: 064, loss = 1.829222559928894\n",
      "   train metrics:  accuracy: 0.344311,\n",
      "   val metrics:  accuracy: 0.335832,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 53 744 142 548  18   1 252  66  31  30   1 118]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 24 244  47 176   5   1  81  20  11   8  50]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_65\n",
      "Epoch: 065, loss = 2.0549049377441406\n",
      "   train metrics:  accuracy: 0.343812,\n",
      "   val metrics:  accuracy: 0.314843,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 56 670 178 575  21   2 258  62  32  31   1 118]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 25 218  55 191   6   1  83  19  11   8  50]\n",
      "Epoch: 066, loss = 2.1021714210510254\n",
      "   train metrics:  accuracy: 0.341816,\n",
      "   val metrics:  accuracy: 0.314843,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 57 635 212 563  22   2 268  63  32  30   1 119]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 25 203  73 185   6   1  87  18  12   8  49]\n",
      "Epoch: 067, loss = 1.9389519691467285\n",
      "   train metrics:  accuracy: 0.341816,\n",
      "   val metrics:  accuracy: 0.319340,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 56 572 260 570  22   4 267  70  32  27   1 123]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 25 184  84 188   6   2  89  20  12   8  49]\n",
      "Epoch: 068, loss = 1.977606177330017\n",
      "   train metrics:  accuracy: 0.347804,\n",
      "   val metrics:  accuracy: 0.319340,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 55 510 307 582  22   4 268  73  32  25   1 125]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 21 173  92 192   6   2  92  20  12   8  49]\n",
      "Epoch: 069, loss = 1.8512946367263794\n",
      "   train metrics:  accuracy: 0.348802,\n",
      "   val metrics:  accuracy: 0.316342,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 47 494 321 557  21   4 288  88  31  25   1 127]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 20 164 103 175   6   2 105  23  11   9  49]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_70\n",
      "Epoch: 070, loss = 1.8968074321746826\n",
      "   train metrics:  accuracy: 0.347804,\n",
      "   val metrics:  accuracy: 0.319340,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 43 484 313 531  23   5 310 109  32  25   1 128]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 19 162  98 169   6   2 112  30  11  10  48]\n",
      "Epoch: 071, loss = 2.2023935317993164\n",
      "   train metrics:  accuracy: 0.348802,\n",
      "   val metrics:  accuracy: 0.314843,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 39 513 296 437  23   5 337 162  34  31   1 126]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 16 167  96 152   6   2 120  39  11  11  47]\n",
      "Epoch: 072, loss = 1.9302866458892822\n",
      "   train metrics:  accuracy: 0.346307,\n",
      "   val metrics:  accuracy: 0.310345,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 28 537 298 361  23   6 370 189  33  35   1 123]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [ 10 173  92 123   6   2 138  54  11  12  46]\n",
      "Epoch: 073, loss = 2.100043535232544\n",
      "   train metrics:  accuracy: 0.347804,\n",
      "   val metrics:  accuracy: 0.310345,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 22 557 272 297  22   7 404 231  35  35   1 121]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  6 185  82  96   6   3 149  71  13  14  42]\n",
      "Epoch: 074, loss = 2.0588366985321045\n",
      "   train metrics:  accuracy: 0.341816,\n",
      "   val metrics:  accuracy: 0.317841,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 15 18],y_pred_counts = [ 18 602 264 207  22   6 424 270  37  41   1 112]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  5 207  78  61   6   3 152  86  15  15  39]\n",
      "Saving off checkpoint model_checkpoints/GAT/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_checkpoints/GAT_n_hidden_channels_8_global_pool_type_mean_n_layers_2_first_heads_8_output_heads_1_classifier_flat_True_lr_0.01_with_skeleton_True_epoch_75\n",
      "Epoch: 075, loss = 1.9974693059921265\n",
      "   train metrics:  accuracy: 0.334331,\n",
      "   val metrics:  accuracy: 0.328336,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18 27],y_pred_counts = [ 19 638 270 147  22   6 437 274  39  44 107   1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  5 222  81  41   6   3 153  87  15  17  37]\n",
      "Epoch: 076, loss = 1.9787731170654297\n",
      "   train metrics:  accuracy: 0.340818,\n",
      "   val metrics:  accuracy: 0.332834,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18 27],y_pred_counts = [ 16 680 294  98  21   6 459 239  45  44 101   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  4 231  91  29   6   2 158  77  15  19  35]\n",
      "Epoch: 077, loss = 1.790694236755371\n",
      "   train metrics:  accuracy: 0.344311,\n",
      "   val metrics:  accuracy: 0.343328,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18 27],y_pred_counts = [ 13 720 286  75  18   7 469 226  47  39 103   1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  3 245  92  18   6   2 161  71  17  14  38]\n",
      "Epoch: 078, loss = 1.9798698425292969\n",
      "   train metrics:  accuracy: 0.350299,\n",
      "   val metrics:  accuracy: 0.352324,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18 27],y_pred_counts = [ 19 714 321  63  19   7 468 206  50  39  97   1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 1  2  3  4  5  6  8  9 12 14 18],y_pred_counts = [  4 244 106  16   4   2 162  60  21  16  32]\n",
      "Epoch: 079, loss = 1.8980838060379028\n",
      "   train metrics:  accuracy: 0.353792,\n",
      "   val metrics:  accuracy: 0.359820,\n"
     ]
    }
   ],
   "source": [
    "for j,config_dict in enumerate(all_run_kwargs):\n",
    "    \n",
    "    architecture_kwargs = config_dict.get(\"architecture_kwargs\",dict())\n",
    "    optimizer_kwargs = config_dict.get(\"optimizer_kwargs\",dict())\n",
    "    \n",
    "    architecture_kwargs = gu.merge_dicts([architecture_kwargs_global.copy(),architecture_kwargs])\n",
    "    optimizer_kwargs = gu.merge_dicts([optimizer_kwargs_global.copy(),optimizer_kwargs])\n",
    "    \n",
    "    run_kwargs = gu.merge_dicts([architecture_kwargs,optimizer_kwargs])\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\\n\\n****------ Running Model Config {j} with following parameters ------****\\n{run_kwargs}\")\n",
    "\n",
    "    model = getattr(gm,model_name)(\n",
    "        dataset_num_node_features=dataset_num_node_features,\n",
    "        dataset_num_classes=dataset_num_classes,\n",
    "        **architecture_kwargs\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **optimizer_kwargs)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------------- Configuring the Tensorboard and Checkpoinns--------------------\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    tensorboard_dir = Path(\"./tensorboard\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "    tensorboard_dir = tensorboard_dir / Path(f\"{model_name}\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    tensorboard_file_name = (f'{model_name}_' \n",
    "                             + \"_\".join([f\"{k}_{v}\" for k,v in run_kwargs.items()]))\n",
    "    tensorboard_file_name += f\"_with_skeleton_{with_skeleton}\"\n",
    "    print(f\"tensorboard_file_name = {tensorboard_file_name}\")\n",
    "    tensorboard_file = tensorboard_dir / Path(f'{tensorboard_file_name}')\n",
    "    try:\n",
    "        su.rm_dir(tensorboard_file)\n",
    "    except:\n",
    "        pass\n",
    "    tensorboard_file.mkdir(exist_ok = True)\n",
    "\n",
    "\n",
    "    #-- when to save a checkpoint of the model\n",
    "    checkpoint_dir = Path(\"./model_checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_dir = checkpoint_dir / Path(f\"{model_name}\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_path = checkpoint_dir / Path(f\"./{tensorboard_file_name}_checkpoints\")\n",
    "\n",
    "    try:\n",
    "        su.rm_dir(checkpoint_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    checkpoint_path.mkdir(exist_ok = True)\n",
    "    n_epoch_for_checkpoint = 5\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_file)\n",
    "    \n",
    "\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    # weights = weight#[0.1,0.5,,1,0.7,1,1,1]\n",
    "    # class_weights = None\n",
    "    class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "    tensor_map = None\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:#train_loader:  # Iterate in batches over the training dataset.\n",
    "            #print(f\"data = {data}\")\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPool\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "            y_true = data.y.squeeze_()\n",
    "            #print(f\"out.shape = {out.shape}, data.y.shape = {data.y.shape}\")\n",
    "            loss = F.nll_loss(\n",
    "                torch.log(out), y_true,\n",
    "                weight = class_weights,\n",
    "            )  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            return loss\n",
    "\n",
    "\n",
    "    def test(loader,verbose = False):\n",
    "        model.eval()\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPool\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "\n",
    "            y_pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            y_true = data.y.squeeze_()\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y_true)\n",
    "    #         error_idx = np.where(pred > 0)[0]\n",
    "    #         if len(error_idx) > 0:\n",
    "    #             print(f\"error_idx = {error_idx}\")\n",
    "        y_pred = torch.cat(y_pred_list)\n",
    "        y_true = torch.cat(y_true_list)\n",
    "\n",
    "        return evu.metric_dict(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            tensor_map=tensor_map,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "    log_to_tensorboard = True\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss = train()\n",
    "        writer.add_scalar('loss',loss,epoch) # new line\n",
    "        train_metric_dict = test(train_loader)#train_loader)\n",
    "        val_metric_dict = test(val_loader)#test_loader)\n",
    "\n",
    "        if epoch % n_epoch_for_checkpoint == 0 and epoch != 0:\n",
    "            val_acc = val_metric_dict['accuracy'].numpy()\n",
    "            checkpoitn_filepath = checkpoint_path / Path(f\"{tensorboard_file_name}_epoch_{epoch}\")#_val_acc_{val_acc:.2f}\")\n",
    "            print(f\"Saving off checkpoint {checkpoitn_filepath}\")\n",
    "            mdlu.save_checkpoint(model,filepath = checkpoitn_filepath,epoch = epoch,loss = loss)\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, loss = {loss}')\n",
    "        for type_name,metric_dict in zip([\"train\",\"val\"],[train_metric_dict,val_metric_dict]):\n",
    "            print_log = f\"   {type_name} metrics: \"\n",
    "            for k,v in metric_dict.items():\n",
    "                if log_to_tensorboard:\n",
    "                    writer.add_scalar(f'{type_name}_{k}',v,epoch)\n",
    "                print_log += f\" {k}: {v:4f},\"\n",
    "\n",
    "            print(print_log)\n",
    "        \n",
    "        if val_metric_dict[\"accuracy\"] < 0.0001:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /pytorch_tools/Applications/Cell_Types_GNN/tensorboard --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard_utils as tbu\n",
    "# df_board = df_tensorboard(\"./tensorboard/\",verbose = True)\n",
    "# df_board.query(\"(run=='DiffPooln_hidden_channels_32') and (name=='train_accuracy')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(checkpoint_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_name = \"GCNFlat_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True\"\n",
    "epoch = 95\n",
    "winning_dir = checkpoint_dir / Path(f\"{winning_name}_checkpoints\") \n",
    "winning_filepath = winning_dir / Path(f\"{winning_name}_epoch_{epoch}\")\n",
    "winning_filepath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True_epoch_95 #good one for seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Embedding for all cell types (Can Run in Batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_kwargs_curr = dict(n_hidden_channels = 64,global_pool_type = \"mean\",n_layers = 2)\n",
    "architecture_kwargs = gu.merge_dicts([architecture_kwargs_global,architecture_kwargs_curr])\n",
    "architecture_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(gm,model_name)(\n",
    "    dataset_num_node_features=dataset_num_node_features,\n",
    "    dataset_num_classes=dataset_num_classes,\n",
    "    **architecture_kwargs,\n",
    "    #use_bn=False\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(winning_filepath)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_loader = data_loader_mod(dataset, batch_size=batch_size,shuffle = False)\n",
    "all_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "embeddings = []\n",
    "labels = []\n",
    "for data in tqdm(all_data_loader):#train_loader:  # Iterate in batches over the training dataset.\n",
    "    data = data.to(device)\n",
    "    if model_name == \"DiffPool\":\n",
    "            out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "            #y_true = data.y.reshape(-1,3)\n",
    "    elif model_name == \"TreeLSTM\":\n",
    "        n = data.x.shape[0]\n",
    "        h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        out = model(\n",
    "            data,\n",
    "            h = h,\n",
    "            c = c,\n",
    "            embeddings = data.x\n",
    "            )\n",
    "    else:\n",
    "        out = model(data)\n",
    "\n",
    "    out_array = out.detach().cpu().numpy()\n",
    "    out_labels = data.y.numpy().reshape(-1)\n",
    "    #print(f\"out_array.shape = {out_array.shape}, out_labels.shape = {out_labels.shape}\")\n",
    "    \n",
    "#     if out_array.shape[0] != out_labels.shape[0]:\n",
    "#         raise Exception(\"\")\n",
    "    \n",
    "    embeddings.append(out_array)\n",
    "    labels.append(out_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "embedding_df[\"cell_type\"] = labels\n",
    "\n",
    "import general_utils as gu\n",
    "decoder_map = dict([(v,k) if k is not None else (v,\"Unknown\") for k,v in cell_type_map.items()])\n",
    "\n",
    "import pandas_utils as pu\n",
    "embedding_df[\"cell_type\"] = pu.new_column_from_dict_mapping(embedding_df,decoder_map,column_name = \"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(embedding_df[\"cell_type\"].to_numpy(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_idx = embedding_df.query(\"cell_type != 'Unknown'\").index.to_numpy()\n",
    "labeled_mask_plotting = np.zeros(len(embedding_df))\n",
    "labeled_mask_plotting[labeled_idx] = 1\n",
    "labeled_mask_plotting = labeled_mask_plotting.astype(\"bool\")\n",
    "labeled_mask_plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Plotting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import visualizations_ml as vml\n",
    "n_components = 3\n",
    "import dimensionality_reduction_ml as dru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ml as pdml\n",
    "X_data,y_labels = pdml.X_y(embedding_df,\"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data[labeled_mask_plotting].to_numpy().astype(\"float\")\n",
    "y = y_labels[labeled_mask_plotting].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep) PCA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_components_test=10\n",
    "pca_data = dru.pca_analysis(\n",
    "    X,\n",
    "    n_components=n_components_test,\n",
    "    plot_sqrt_eigvals=False,\n",
    "    plot_perc_variance_explained=True\n",
    ")\n",
    "\n",
    "X_pca = pca_data[\"data_proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/meshAfterParty/meshAfterParty/')\n",
    "import datajoint_utils as du\n",
    "import cell_type_utils as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "import cell_type_utils as ctu\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"pca\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"umap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"umap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"isomap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X_pca[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
