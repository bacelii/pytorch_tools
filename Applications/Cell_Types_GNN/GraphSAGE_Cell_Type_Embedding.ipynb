{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Implementation fo DiffPool\n",
    "graph coarsening manner\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "#sys.path.append(\"/meshAfterParty/meshAfterParty\")\n",
    "sys.path.append(\"/python_tools/python_tools\")\n",
    "sys.path.append(\"/machine_learning_tools/machine_learning_tools/\")\n",
    "sys.path.append(\"/pytorch_tools/pytorch_tools/\")\n",
    "sys.path.append(\"/neuron_morphology_tools/neuron_morphology_tools/\")\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/m65_full/df_morphometrics.pbz2'),\n",
       " PosixPath('data/m65_full/cell_type_fine_with_skeleton_no_dense'),\n",
       " PosixPath('data/m65_full/df_cell_type_fine.pbz2')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"./data/m65_full/\")\n",
    "list(data_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python_tools modules\n",
    "import system_utils as su\n",
    "import pandas_utils as pu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_utils as nu\n",
    "import networkx_utils as xu\n",
    "from tqdm_utils import tqdm\n",
    "\n",
    "#neuron_morphology_tools modules\n",
    "import neuron_nx_io as nxio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import transforms\n",
    "\n",
    "# for the dataset object\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import DenseDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_tools modules\n",
    "import preprocessing_utils as pret\n",
    "import geometric_models as gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Choosing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_adj= True, directed = False\n"
     ]
    }
   ],
   "source": [
    "model_name = \"GraphSAGE\"\n",
    "model_class = getattr(gm,model_name)\n",
    "dense_adj = getattr(model_class,\"dense_adj\",False)\n",
    "directed = getattr(model_class,\"directed\",False)\n",
    "print(f\"dense_adj= {dense_adj}, directed = {directed}\")\n",
    "\n",
    "gnn_task = \"cell_type_fine\"\n",
    "label_name = None\n",
    "graph_label = \"cell_type_fine_label\"\n",
    "data_file = \"df_cell_type_fine.pbz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "with_skeleton = True\n",
    "\n",
    "features_to_delete = [\n",
    "    \"mesh_volume\",\n",
    "    \"apical_label\",\n",
    "    \"basal_label\",\n",
    "]\n",
    "\n",
    "if not with_skeleton:\n",
    "    features_to_delete +=[\n",
    "        \"skeleton_vector_downstream_phi\",      \n",
    "        \"skeleton_vector_downstream_theta\",    \n",
    "        \"skeleton_vector_upstream_phi\",        \n",
    "        \"skeleton_vector_upstream_theta\",  \n",
    "    ]\n",
    "\n",
    "features_to_keep = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading the Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = Path(data_path) / Path(data_file)\n",
    "\n",
    "data_df = su.decompress_pickle(data_filepath)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note the cell_type_fine is the column\n",
    "that has all of the graph data stored\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.query(\"cell_type_fine_label == cell_type_fine_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_df[[\"cell_type_fine\"]].iloc[1].to_list()[0][0]\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating the Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- a) Getting Means and Std Dev for Normalization --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batch_df = pd.concat([nxio.feature_df_from_gnn_info(\n",
    "    k[0],\n",
    "    return_data_labels_split = False) for k in data_df[gnn_task].to_list()])\n",
    "\n",
    "if label_name is not None:\n",
    "    all_batch_df = all_batch_df[[k for k in \n",
    "            all_batch_df.columns if k not in nu.convert_to_array_like(label_name)]]\n",
    "else:\n",
    "    all_batch_df = all_batch_df\n",
    "    \n",
    "# will use these to normalize the data\n",
    "col_means = all_batch_df.mean(axis=0).to_numpy()\n",
    "col_stds = all_batch_df.std(axis=0).to_numpy()\n",
    "\n",
    "all_batch_df_norm = pu.normalize_df(all_batch_df,column_means=col_means,\n",
    "                                 column_stds = col_stds)\n",
    "all_batch_df_norm.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- b) Creating the Dataset Class --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- mapping of the labels to integers --\n",
    "total_labels,label_counts = np.unique((data_df.query(f\"{graph_label}=={graph_label}\")[\n",
    "    graph_label]).to_numpy(),return_counts = True)\n",
    "cell_type_map = {k:i+1 for i,k in enumerate(total_labels)}\n",
    "cell_type_map[None] = 0\n",
    "cell_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell_type_map\n",
    "cell_type_fine_classifier_weights = {\n",
    "'23P': 0.25,#1294\n",
    "'4P': 0.3,#890\n",
    "'5P_IT': 0.5,#465\n",
    "'6P': 0.8,#342\n",
    "'6P_IT': 0.8,#263\n",
    "'5P_PT': 0.8,#224\n",
    "}\n",
    "\n",
    "\n",
    "class_idx = np.array(list(cell_type_map.values()) )\n",
    "class_labels = np.array(list(cell_type_map.keys()) )\n",
    "weights = np.array([cell_type_fine_classifier_weights.get(k,1) for k in class_labels])\n",
    "weights = weights[np.argsort(class_idx)]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_data_from_gnn_info(\n",
    "    gnn_info,\n",
    "    y = None,\n",
    "    verbose = False,\n",
    "    normalize = True,\n",
    "    features_to_delete=None,\n",
    "    features_to_keep = None\n",
    "    ): \n",
    "    \"\"\"\n",
    "    Purpose: To convert our data format into pytorch Data object\n",
    "\n",
    "    Pseudocode: \n",
    "    1) Create the edgelist (turn into tensor)\n",
    "    2) Get the \n",
    "    \"\"\"\n",
    "    edgelist = torch.tensor(xu.edgelist_from_adjacency_matrix(\n",
    "        array = gnn_info[\"adjacency\"],\n",
    "        verbose = False,\n",
    "    ).T,dtype=torch.long)\n",
    "\n",
    "    x,y_raw = nxio.feature_df_from_gnn_info(\n",
    "        gnn_info,\n",
    "        return_data_labels_split = True)\n",
    "    if y is None:\n",
    "        y = y_raw\n",
    "        \n",
    "    if not type(y) == str:\n",
    "        y = None\n",
    "        \n",
    "    y_int = np.array(cell_type_map[y] ).reshape(1,-1)\n",
    "    \n",
    "    if normalize:\n",
    "        x = (x-col_means)/col_stds\n",
    "    \n",
    "    # --- keeping or not keeping sertain features\n",
    "    gnn_features = gnn_info[\"features\"]\n",
    "\n",
    "    keep_idx = np.arange(len(gnn_features))\n",
    "    if features_to_delete is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k not in features_to_delete])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER DELETE= {keep_idx}\")\n",
    "    if features_to_keep is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k in features_to_keep])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER KEEP = {keep_idx}\")\n",
    "\n",
    "    x = x[:,keep_idx]\n",
    "\n",
    "    x = torch.tensor(x,dtype=torch.float)\n",
    "    y = torch.tensor(y_int,dtype=torch.long)\n",
    "    \n",
    "    if len(y) > 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "        \n",
    "    if y.shape[0] != 1 or y.shape[1] != 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"x.shape = {x.shape},y.shape ={y.shape}\")\n",
    "\n",
    "    data = Data(x=x,y=y,edge_index=edgelist)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellTypeDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        #return ['some_file_1', 'some_file_2', ...]\n",
    "        return [str(data_filepath.absolute())]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # def download(self):\n",
    "    #     # Download to `self.raw_dir`.\n",
    "    #     download_url(url, self.raw_dir)\n",
    "    #     ...\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        #data_list = [...]\n",
    "\n",
    "#         if data_df is None:\n",
    "#             data_df = su.decompress_pickle(self.raw_file_names[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        data_list = []\n",
    "        for k,y in tqdm(zip(\n",
    "            data_df[gnn_task].to_list(),\n",
    "            data_df[graph_label].to_list())):\n",
    "            \n",
    "            data_list.append(pytorch_data_from_gnn_info(\n",
    "                k[0],\n",
    "                y=y,\n",
    "                features_to_delete=features_to_delete,\n",
    "                features_to_keep = features_to_keep,\n",
    "                verbose = False))\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list_final = []\n",
    "            for data in data_list:\n",
    "                try:\n",
    "                    if self.pre_filter(data):\n",
    "                        data_list_final.append(data)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}\")\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list_final = []\n",
    "            for j,data in enumerate(data_list):\n",
    "                try:\n",
    "                    curr_t = self.pre_transform(data)\n",
    "                    if curr_t.y.shape[0] != 1 or curr_t.y.shape[1] != 1:\n",
    "                        raise Exception(f\"{j}, data = {curr_t}\")\n",
    "                    data_list_final.append(curr_t)\n",
    "                except:\n",
    "                    continue\n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}, data = {d}\")\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_skeleton:\n",
    "    gnn_task_name = f\"{gnn_task}_with_skeleton\"\n",
    "else:\n",
    "    gnn_task_name = f\"{gnn_task}\"\n",
    "\n",
    "if dense_adj:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}\")#_processed_dense\")\n",
    "elif directed:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_directed\")#_processed_dense\")\n",
    "else:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_no_dense\")#_processed_dense\")\n",
    "    \n",
    "# try:\n",
    "#     su.rm_dir(processed_data_folder)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "processed_data_folder.mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes = np.max(all_batch_df_norm.index.to_numpy()) + 1\n",
    "\n",
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return data.num_nodes <= max_nodes\n",
    "    \n",
    "if dense_adj:\n",
    "    #gets the maximum number of nodes in any of the graphs\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "        T.ToDense(max_nodes),\n",
    "        #transforms.NormalizeFeatures(),\n",
    "    ]\n",
    "    pre_filter = MyFilter()\n",
    "elif directed:\n",
    "    transform_list = []\n",
    "    pre_filter = None\n",
    "else:\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "    ]\n",
    "    \n",
    "    pre_filter = None\n",
    "    \n",
    "\n",
    "transform_norm = transforms.Compose(transform_list)\n",
    "dataset = CellTypeDataset(\n",
    "        processed_data_folder.absolute(),\n",
    "        pre_transform = transform_norm,\n",
    "        pre_filter = pre_filter,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,d in enumerate(dataset):\n",
    "    if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "        raise Exception(f\"{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num_node_features = dataset.num_node_features\n",
    "dataset_num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the dataset\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset_num_node_features}')\n",
    "print(f'Number of classes: {dataset_num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "# print(f'Has self-loops: {data.has_self_loops()}')\n",
    "# print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- c) Splitting the Data into Labeled and unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_mask = np.array(\n",
    "    [True if k.y[0][0] > 0 else False for k in dataset]\n",
    ").astype('int')\n",
    "dataset_labeled = dataset[np.where(labeled_mask)[0]]\n",
    "len(dataset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_curr = dataset_labeled\n",
    "torch.manual_seed(12345)\n",
    "dataset_curr = dataset_curr.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- d) Split Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To turn percentages into raw lengths\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "(train_dataset,\n",
    "val_dataset,\n",
    "test_dataset,) = pret.train_val_test_split(\n",
    "    dataset_curr,\n",
    "    return_dict=False,\n",
    "    verbose = True)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "print(f'Number of val graphs: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if wanted to try and add weights\n",
    "# y_train = np.array([int(data.y[0][0].numpy()) for data in train_dataset])\n",
    "# y_train_classes,y_train_count = np.unique(y_train,return_counts = True)\n",
    "# y_train_classes,y_train_count\n",
    "\n",
    "# sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "# weight = {i:1/}\n",
    "# samples_weight = np.array([weight[t] for t in y_train])\n",
    "# samples_weight = torch.from_numpy(samples_weight)\n",
    "# samples_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dense_adj:\n",
    "    data_loader_mod = DenseDataLoader\n",
    "else:\n",
    "    data_loader_mod = DataLoader\n",
    "\n",
    "\n",
    "train_loader = data_loader_mod(train_dataset, batch_size=batch_size,shuffle = True)\n",
    "test_loader = data_loader_mod(test_dataset, batch_size=batch_size,shuffle=False)\n",
    "val_loader = data_loader_mod(val_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3a; Picking the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GAT\"\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "import general_utils as gu\n",
    "architecture_kwargs_global = dict(\n",
    "    n_hidden_channels = 32, \n",
    "    #n_hidden_channels=64, \n",
    "    #first_heads=8, \n",
    "    #output_heads=1, \n",
    "    #dropout=0.6,\n",
    "    global_pool_type=\"mean\",\n",
    "    n_layers = 2,\n",
    "    heads = 3\n",
    ")\n",
    "\n",
    "optimizer_kwargs_global = dict(\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "all_run_kwargs = [\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 8))\n",
    "    dict(),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 64),),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 128),),\n",
    "    \n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 32),),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 64),),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 128),),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 8))\n",
    "    \n",
    "    \n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_utils as evu\n",
    "import torch.nn.functional as F\n",
    "import model_utils as mdlu\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j,config_dict in enumerate(all_run_kwargs):\n",
    "    \n",
    "    architecture_kwargs = config_dict.get(\"architecture_kwargs\",dict())\n",
    "    optimizer_kwargs = config_dict.get(\"optimizer_kwargs\",dict())\n",
    "    \n",
    "    architecture_kwargs = gu.merge_dicts([architecture_kwargs_global.copy(),architecture_kwargs])\n",
    "    optimizer_kwargs = gu.merge_dicts([optimizer_kwargs_global.copy(),optimizer_kwargs])\n",
    "    \n",
    "    run_kwargs = gu.merge_dicts([architecture_kwargs,optimizer_kwargs])\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\\n\\n****------ Running Model Config {j} with following parameters ------****\\n{run_kwargs}\")\n",
    "\n",
    "    model = getattr(gm,model_name)(\n",
    "        dataset_num_node_features=dataset_num_node_features,\n",
    "        dataset_num_classes=dataset_num_classes,\n",
    "        **architecture_kwargs\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **optimizer_kwargs)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------------- Configuring the Tensorboard and Checkpoinns--------------------\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    tensorboard_dir = Path(\"./tensorboard\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "    tensorboard_dir = tensorboard_dir / Path(f\"{model_name}\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    tensorboard_file_name = (f'{model_name}_' \n",
    "                             + \"_\".join([f\"{k}_{v}\" for k,v in run_kwargs.items()]))\n",
    "    tensorboard_file_name += f\"_with_skeleton_{with_skeleton}\"\n",
    "    print(f\"tensorboard_file_name = {tensorboard_file_name}\")\n",
    "    tensorboard_file = tensorboard_dir / Path(f'{tensorboard_file_name}')\n",
    "    try:\n",
    "        su.rm_dir(tensorboard_file)\n",
    "    except:\n",
    "        pass\n",
    "    tensorboard_file.mkdir(exist_ok = True)\n",
    "\n",
    "\n",
    "    #-- when to save a checkpoint of the model\n",
    "    checkpoint_dir = Path(\"./model_checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_dir = checkpoint_dir / Path(f\"{model_name}\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_path = checkpoint_dir / Path(f\"./{tensorboard_file_name}_checkpoints\")\n",
    "\n",
    "    try:\n",
    "        su.rm_dir(checkpoint_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    checkpoint_path.mkdir(exist_ok = True)\n",
    "    n_epoch_for_checkpoint = 5\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_file)\n",
    "    \n",
    "\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    # weights = weight#[0.1,0.5,,1,0.7,1,1,1]\n",
    "    # class_weights = None\n",
    "    class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "    tensor_map = None\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:#train_loader:  # Iterate in batches over the training dataset.\n",
    "            #print(f\"data = {data}\")\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPool\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "            y_true = data.y.squeeze_()\n",
    "            #print(f\"out.shape = {out.shape}, data.y.shape = {data.y.shape}\")\n",
    "            loss = F.nll_loss(\n",
    "                torch.log(out), y_true,\n",
    "                weight = class_weights,\n",
    "            )  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            return loss\n",
    "\n",
    "\n",
    "    def test(loader,verbose = False):\n",
    "        model.eval()\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPool\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "\n",
    "            y_pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            y_true = data.y.squeeze_()\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y_true)\n",
    "    #         error_idx = np.where(pred > 0)[0]\n",
    "    #         if len(error_idx) > 0:\n",
    "    #             print(f\"error_idx = {error_idx}\")\n",
    "        y_pred = torch.cat(y_pred_list)\n",
    "        y_true = torch.cat(y_true_list)\n",
    "\n",
    "        return evu.metric_dict(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            tensor_map=tensor_map,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "    log_to_tensorboard = True\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss = train()\n",
    "        writer.add_scalar('loss',loss,epoch) # new line\n",
    "        train_metric_dict = test(train_loader)#train_loader)\n",
    "        val_metric_dict = test(val_loader)#test_loader)\n",
    "\n",
    "        if epoch % n_epoch_for_checkpoint == 0 and epoch != 0:\n",
    "            val_acc = val_metric_dict['accuracy'].numpy()\n",
    "            checkpoitn_filepath = checkpoint_path / Path(f\"{tensorboard_file_name}_epoch_{epoch}\")#_val_acc_{val_acc:.2f}\")\n",
    "            print(f\"Saving off checkpoint {checkpoitn_filepath}\")\n",
    "            mdlu.save_checkpoint(model,filepath = checkpoitn_filepath,epoch = epoch,loss = loss)\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, loss = {loss}')\n",
    "        for type_name,metric_dict in zip([\"train\",\"val\"],[train_metric_dict,val_metric_dict]):\n",
    "            print_log = f\"   {type_name} metrics: \"\n",
    "            for k,v in metric_dict.items():\n",
    "                if log_to_tensorboard:\n",
    "                    writer.add_scalar(f'{type_name}_{k}',v,epoch)\n",
    "                print_log += f\" {k}: {v:4f},\"\n",
    "\n",
    "            print(print_log)\n",
    "        \n",
    "        if val_metric_dict[\"accuracy\"] < 0.0001:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /pytorch_tools/Applications/Cell_Types_GNN/tensorboard --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard_utils as tbu\n",
    "# df_board = df_tensorboard(\"./tensorboard/\",verbose = True)\n",
    "# df_board.query(\"(run=='DiffPooln_hidden_channels_32') and (name=='train_accuracy')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(checkpoint_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_name = \"GCNFlat_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True\"\n",
    "epoch = 95\n",
    "winning_dir = checkpoint_dir / Path(f\"{winning_name}_checkpoints\") \n",
    "winning_filepath = winning_dir / Path(f\"{winning_name}_epoch_{epoch}\")\n",
    "winning_filepath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True_epoch_95 #good one for seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Embedding for all cell types (Can Run in Batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_kwargs_curr = dict(n_hidden_channels = 64,global_pool_type = \"mean\",n_layers = 2)\n",
    "architecture_kwargs = gu.merge_dicts([architecture_kwargs_global,architecture_kwargs_curr])\n",
    "architecture_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(gm,model_name)(\n",
    "    dataset_num_node_features=dataset_num_node_features,\n",
    "    dataset_num_classes=dataset_num_classes,\n",
    "    **architecture_kwargs,\n",
    "    #use_bn=False\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(winning_filepath)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_loader = data_loader_mod(dataset, batch_size=batch_size,shuffle = False)\n",
    "all_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "embeddings = []\n",
    "labels = []\n",
    "for data in tqdm(all_data_loader):#train_loader:  # Iterate in batches over the training dataset.\n",
    "    data = data.to(device)\n",
    "    if model_name == \"DiffPool\":\n",
    "            out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "            #y_true = data.y.reshape(-1,3)\n",
    "    elif model_name == \"TreeLSTM\":\n",
    "        n = data.x.shape[0]\n",
    "        h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        out = model(\n",
    "            data,\n",
    "            h = h,\n",
    "            c = c,\n",
    "            embeddings = data.x\n",
    "            )\n",
    "    else:\n",
    "        out = model(data)\n",
    "\n",
    "    out_array = out.detach().cpu().numpy()\n",
    "    out_labels = data.y.numpy().reshape(-1)\n",
    "    #print(f\"out_array.shape = {out_array.shape}, out_labels.shape = {out_labels.shape}\")\n",
    "    \n",
    "#     if out_array.shape[0] != out_labels.shape[0]:\n",
    "#         raise Exception(\"\")\n",
    "    \n",
    "    embeddings.append(out_array)\n",
    "    labels.append(out_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "embedding_df[\"cell_type\"] = labels\n",
    "\n",
    "import general_utils as gu\n",
    "decoder_map = dict([(v,k) if k is not None else (v,\"Unknown\") for k,v in cell_type_map.items()])\n",
    "\n",
    "import pandas_utils as pu\n",
    "embedding_df[\"cell_type\"] = pu.new_column_from_dict_mapping(embedding_df,decoder_map,column_name = \"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(embedding_df[\"cell_type\"].to_numpy(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_idx = embedding_df.query(\"cell_type != 'Unknown'\").index.to_numpy()\n",
    "labeled_mask_plotting = np.zeros(len(embedding_df))\n",
    "labeled_mask_plotting[labeled_idx] = 1\n",
    "labeled_mask_plotting = labeled_mask_plotting.astype(\"bool\")\n",
    "labeled_mask_plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Plotting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import visualizations_ml as vml\n",
    "n_components = 3\n",
    "import dimensionality_reduction_ml as dru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ml as pdml\n",
    "X_data,y_labels = pdml.X_y(embedding_df,\"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data[labeled_mask_plotting].to_numpy().astype(\"float\")\n",
    "y = y_labels[labeled_mask_plotting].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep) PCA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_components_test=10\n",
    "pca_data = dru.pca_analysis(\n",
    "    X,\n",
    "    n_components=n_components_test,\n",
    "    plot_sqrt_eigvals=False,\n",
    "    plot_perc_variance_explained=True\n",
    ")\n",
    "\n",
    "X_pca = pca_data[\"data_proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/meshAfterParty/meshAfterParty/')\n",
    "import datajoint_utils as du\n",
    "import cell_type_utils as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "import cell_type_utils as ctu\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"pca\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"umap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"umap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"isomap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X_pca[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
