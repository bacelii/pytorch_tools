{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: Implementation fo DiffPool\\ngraph coarsening manner\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: Implementation fo DiffPool\n",
    "graph coarsening manner\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "#sys.path.append(\"/meshAfterParty/meshAfterParty\")\n",
    "sys.path.append(\"/python_tools/python_tools\")\n",
    "sys.path.append(\"/machine_learning_tools/machine_learning_tools/\")\n",
    "sys.path.append(\"/pytorch_tools/pytorch_tools/\")\n",
    "sys.path.append(\"/neuron_morphology_tools/neuron_morphology_tools/\")\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/m65_full/cell_type_fine_with_skeleton'),\n",
       " PosixPath('data/m65_full/df_morphometrics.pbz2'),\n",
       " PosixPath('data/m65_full/cell_type_fine_with_skeleton_no_dense'),\n",
       " PosixPath('data/m65_full/df_cell_type_fine.pbz2')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"./data/m65_full/\")\n",
    "list(data_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python_tools modules\n",
    "import system_utils as su\n",
    "import pandas_utils as pu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_utils as nu\n",
    "import networkx_utils as xu\n",
    "from tqdm_utils import tqdm\n",
    "\n",
    "#neuron_morphology_tools modules\n",
    "import neuron_nx_io as nxio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import transforms\n",
    "\n",
    "# for the dataset object\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import DenseDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_tools modules\n",
    "import preprocessing_utils as pret\n",
    "import geometric_models as gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Choosing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_adj= False, directed = True\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TreeLSTM\"\n",
    "model_class = getattr(gm,model_name)\n",
    "dense_adj = getattr(model_class,\"dense_adj\",False)\n",
    "directed = getattr(model_class,\"directed\",False)\n",
    "print(f\"dense_adj= {dense_adj}, directed = {directed}\")\n",
    "\n",
    "gnn_task = \"cell_type_fine\"\n",
    "label_name = None\n",
    "graph_label = \"cell_type_fine_label\"\n",
    "data_file = \"df_cell_type_fine.pbz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "with_skeleton = True\n",
    "\n",
    "features_to_delete = [\n",
    "    \"mesh_volume\",\n",
    "    \"apical_label\",\n",
    "    \"basal_label\",\n",
    "]\n",
    "\n",
    "if not with_skeleton:\n",
    "    features_to_delete +=[\n",
    "        \"skeleton_vector_downstream_phi\",      \n",
    "        \"skeleton_vector_downstream_theta\",    \n",
    "        \"skeleton_vector_upstream_phi\",        \n",
    "        \"skeleton_vector_upstream_theta\",  \n",
    "    ]\n",
    "\n",
    "features_to_keep = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading the Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>split_index</th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>external_layer</th>\n",
       "      <th>external_visual_area</th>\n",
       "      <th>cell_type_fine</th>\n",
       "      <th>cell_type_fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691134277239760</td>\n",
       "      <td>0</td>\n",
       "      <td>89719</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864691134339067925</td>\n",
       "      <td>0</td>\n",
       "      <td>624899</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>AL</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864691134366116139</td>\n",
       "      <td>0</td>\n",
       "      <td>476756</td>\n",
       "      <td>WHITE_MATTER</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_6'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>864691134378215335</td>\n",
       "      <td>0</td>\n",
       "      <td>3799</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864691134527727930</td>\n",
       "      <td>0</td>\n",
       "      <td>631380</td>\n",
       "      <td>WHITE_MATTER</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_0'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60448</th>\n",
       "      <td>864691137197334593</td>\n",
       "      <td>0</td>\n",
       "      <td>376218</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_5',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60449</th>\n",
       "      <td>864691137197344065</td>\n",
       "      <td>0</td>\n",
       "      <td>191436</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_4', 'L0_6',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60450</th>\n",
       "      <td>864691137197345345</td>\n",
       "      <td>0</td>\n",
       "      <td>584463</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_13', 'L4_5', 'L0_8', 'L2_5'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60451</th>\n",
       "      <td>864691137197353281</td>\n",
       "      <td>0</td>\n",
       "      <td>591241</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L4_6', 'L1_10', 'L3_4', 'L0_10...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60452</th>\n",
       "      <td>864691137197364801</td>\n",
       "      <td>0</td>\n",
       "      <td>488097</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_9', 'L0_8', 'L0_10', 'L0_6'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60453 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               segment_id  split_index  nucleus_id external_layer  \\\n",
       "0      864691134277239760            0       89719        LAYER_6   \n",
       "1      864691134339067925            0      624899        LAYER_6   \n",
       "2      864691134366116139            0      476756   WHITE_MATTER   \n",
       "3      864691134378215335            0        3799      LAYER_2/3   \n",
       "4      864691134527727930            0      631380   WHITE_MATTER   \n",
       "...                   ...          ...         ...            ...   \n",
       "60448  864691137197334593            0      376218        LAYER_6   \n",
       "60449  864691137197344065            0      191436      LAYER_2/3   \n",
       "60450  864691137197345345            0      584463      LAYER_2/3   \n",
       "60451  864691137197353281            0      591241        LAYER_5   \n",
       "60452  864691137197364801            0      488097      LAYER_2/3   \n",
       "\n",
       "      external_visual_area                                     cell_type_fine  \\\n",
       "0                       V1  [{'nodelist': ['L0_0'], 'features': ['mesh_vol...   \n",
       "1                       AL  [{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...   \n",
       "2                       RL  [{'nodelist': ['L0_6'], 'features': ['mesh_vol...   \n",
       "3                       V1  [{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...   \n",
       "4                       RL  [{'nodelist': ['L0_0'], 'features': ['mesh_vol...   \n",
       "...                    ...                                                ...   \n",
       "60448                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_5',...   \n",
       "60449                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_4', 'L0_6',...   \n",
       "60450                   RL  [{'nodelist': ['L0_13', 'L4_5', 'L0_8', 'L2_5'...   \n",
       "60451                   RL  [{'nodelist': ['L4_6', 'L1_10', 'L3_4', 'L0_10...   \n",
       "60452                   RL  [{'nodelist': ['L0_9', 'L0_8', 'L0_10', 'L0_6'...   \n",
       "\n",
       "      cell_type_fine_label  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "...                    ...  \n",
       "60448                  NaN  \n",
       "60449                  NaN  \n",
       "60450                  NaN  \n",
       "60451                  NaN  \n",
       "60452                  NaN  \n",
       "\n",
       "[60453 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filepath = Path(data_path) / Path(data_file)\n",
    "\n",
    "data_df = su.decompress_pickle(data_filepath)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote the cell_type_fine is the column\\nthat has all of the graph data stored\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note the cell_type_fine is the column\n",
    "that has all of the graph data stored\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>split_index</th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>external_layer</th>\n",
       "      <th>external_visual_area</th>\n",
       "      <th>cell_type_fine</th>\n",
       "      <th>cell_type_fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>864691134884748026</td>\n",
       "      <td>0</td>\n",
       "      <td>366181</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_10', 'L0_11', 'L0_12', 'L0_...</td>\n",
       "      <td>4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>864691134884761338</td>\n",
       "      <td>0</td>\n",
       "      <td>458241</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>23P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>864691134884769786</td>\n",
       "      <td>0</td>\n",
       "      <td>592718</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_18', 'L0_11', 'L0_17', 'L0_...</td>\n",
       "      <td>5P_IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>864691134884879610</td>\n",
       "      <td>0</td>\n",
       "      <td>304873</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L2_4', 'L1_1', 'L0_3', 'L1_6',...</td>\n",
       "      <td>IT_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>864691134884945146</td>\n",
       "      <td>0</td>\n",
       "      <td>63499</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>23P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60416</th>\n",
       "      <td>864691137197239105</td>\n",
       "      <td>0</td>\n",
       "      <td>262000</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60417</th>\n",
       "      <td>864691137197241665</td>\n",
       "      <td>0</td>\n",
       "      <td>308938</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L1_4', 'L0_18', 'L4_1', 'L0_16...</td>\n",
       "      <td>Unsure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60433</th>\n",
       "      <td>864691137197306177</td>\n",
       "      <td>0</td>\n",
       "      <td>304611</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_3', 'L0_6', 'L0_0',...</td>\n",
       "      <td>5P_NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60442</th>\n",
       "      <td>864691137197321281</td>\n",
       "      <td>0</td>\n",
       "      <td>434601</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_3', 'L3_3', 'L2_2', 'L2_3',...</td>\n",
       "      <td>6P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60445</th>\n",
       "      <td>864691137197329985</td>\n",
       "      <td>0</td>\n",
       "      <td>260468</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_3', 'L1_79', 'L1_82', 'L0_1...</td>\n",
       "      <td>BPC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3338 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               segment_id  split_index  nucleus_id external_layer  \\\n",
       "115    864691134884748026            0      366181        LAYER_5   \n",
       "147    864691134884761338            0      458241        LAYER_4   \n",
       "170    864691134884769786            0      592718        LAYER_5   \n",
       "205    864691134884879610            0      304873        LAYER_5   \n",
       "213    864691134884945146            0       63499      LAYER_2/3   \n",
       "...                   ...          ...         ...            ...   \n",
       "60416  864691137197239105            0      262000        LAYER_4   \n",
       "60417  864691137197241665            0      308938        LAYER_6   \n",
       "60433  864691137197306177            0      304611        LAYER_5   \n",
       "60442  864691137197321281            0      434601        LAYER_5   \n",
       "60445  864691137197329985            0      260468        LAYER_4   \n",
       "\n",
       "      external_visual_area                                     cell_type_fine  \\\n",
       "115                     V1  [{'nodelist': ['L0_10', 'L0_11', 'L0_12', 'L0_...   \n",
       "147                     RL  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "170                     RL  [{'nodelist': ['L0_18', 'L0_11', 'L0_17', 'L0_...   \n",
       "205                     V1  [{'nodelist': ['L2_4', 'L1_1', 'L0_3', 'L1_6',...   \n",
       "213                     V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "...                    ...                                                ...   \n",
       "60416                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "60417                   V1  [{'nodelist': ['L1_4', 'L0_18', 'L4_1', 'L0_16...   \n",
       "60433                   V1  [{'nodelist': ['L0_1', 'L0_3', 'L0_6', 'L0_0',...   \n",
       "60442                   RL  [{'nodelist': ['L0_3', 'L3_3', 'L2_2', 'L2_3',...   \n",
       "60445                   V1  [{'nodelist': ['L0_3', 'L1_79', 'L1_82', 'L0_1...   \n",
       "\n",
       "      cell_type_fine_label  \n",
       "115                     4P  \n",
       "147                    23P  \n",
       "170                  5P_IT  \n",
       "205               IT_short  \n",
       "213                    23P  \n",
       "...                    ...  \n",
       "60416                   4P  \n",
       "60417               Unsure  \n",
       "60433                5P_NP  \n",
       "60442                   6P  \n",
       "60445                  BPC  \n",
       "\n",
       "[3338 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.query(\"cell_type_fine_label == cell_type_fine_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodelist': array(['L0_1', 'L0_0', 'L0_2'], dtype=object),\n",
       " 'features': array(['mesh_volume', 'n_spines', 'n_synapses_head', 'n_synapses_neck',\n",
       "        'n_synapses_post', 'n_synapses_pre', 'skeletal_length',\n",
       "        'total_spine_volume', 'width_upstream', 'width_downstream',\n",
       "        'apical_label', 'basal_label', 'skeleton_vector_downstream_phi',\n",
       "        'skeleton_vector_downstream_theta', 'skeleton_vector_upstream_phi',\n",
       "        'skeleton_vector_upstream_theta', 'width_no_spine'], dtype=object),\n",
       " 'adjacency': array([[0, 1, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]),\n",
       " 'feature_matrix': array([[ 3.71170715e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.43345470e+03,  0.00000000e+00,  5.91837642e+01,\n",
       "          5.91837642e+01,  0.00000000e+00,  1.00000000e+00,\n",
       "         -6.22637047e-01,  2.00297982e+00, -6.22637047e-01,\n",
       "          2.00297982e+00,  5.91837642e+01],\n",
       "        [ 2.99289539e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          6.76722467e+03,  0.00000000e+00,  4.71136497e+02,\n",
       "          8.69454592e+02,  0.00000000e+00,  1.00000000e+00,\n",
       "          3.01881171e+00,  1.61006762e+00, -2.79826200e+00,\n",
       "          2.06957198e+00,  7.32983276e+02],\n",
       "        [ 8.07215086e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          4.79074407e+03,  0.00000000e+00,  1.97902300e+02,\n",
       "          4.48326427e+02,  0.00000000e+00,  1.00000000e+00,\n",
       "          1.19580679e+00,  7.15696534e-01,  1.01784622e+00,\n",
       "          7.42017670e-01,  3.82383945e+02]]),\n",
       " 'label_name': None,\n",
       " 'graph_label': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = data_df[[\"cell_type_fine\"]].iloc[1].to_list()[0][0]\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating the Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- a) Getting Means and Std Dev for Normalization --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mesh_volume                         1.0\n",
       "n_spines                            1.0\n",
       "n_synapses_head                     1.0\n",
       "n_synapses_neck                     1.0\n",
       "n_synapses_post                     1.0\n",
       "n_synapses_pre                      1.0\n",
       "skeletal_length                     1.0\n",
       "total_spine_volume                  1.0\n",
       "width_upstream                      1.0\n",
       "width_downstream                    1.0\n",
       "apical_label                        1.0\n",
       "basal_label                         1.0\n",
       "skeleton_vector_downstream_phi      1.0\n",
       "skeleton_vector_downstream_theta    1.0\n",
       "skeleton_vector_upstream_phi        1.0\n",
       "skeleton_vector_upstream_theta      1.0\n",
       "width_no_spine                      1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_batch_df = pd.concat([nxio.feature_df_from_gnn_info(\n",
    "    k[0],\n",
    "    return_data_labels_split = False) for k in data_df[gnn_task].to_list()])\n",
    "\n",
    "if label_name is not None:\n",
    "    all_batch_df = all_batch_df[[k for k in \n",
    "            all_batch_df.columns if k not in nu.convert_to_array_like(label_name)]]\n",
    "else:\n",
    "    all_batch_df = all_batch_df\n",
    "    \n",
    "# will use these to normalize the data\n",
    "col_means = all_batch_df.mean(axis=0).to_numpy()\n",
    "col_stds = all_batch_df.std(axis=0).to_numpy()\n",
    "\n",
    "all_batch_df_norm = pu.normalize_df(all_batch_df,column_means=col_means,\n",
    "                                 column_stds = col_stds)\n",
    "all_batch_df_norm.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- b) Creating the Dataset Class --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1P': 1,\n",
       " '23P': 2,\n",
       " '4P': 3,\n",
       " '5P_IT': 4,\n",
       " '5P_NP': 5,\n",
       " '5P_PT': 6,\n",
       " '6CT': 7,\n",
       " '6P': 8,\n",
       " '6P_CT': 9,\n",
       " '6P_IT': 10,\n",
       " '6P_U': 11,\n",
       " 'BC': 12,\n",
       " 'BPC': 13,\n",
       " 'I targeting non_bpc': 14,\n",
       " 'IT_big_tuft': 15,\n",
       " 'IT_short': 16,\n",
       " 'IT_small_tuft': 17,\n",
       " 'Martinotti': 18,\n",
       " 'NGC': 19,\n",
       " 'Pvalb': 20,\n",
       " 'SST': 21,\n",
       " 'Unsure': 22,\n",
       " 'VIP': 23,\n",
       " 'WM_P': 24,\n",
       " 'cb1 basket': 25,\n",
       " 'chandelier': 26,\n",
       " 'l1vip': 27,\n",
       " 'ndnf+npy_': 28,\n",
       " 'ngfc': 29,\n",
       " 'prox targeting': 30,\n",
       " 'small basket': 31,\n",
       " None: 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- mapping of the labels to integers --\n",
    "total_labels,label_counts = np.unique((data_df.query(f\"{graph_label}=={graph_label}\")[\n",
    "    graph_label]).to_numpy(),return_counts = True)\n",
    "cell_type_map = {k:i+1 for i,k in enumerate(total_labels)}\n",
    "cell_type_map[None] = 0\n",
    "cell_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 0.25, 0.3 , 0.5 , 1.  , 0.8 , 1.  , 0.8 , 1.  , 0.8 ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cell_type_map\n",
    "cell_type_fine_classifier_weights = {\n",
    "'23P': 0.25,#1294\n",
    "'4P': 0.3,#890\n",
    "'5P_IT': 0.5,#465\n",
    "'6P': 0.8,#342\n",
    "'6P_IT': 0.8,#263\n",
    "'5P_PT': 0.8,#224\n",
    "}\n",
    "\n",
    "\n",
    "class_idx = np.array(list(cell_type_map.values()) )\n",
    "class_labels = np.array(list(cell_type_map.keys()) )\n",
    "weights = np.array([cell_type_fine_classifier_weights.get(k,1) for k in class_labels])\n",
    "weights = weights[np.argsort(class_idx)]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_data_from_gnn_info(\n",
    "    gnn_info,\n",
    "    y = None,\n",
    "    verbose = False,\n",
    "    normalize = True,\n",
    "    features_to_delete=None,\n",
    "    features_to_keep = None\n",
    "    ): \n",
    "    \"\"\"\n",
    "    Purpose: To convert our data format into pytorch Data object\n",
    "\n",
    "    Pseudocode: \n",
    "    1) Create the edgelist (turn into tensor)\n",
    "    2) Get the \n",
    "    \"\"\"\n",
    "    edgelist = torch.tensor(xu.edgelist_from_adjacency_matrix(\n",
    "        array = gnn_info[\"adjacency\"],\n",
    "        verbose = False,\n",
    "    ).T,dtype=torch.long)\n",
    "\n",
    "    x,y_raw = nxio.feature_df_from_gnn_info(\n",
    "        gnn_info,\n",
    "        return_data_labels_split = True)\n",
    "    if y is None:\n",
    "        y = y_raw\n",
    "        \n",
    "    if not type(y) == str:\n",
    "        y = None\n",
    "        \n",
    "    y_int = np.array(cell_type_map[y] ).reshape(1,-1)\n",
    "    \n",
    "    if normalize:\n",
    "        x = (x-col_means)/col_stds\n",
    "    \n",
    "    # --- keeping or not keeping sertain features\n",
    "    gnn_features = gnn_info[\"features\"]\n",
    "\n",
    "    keep_idx = np.arange(len(gnn_features))\n",
    "    if features_to_delete is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k not in features_to_delete])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER DELETE= {keep_idx}\")\n",
    "    if features_to_keep is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k in features_to_keep])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER KEEP = {keep_idx}\")\n",
    "\n",
    "    x = x[:,keep_idx]\n",
    "\n",
    "    x = torch.tensor(x,dtype=torch.float)\n",
    "    y = torch.tensor(y_int,dtype=torch.long)\n",
    "    \n",
    "    if len(y) > 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "        \n",
    "    if y.shape[0] != 1 or y.shape[1] != 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"x.shape = {x.shape},y.shape ={y.shape}\")\n",
    "\n",
    "    data = Data(x=x,y=y,edge_index=edgelist)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellTypeDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        #return ['some_file_1', 'some_file_2', ...]\n",
    "        return [str(data_filepath.absolute())]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # def download(self):\n",
    "    #     # Download to `self.raw_dir`.\n",
    "    #     download_url(url, self.raw_dir)\n",
    "    #     ...\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        #data_list = [...]\n",
    "\n",
    "#         if data_df is None:\n",
    "#             data_df = su.decompress_pickle(self.raw_file_names[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        data_list = []\n",
    "        for k,y in tqdm(zip(\n",
    "            data_df[gnn_task].to_list(),\n",
    "            data_df[graph_label].to_list())):\n",
    "            \n",
    "\n",
    "            if not type(y) == str:\n",
    "                y = None\n",
    "            \n",
    "            if cell_type_map[y] == 0:\n",
    "                continue\n",
    "            \n",
    "            data_list.append(pytorch_data_from_gnn_info(\n",
    "                k[0],\n",
    "                y=y,\n",
    "                features_to_delete=features_to_delete,\n",
    "                features_to_keep = features_to_keep,\n",
    "                verbose = False))\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list_final = []\n",
    "            for data in data_list:\n",
    "                try:\n",
    "                    if self.pre_filter(data):\n",
    "                        data_list_final.append(data)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}\")\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list_final = []\n",
    "            for j,data in enumerate(data_list):\n",
    "                try:\n",
    "                    curr_t = self.pre_transform(data)\n",
    "                    if curr_t.y.shape[0] != 1 or curr_t.y.shape[1] != 1:\n",
    "                        raise Exception(f\"{j}, data = {curr_t}\")\n",
    "                    data_list_final.append(curr_t)\n",
    "                except:\n",
    "                    continue\n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}, data = {d}\")\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/m65_full/cell_type_fine_with_skeleton_directed')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if with_skeleton:\n",
    "    gnn_task_name = f\"{gnn_task}_with_skeleton\"\n",
    "else:\n",
    "    gnn_task_name = f\"{gnn_task}\"\n",
    "\n",
    "if dense_adj:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}\")#_processed_dense\")\n",
    "elif directed:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_directed\")#_processed_dense\")\n",
    "else:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_no_dense\")#_processed_dense\")\n",
    "    \n",
    "processed_data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     su.rm_dir(processed_data_folder)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "processed_data_folder.mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1549c98c56034fa480a5ab87fbd46bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "max_nodes = np.max(all_batch_df_norm.index.to_numpy()) + 1\n",
    "\n",
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return data.num_nodes <= max_nodes\n",
    "    \n",
    "if dense_adj:\n",
    "    #gets the maximum number of nodes in any of the graphs\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "        T.ToDense(max_nodes),\n",
    "        #transforms.NormalizeFeatures(),\n",
    "    ]\n",
    "    pre_filter = MyFilter()\n",
    "elif directed:\n",
    "    transform_list = []\n",
    "    pre_filter = None\n",
    "else:\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "    ]\n",
    "    \n",
    "    pre_filter = None\n",
    "    \n",
    "\n",
    "transform_norm = transforms.Compose(transform_list)\n",
    "dataset = CellTypeDataset(\n",
    "        processed_data_folder.absolute(),\n",
    "        pre_transform = transform_norm,\n",
    "        pre_filter = pre_filter,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j,d in enumerate(dataset):\n",
    "#     if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "#         raise Exception(f\"{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num_node_features = dataset.num_node_features\n",
    "dataset_num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: CellTypeDataset(3338):\n",
      "====================\n",
      "Number of graphs: 3338\n",
      "Number of features: 14\n",
      "Number of classes: 32\n",
      "\n",
      "Data(x=[49, 14], edge_index=[2, 41], y=[1, 1])\n",
      "=============================================================\n",
      "Number of nodes: 49\n",
      "Number of edges: 41\n",
      "Average node degree: 0.84\n"
     ]
    }
   ],
   "source": [
    "# looking at the dataset\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset_num_node_features}')\n",
    "print(f'Number of classes: {dataset_num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "# print(f'Has self-loops: {data.has_self_loops()}')\n",
    "# print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- c) Splitting the Data into Labeled and unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3338"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_mask = np.array(\n",
    "    [True if k.y[0][0] > 0 else False for k in dataset]\n",
    ").astype('int')\n",
    "dataset_labeled = dataset[np.where(labeled_mask)[0]]\n",
    "len(dataset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_curr = dataset_labeled\n",
    "torch.manual_seed(12345)\n",
    "dataset_curr = dataset_curr.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- d) Split Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: To turn percentages into raw lengths\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: To turn percentages into raw lengths\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size = 667.0 (0.2 %)\n",
      "validation size = 667.0 (0.2 %)\n",
      "train_size = 2004.0\n",
      "data_lengths_with_train = [2004  667  667]\n",
      "Number of training graphs: 2004\n",
      "Number of test graphs: 667\n",
      "Number of val graphs: 667\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "(train_dataset,\n",
    "val_dataset,\n",
    "test_dataset,) = pret.train_val_test_split(\n",
    "    dataset_curr,\n",
    "    return_dict=False,\n",
    "    verbose = True)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "print(f'Number of val graphs: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if wanted to try and add weights\n",
    "# y_train = np.array([int(data.y[0][0].numpy()) for data in train_dataset])\n",
    "# y_train_classes,y_train_count = np.unique(y_train,return_counts = True)\n",
    "# y_train_classes,y_train_count\n",
    "\n",
    "# sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "# weight = {i:1/}\n",
    "# samples_weight = np.array([weight[t] for t in y_train])\n",
    "# samples_weight = torch.from_numpy(samples_weight)\n",
    "# samples_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dense_adj:\n",
    "    data_loader_mod = DenseDataLoader\n",
    "else:\n",
    "    data_loader_mod = DataLoader\n",
    "\n",
    "\n",
    "train_loader = data_loader_mod(train_dataset, batch_size=batch_size,shuffle = True)\n",
    "test_loader = data_loader_mod(test_dataset, batch_size=batch_size,shuffle=False)\n",
    "val_loader = data_loader_mod(val_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3a; Picking the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "\n",
    "import general_utils as gu\n",
    "architecture_kwargs_global = dict(\n",
    "    n_hidden_channels = 32, \n",
    "    cell_type = \"childsum\"\n",
    ")\n",
    "\n",
    "optimizer_kwargs_global = dict(\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "all_run_kwargs = [\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 8)),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 16)),\n",
    "    dict(),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 64),),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 128),),\n",
    "    \n",
    "#     dict(architecture_kwargs = dict(n_pool_layers = 1,n_hidden_channels = 8)),\n",
    "#     dict(architecture_kwargs = dict(n_pool_layers = 1,n_hidden_channels = 32),),\n",
    "#     dict(architecture_kwargs = dict(n_pool_layers = 1,n_hidden_channels = 64),),\n",
    "#     dict(architecture_kwargs = dict(n_pool_layers = 1,n_hidden_channels = 128),),\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_utils as evu\n",
    "import torch.nn.functional as F\n",
    "import model_utils as mdlu\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****------ Running Model Config 0 with following parameters ------****\n",
      "{'n_hidden_channels': 8, 'cell_type': 'childsum', 'lr': 0.01}\n",
      "TreeLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (cell): ChildSumTreeLSTMCell(\n",
      "    (W_iou): Linear(in_features=14, out_features=24, bias=False)\n",
      "    (U_iou): Linear(in_features=8, out_features=24, bias=False)\n",
      "    (U_f): Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "tensorboard_file_name = TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True\n",
      "TreeLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (cell): ChildSumTreeLSTMCell(\n",
      "    (W_iou): Linear(in_features=14, out_features=24, bias=False)\n",
      "    (U_iou): Linear(in_features=8, out_features=24, bias=False)\n",
      "    (U_f): Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/dgl/core.py:79: DGLWarning: The input graph for the user-defined edge function does not contain valid edges\n",
      "  dgl_warning('The input graph for the user-defined edge function ' \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 3  9 10 12 24],y_pred_counts = [1117   47    1    1  838]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 3  9 24],y_pred_counts = [349  17 301]\n",
      "Epoch: 000, loss = 3.4598584175109863\n",
      "   train metrics:  accuracy: 0.122754,\n",
      "   val metrics:  accuracy: 0.094453,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 3  9 10 12 24],y_pred_counts = [1315   28    1    1  659]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 3  9 24],y_pred_counts = [433   8 226]\n",
      "Epoch: 001, loss = 3.3565049171447754\n",
      "   train metrics:  accuracy: 0.143214,\n",
      "   val metrics:  accuracy: 0.124438,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 3  9 10 12 24],y_pred_counts = [1572    9    1    3  419]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 3  9 24],y_pred_counts = [544   2 121]\n",
      "Epoch: 002, loss = 3.446180820465088\n",
      "   train metrics:  accuracy: 0.172156,\n",
      "   val metrics:  accuracy: 0.157421,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3  9 12 24],y_pred_counts = [  61 1829    1    2  111]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 24],y_pred_counts = [ 23 612  32]\n",
      "Epoch: 003, loss = 3.385725975036621\n",
      "   train metrics:  accuracy: 0.210080,\n",
      "   val metrics:  accuracy: 0.199400,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 24],y_pred_counts = [ 394 1605    5]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 24],y_pred_counts = [115 550   2]\n",
      "Epoch: 004, loss = 3.3584933280944824\n",
      "   train metrics:  accuracy: 0.238523,\n",
      "   val metrics:  accuracy: 0.211394,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [ 763 1241]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 24],y_pred_counts = [266 400   1]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_5\n",
      "Epoch: 005, loss = 3.3492391109466553\n",
      "   train metrics:  accuracy: 0.268962,\n",
      "   val metrics:  accuracy: 0.248876,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1047  957]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [362 305]\n",
      "Epoch: 006, loss = 3.293269157409668\n",
      "   train metrics:  accuracy: 0.285429,\n",
      "   val metrics:  accuracy: 0.274363,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1276  728]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [437 230]\n",
      "Epoch: 007, loss = 3.30322527885437\n",
      "   train metrics:  accuracy: 0.290419,\n",
      "   val metrics:  accuracy: 0.284858,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1458  546]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [496 171]\n",
      "Epoch: 008, loss = 3.322920560836792\n",
      "   train metrics:  accuracy: 0.290419,\n",
      "   val metrics:  accuracy: 0.286357,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1602  402]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [539 128]\n",
      "Epoch: 009, loss = 3.3236000537872314\n",
      "   train metrics:  accuracy: 0.294910,\n",
      "   val metrics:  accuracy: 0.295352,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1722  282]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [581  86]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_10\n",
      "Epoch: 010, loss = 3.3203892707824707\n",
      "   train metrics:  accuracy: 0.287924,\n",
      "   val metrics:  accuracy: 0.298351,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1800  204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [600  67]\n",
      "Epoch: 011, loss = 3.2374119758605957\n",
      "   train metrics:  accuracy: 0.291417,\n",
      "   val metrics:  accuracy: 0.296852,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1860  144]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [621  46]\n",
      "Epoch: 012, loss = 3.1719131469726562\n",
      "   train metrics:  accuracy: 0.286926,\n",
      "   val metrics:  accuracy: 0.301349,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1902  102]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [633  34]\n",
      "Epoch: 013, loss = 3.1470351219177246\n",
      "   train metrics:  accuracy: 0.286427,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1931   73]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [645  22]\n",
      "Epoch: 014, loss = 3.202296495437622\n",
      "   train metrics:  accuracy: 0.283433,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1958   46]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [655  12]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_15\n",
      "Epoch: 015, loss = 3.110356092453003\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1978   26]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [658   9]\n",
      "Epoch: 016, loss = 3.2249808311462402\n",
      "   train metrics:  accuracy: 0.282934,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1989   15]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [660   7]\n",
      "Epoch: 017, loss = 3.1712839603424072\n",
      "   train metrics:  accuracy: 0.282435,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1994   10]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [663   4]\n",
      "Epoch: 018, loss = 3.1543431282043457\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1996    8]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [664   3]\n",
      "Epoch: 019, loss = 3.121333360671997\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1998    6]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [666   1]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_20\n",
      "Epoch: 020, loss = 2.9360156059265137\n",
      "   train metrics:  accuracy: 0.281936,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2000    4]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 021, loss = 3.0017318725585938\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2001    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 022, loss = 2.906708002090454\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.305847,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2001    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 023, loss = 2.907601833343506\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2002    2]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 024, loss = 3.114969253540039\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2002    2]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_25\n",
      "Epoch: 025, loss = 2.854461908340454\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2003    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 026, loss = 3.138397455215454\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2003    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 027, loss = 2.9255661964416504\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2003    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 028, loss = 2.9817051887512207\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2003    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 029, loss = 2.6397087574005127\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2002    2]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_30\n",
      "Epoch: 030, loss = 2.9736549854278564\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2001    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 031, loss = 2.905688524246216\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2000    4]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 032, loss = 2.773693323135376\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [2000    4]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2],y_pred_counts = [667]\n",
      "Epoch: 033, loss = 2.6100831031799316\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1994   10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [664   3]\n",
      "Epoch: 034, loss = 2.513437032699585\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1985   19]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [661   6]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_35\n",
      "Epoch: 035, loss = 2.9433791637420654\n",
      "   train metrics:  accuracy: 0.282435,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1947   57]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [649  18]\n",
      "Epoch: 036, loss = 2.739515781402588\n",
      "   train metrics:  accuracy: 0.281936,\n",
      "   val metrics:  accuracy: 0.308846,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1856  148]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [622  45]\n",
      "Epoch: 037, loss = 2.945293664932251\n",
      "   train metrics:  accuracy: 0.284930,\n",
      "   val metrics:  accuracy: 0.299850,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1629  375]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [553 114]\n",
      "Epoch: 038, loss = 2.8688509464263916\n",
      "   train metrics:  accuracy: 0.294411,\n",
      "   val metrics:  accuracy: 0.287856,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1292  712]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [438 229]\n",
      "Epoch: 039, loss = 2.870811700820923\n",
      "   train metrics:  accuracy: 0.292415,\n",
      "   val metrics:  accuracy: 0.284858,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [ 671 1333]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [231 436]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_40\n",
      "Epoch: 040, loss = 2.539362907409668\n",
      "   train metrics:  accuracy: 0.292415,\n",
      "   val metrics:  accuracy: 0.277361,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [  10 1994]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [  6 661]\n",
      "Epoch: 041, loss = 2.6366214752197266\n",
      "   train metrics:  accuracy: 0.231038,\n",
      "   val metrics:  accuracy: 0.214393,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [3],y_pred_counts = [2004]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [  1 666]\n",
      "Epoch: 042, loss = 2.788167953491211\n",
      "   train metrics:  accuracy: 0.228044,\n",
      "   val metrics:  accuracy: 0.208396,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [3],y_pred_counts = [2004]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3],y_pred_counts = [667]\n",
      "Epoch: 043, loss = 2.5938644409179688\n",
      "   train metrics:  accuracy: 0.228044,\n",
      "   val metrics:  accuracy: 0.209895,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [3],y_pred_counts = [2004]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3],y_pred_counts = [667]\n",
      "Epoch: 044, loss = 2.9714972972869873\n",
      "   train metrics:  accuracy: 0.228044,\n",
      "   val metrics:  accuracy: 0.209895,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [   1 2003]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [  1 666]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_45\n",
      "Epoch: 045, loss = 2.579066514968872\n",
      "   train metrics:  accuracy: 0.228543,\n",
      "   val metrics:  accuracy: 0.208396,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [   8 1996]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [  5 662]\n",
      "Epoch: 046, loss = 2.9585678577423096\n",
      "   train metrics:  accuracy: 0.230539,\n",
      "   val metrics:  accuracy: 0.212894,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [   4 2000]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [  3 664]\n",
      "Epoch: 047, loss = 2.894249677658081\n",
      "   train metrics:  accuracy: 0.229541,\n",
      "   val metrics:  accuracy: 0.211394,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [ 141 1863]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [ 45 622]\n",
      "Epoch: 048, loss = 2.8554444313049316\n",
      "   train metrics:  accuracy: 0.257485,\n",
      "   val metrics:  accuracy: 0.244378,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [ 439 1565]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [160 507]\n",
      "Epoch: 049, loss = 2.7787160873413086\n",
      "   train metrics:  accuracy: 0.277944,\n",
      "   val metrics:  accuracy: 0.280360,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [ 722 1282]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [248 419]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_50\n",
      "Epoch: 050, loss = 2.8442156314849854\n",
      "   train metrics:  accuracy: 0.295908,\n",
      "   val metrics:  accuracy: 0.278861,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [ 935 1069]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [320 347]\n",
      "Epoch: 051, loss = 2.5818369388580322\n",
      "   train metrics:  accuracy: 0.291916,\n",
      "   val metrics:  accuracy: 0.287856,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1062  942]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [360 307]\n",
      "Epoch: 052, loss = 2.761650800704956\n",
      "   train metrics:  accuracy: 0.295409,\n",
      "   val metrics:  accuracy: 0.293853,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1123  881]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [378 289]\n",
      "Epoch: 053, loss = 2.7302608489990234\n",
      "   train metrics:  accuracy: 0.297405,\n",
      "   val metrics:  accuracy: 0.290855,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1158  846]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [396 271]\n",
      "Epoch: 054, loss = 3.016044855117798\n",
      "   train metrics:  accuracy: 0.293413,\n",
      "   val metrics:  accuracy: 0.290855,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1232  772]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [426 241]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_55\n",
      "Epoch: 055, loss = 2.6316981315612793\n",
      "   train metrics:  accuracy: 0.292415,\n",
      "   val metrics:  accuracy: 0.287856,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1309  695]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [453 214]\n",
      "Epoch: 056, loss = 2.8868541717529297\n",
      "   train metrics:  accuracy: 0.296906,\n",
      "   val metrics:  accuracy: 0.286357,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [1413  591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [489 178]\n",
      "Epoch: 057, loss = 2.8778419494628906\n",
      "   train metrics:  accuracy: 0.298403,\n",
      "   val metrics:  accuracy: 0.280360,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1608  393    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [533 134]\n",
      "Epoch: 058, loss = 2.9305522441864014\n",
      "   train metrics:  accuracy: 0.287425,\n",
      "   val metrics:  accuracy: 0.290855,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1712  289    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [569  98]\n",
      "Epoch: 059, loss = 3.0467069149017334\n",
      "   train metrics:  accuracy: 0.293413,\n",
      "   val metrics:  accuracy: 0.296852,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1772  229    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [578  89]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_60\n",
      "Epoch: 060, loss = 2.6945996284484863\n",
      "   train metrics:  accuracy: 0.289421,\n",
      "   val metrics:  accuracy: 0.298351,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1771  229    4]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [2 3],y_pred_counts = [580  87]\n",
      "Epoch: 061, loss = 2.698573350906372\n",
      "   train metrics:  accuracy: 0.289421,\n",
      "   val metrics:  accuracy: 0.298351,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1746  252    6]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [568  98   1]\n",
      "Epoch: 062, loss = 2.992302179336548\n",
      "   train metrics:  accuracy: 0.290419,\n",
      "   val metrics:  accuracy: 0.298351,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1721  277    6]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [562 103   2]\n",
      "Epoch: 063, loss = 2.857729911804199\n",
      "   train metrics:  accuracy: 0.287924,\n",
      "   val metrics:  accuracy: 0.299850,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1680  318    6]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [552 112   3]\n",
      "Epoch: 064, loss = 2.8846826553344727\n",
      "   train metrics:  accuracy: 0.292415,\n",
      "   val metrics:  accuracy: 0.296852,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1654  342    8]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [544 120   3]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_65\n",
      "Epoch: 065, loss = 2.786410093307495\n",
      "   train metrics:  accuracy: 0.290918,\n",
      "   val metrics:  accuracy: 0.293853,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1608  384   12]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [535 128   4]\n",
      "Epoch: 066, loss = 2.7465028762817383\n",
      "   train metrics:  accuracy: 0.290918,\n",
      "   val metrics:  accuracy: 0.293853,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1659  333   12]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [544 119   4]\n",
      "Epoch: 067, loss = 2.6185097694396973\n",
      "   train metrics:  accuracy: 0.290419,\n",
      "   val metrics:  accuracy: 0.296852,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1711  280   13]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [559 104   4]\n",
      "Epoch: 068, loss = 2.858431816101074\n",
      "   train metrics:  accuracy: 0.291417,\n",
      "   val metrics:  accuracy: 0.302849,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1731  260   13]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [563  98   6]\n",
      "Epoch: 069, loss = 2.6499054431915283\n",
      "   train metrics:  accuracy: 0.286926,\n",
      "   val metrics:  accuracy: 0.302849,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1755  236   13]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [571  90   6]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_70\n",
      "Epoch: 070, loss = 2.551743268966675\n",
      "   train metrics:  accuracy: 0.285429,\n",
      "   val metrics:  accuracy: 0.304348,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1798  190   16]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [586  75   6]\n",
      "Epoch: 071, loss = 2.663703680038452\n",
      "   train metrics:  accuracy: 0.284431,\n",
      "   val metrics:  accuracy: 0.307346,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1831  154   19]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [600  59   8]\n",
      "Epoch: 072, loss = 2.723069906234741\n",
      "   train metrics:  accuracy: 0.281936,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1856  123   25]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [614  43  10]\n",
      "Epoch: 073, loss = 2.5278501510620117\n",
      "   train metrics:  accuracy: 0.280439,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1878   94   32]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [618  35  14]\n",
      "Epoch: 074, loss = 2.9431469440460205\n",
      "   train metrics:  accuracy: 0.280938,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1896   67   41]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [621  31  15]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_75\n",
      "Epoch: 075, loss = 2.851172685623169\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1899   56   49]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [621  27  19]\n",
      "Epoch: 076, loss = 2.739776134490967\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1899   46   59]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [623  24  20]\n",
      "Epoch: 077, loss = 2.9150002002716064\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1896   45   63]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [622  23  22]\n",
      "Epoch: 078, loss = 2.789240598678589\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1889   41   74]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [621  19  27]\n",
      "Epoch: 079, loss = 2.667714834213257\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [1886   36   82]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [620  15  32]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_80\n",
      "Epoch: 080, loss = 2.513540506362915\n",
      "   train metrics:  accuracy: 0.281437,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1874   42   87    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [618  14  35]\n",
      "Epoch: 081, loss = 2.806198835372925\n",
      "   train metrics:  accuracy: 0.281936,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1870   45   88    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [615  15  37]\n",
      "Epoch: 082, loss = 2.9356319904327393\n",
      "   train metrics:  accuracy: 0.281936,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1866   49   88    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [612  18  37]\n",
      "Epoch: 083, loss = 2.762746572494507\n",
      "   train metrics:  accuracy: 0.281936,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1854   60   88    2]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [611  19  37]\n",
      "Epoch: 084, loss = 2.457273244857788\n",
      "   train metrics:  accuracy: 0.282934,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1845   70   86    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [603  29  35]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_85\n",
      "Epoch: 085, loss = 2.7369115352630615\n",
      "   train metrics:  accuracy: 0.282435,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1822   97   82    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [592  44  31]\n",
      "Epoch: 086, loss = 2.9338595867156982\n",
      "   train metrics:  accuracy: 0.282435,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1773  153   73    5]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [577  63  27]\n",
      "Epoch: 087, loss = 2.5143988132476807\n",
      "   train metrics:  accuracy: 0.283932,\n",
      "   val metrics:  accuracy: 0.302849,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1718  218   62    6]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10],y_pred_counts = [562  82  23]\n",
      "Epoch: 088, loss = 2.712228536605835\n",
      "   train metrics:  accuracy: 0.285928,\n",
      "   val metrics:  accuracy: 0.302849,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1670  270   57    7]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [544 101  20   2]\n",
      "Epoch: 089, loss = 2.6444053649902344\n",
      "   train metrics:  accuracy: 0.286926,\n",
      "   val metrics:  accuracy: 0.305847,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1564  379   50   11]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [518 130  16   3]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_90\n",
      "Epoch: 090, loss = 2.7665672302246094\n",
      "   train metrics:  accuracy: 0.290419,\n",
      "   val metrics:  accuracy: 0.305847,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1493  449   50   12]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [498 150  16   3]\n",
      "Epoch: 091, loss = 2.669142246246338\n",
      "   train metrics:  accuracy: 0.288423,\n",
      "   val metrics:  accuracy: 0.295352,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1430  507   55   12]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [474 171  19   3]\n",
      "Epoch: 092, loss = 2.710853338241577\n",
      "   train metrics:  accuracy: 0.292914,\n",
      "   val metrics:  accuracy: 0.299850,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1352  580   59   13]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [458 186  19   4]\n",
      "Epoch: 093, loss = 2.568258762359619\n",
      "   train metrics:  accuracy: 0.295908,\n",
      "   val metrics:  accuracy: 0.293853,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1238  693   59   14]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [429 212  22   4]\n",
      "Epoch: 094, loss = 2.6197354793548584\n",
      "   train metrics:  accuracy: 0.303393,\n",
      "   val metrics:  accuracy: 0.295352,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1161  762   66   15]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [403 236  24   4]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_8_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_95\n",
      "Epoch: 095, loss = 2.755106210708618\n",
      "   train metrics:  accuracy: 0.301896,\n",
      "   val metrics:  accuracy: 0.301349,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1114  808   67   15]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [376 263  23   5]\n",
      "Epoch: 096, loss = 2.5984697341918945\n",
      "   train metrics:  accuracy: 0.306387,\n",
      "   val metrics:  accuracy: 0.298351,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1100  822   67   15]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [367 271  24   5]\n",
      "Epoch: 097, loss = 2.5705950260162354\n",
      "   train metrics:  accuracy: 0.305888,\n",
      "   val metrics:  accuracy: 0.299850,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1144  776   69   15]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [396 241  25   5]\n",
      "Epoch: 098, loss = 2.6741740703582764\n",
      "   train metrics:  accuracy: 0.304391,\n",
      "   val metrics:  accuracy: 0.302849,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [1158  756   74   16]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 2  3 10 12],y_pred_counts = [399 235  27   6]\n",
      "Epoch: 099, loss = 2.6482632160186768\n",
      "   train metrics:  accuracy: 0.302894,\n",
      "   val metrics:  accuracy: 0.302849,\n",
      "\n",
      "\n",
      "\n",
      "****------ Running Model Config 1 with following parameters ------****\n",
      "{'n_hidden_channels': 16, 'cell_type': 'childsum', 'lr': 0.01}\n",
      "TreeLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (cell): ChildSumTreeLSTMCell(\n",
      "    (W_iou): Linear(in_features=14, out_features=48, bias=False)\n",
      "    (U_iou): Linear(in_features=16, out_features=48, bias=False)\n",
      "    (U_f): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      ")\n",
      "tensorboard_file_name = TreeLSTM_n_hidden_channels_16_cell_type_childsum_lr_0.01_with_skeleton_True\n",
      "TreeLSTM(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (cell): ChildSumTreeLSTMCell(\n",
      "    (W_iou): Linear(in_features=14, out_features=48, bias=False)\n",
      "    (U_iou): Linear(in_features=16, out_features=48, bias=False)\n",
      "    (U_f): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      ")\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 0  4  5 17 24 27],y_pred_counts = [ 104    5   87 1762   36   10]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 0  4  5 17 24 27],y_pred_counts = [ 22   1  36 592  13   3]\n",
      "Epoch: 000, loss = 3.491222620010376\n",
      "   train metrics:  accuracy: 0.009481,\n",
      "   val metrics:  accuracy: 0.005997,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 0  3  4  5 17 24 27],y_pred_counts = [  82    1  437   43 1412   27    2]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 0  3  4  5 17 24 27],y_pred_counts = [ 15   1 148  24 467  10   2]\n",
      "Epoch: 001, loss = 3.5047874450683594\n",
      "   train metrics:  accuracy: 0.036427,\n",
      "   val metrics:  accuracy: 0.026987,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 0  3  4  5 16 17 24 27],y_pred_counts = [  29    4 1783   10    1  164   12    1]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 0  3  4  5 17 24 27],y_pred_counts = [ 10   2 578   9  65   2   1]\n",
      "Epoch: 002, loss = 3.4650630950927734\n",
      "   train metrics:  accuracy: 0.101297,\n",
      "   val metrics:  accuracy: 0.100450,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [ 0  3  4  5 16 17 24],y_pred_counts = [   7   13 1965    1    1   14    3]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [ 0  3  4 17],y_pred_counts = [  3   6 650   8]\n",
      "Epoch: 003, loss = 3.4328348636627197\n",
      "   train metrics:  accuracy: 0.112774,\n",
      "   val metrics:  accuracy: 0.109445,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [0 3 4],y_pred_counts = [   2   10 1992]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3 4],y_pred_counts = [  6 661]\n",
      "Epoch: 004, loss = 3.404658555984497\n",
      "   train metrics:  accuracy: 0.113273,\n",
      "   val metrics:  accuracy: 0.110945,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [0 3 4],y_pred_counts = [   2    7 1995]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3 4],y_pred_counts = [  4 663]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_16_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_16_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_5\n",
      "Epoch: 005, loss = 3.418506383895874\n",
      "   train metrics:  accuracy: 0.112275,\n",
      "   val metrics:  accuracy: 0.110945,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [3 4],y_pred_counts = [   4 2000]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3 4 8],y_pred_counts = [  3 663   1]\n",
      "Epoch: 006, loss = 3.349351167678833\n",
      "   train metrics:  accuracy: 0.110778,\n",
      "   val metrics:  accuracy: 0.109445,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [3 4],y_pred_counts = [   1 2003]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3 4 8],y_pred_counts = [  2 664   1]\n",
      "Epoch: 007, loss = 3.3106608390808105\n",
      "   train metrics:  accuracy: 0.110778,\n",
      "   val metrics:  accuracy: 0.110945,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [1953   51]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3 4 8],y_pred_counts = [  2 650  15]\n",
      "Epoch: 008, loss = 3.257871150970459\n",
      "   train metrics:  accuracy: 0.109780,\n",
      "   val metrics:  accuracy: 0.112444,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [1526  478]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [3 4 8],y_pred_counts = [  1 495 171]\n",
      "Epoch: 009, loss = 3.2975661754608154\n",
      "   train metrics:  accuracy: 0.095309,\n",
      "   val metrics:  accuracy: 0.098951,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [1201  803]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [394 273]\n",
      "Saving off checkpoint model_checkpoints/TreeLSTM/TreeLSTM_n_hidden_channels_16_cell_type_childsum_lr_0.01_with_skeleton_True_checkpoints/TreeLSTM_n_hidden_channels_16_cell_type_childsum_lr_0.01_with_skeleton_True_epoch_10\n",
      "Epoch: 010, loss = 3.188014030456543\n",
      "   train metrics:  accuracy: 0.089820,\n",
      "   val metrics:  accuracy: 0.092954,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [ 933 1071]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [293 374]\n",
      "Epoch: 011, loss = 3.2105114459991455\n",
      "   train metrics:  accuracy: 0.091816,\n",
      "   val metrics:  accuracy: 0.092954,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [ 795 1209]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [244 423]\n",
      "Epoch: 012, loss = 3.123161792755127\n",
      "   train metrics:  accuracy: 0.090818,\n",
      "   val metrics:  accuracy: 0.091454,\n",
      "   y_true_unique= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 26 27 28 29 30 31],y_true_counts = [ 10 564 457 222  46  76  15 149  67 128  10  44  36   9  16  18  22  47\n",
      "   5   2  20  14   3   8   1   2   6   5   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [ 672 1332]\n",
      "   y_true_unique= [ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 28 29],y_true_counts = [  1 204 140  75  11  33  47  22  35   4  17  15   1   8  10   4  13   5\n",
      "   2   7   4   2   3   2   1   1]\n",
      "   y_pred_unique= [4 8],y_pred_counts = [206 461]\n",
      "Epoch: 013, loss = 2.9362895488739014\n",
      "   train metrics:  accuracy: 0.087824,\n",
      "   val metrics:  accuracy: 0.092954,\n"
     ]
    }
   ],
   "source": [
    "for j,config_dict in enumerate(all_run_kwargs):\n",
    "    \n",
    "    architecture_kwargs = config_dict.get(\"architecture_kwargs\",dict())\n",
    "    optimizer_kwargs = config_dict.get(\"optimizer_kwargs\",dict())\n",
    "    \n",
    "    architecture_kwargs = gu.merge_dicts([architecture_kwargs_global.copy(),architecture_kwargs])\n",
    "    optimizer_kwargs = gu.merge_dicts([optimizer_kwargs_global.copy(),optimizer_kwargs])\n",
    "    \n",
    "    run_kwargs = gu.merge_dicts([architecture_kwargs,optimizer_kwargs])\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\\n\\n****------ Running Model Config {j} with following parameters ------****\\n{run_kwargs}\")\n",
    "\n",
    "    model = getattr(gm,model_name)(\n",
    "        dataset_num_node_features=dataset_num_node_features,\n",
    "        dataset_num_classes=dataset_num_classes,\n",
    "        **architecture_kwargs\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **optimizer_kwargs)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------------- Configuring the Tensorboard and Checkpoinns--------------------\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    tensorboard_dir = Path(\"./tensorboard\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "    tensorboard_dir = tensorboard_dir / Path(f\"{model_name}\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    tensorboard_file_name = (f'{model_name}_' \n",
    "                             + \"_\".join([f\"{k}_{v}\" for k,v in run_kwargs.items()]))\n",
    "    tensorboard_file_name += f\"_with_skeleton_{with_skeleton}\"\n",
    "    print(f\"tensorboard_file_name = {tensorboard_file_name}\")\n",
    "    tensorboard_file = tensorboard_dir / Path(f'{tensorboard_file_name}')\n",
    "    try:\n",
    "        su.rm_dir(tensorboard_file)\n",
    "    except:\n",
    "        pass\n",
    "    tensorboard_file.mkdir(exist_ok = True)\n",
    "\n",
    "\n",
    "    #-- when to save a checkpoint of the model\n",
    "    checkpoint_dir = Path(\"./model_checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_dir = checkpoint_dir / Path(f\"{model_name}\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_path = checkpoint_dir / Path(f\"./{tensorboard_file_name}_checkpoints\")\n",
    "\n",
    "    try:\n",
    "        su.rm_dir(checkpoint_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    checkpoint_path.mkdir(exist_ok = True)\n",
    "    n_epoch_for_checkpoint = 5\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_file)\n",
    "    \n",
    "\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    # weights = weight#[0.1,0.5,,1,0.7,1,1,1]\n",
    "    # class_weights = None\n",
    "    class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "    tensor_map = None\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:#train_loader:  # Iterate in batches over the training dataset.\n",
    "            #print(f\"data = {data}\")\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPoolGCN\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "            y_true = data.y.squeeze_()\n",
    "            #print(f\"out.shape = {out.shape}, data.y.shape = {data.y.shape}\")\n",
    "            loss = F.nll_loss(\n",
    "                torch.log(out), y_true,\n",
    "                weight = class_weights,\n",
    "            )  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            return loss\n",
    "\n",
    "\n",
    "    def test(loader,verbose = False):\n",
    "        model.eval()\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPoolGCN\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "\n",
    "            y_pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            y_true = data.y.squeeze_()\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y_true)\n",
    "    #         error_idx = np.where(pred > 0)[0]\n",
    "    #         if len(error_idx) > 0:\n",
    "    #             print(f\"error_idx = {error_idx}\")\n",
    "        y_pred = torch.cat(y_pred_list)\n",
    "        y_true = torch.cat(y_true_list)\n",
    "\n",
    "        return evu.metric_dict(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            tensor_map=tensor_map,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "    log_to_tensorboard = True\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss = train()\n",
    "        writer.add_scalar('loss',loss,epoch) # new line\n",
    "        train_metric_dict = test(train_loader)#train_loader)\n",
    "        val_metric_dict = test(val_loader)#test_loader)\n",
    "\n",
    "        if epoch % n_epoch_for_checkpoint == 0 and epoch != 0:\n",
    "            val_acc = val_metric_dict['accuracy'].numpy()\n",
    "            checkpoitn_filepath = checkpoint_path / Path(f\"{tensorboard_file_name}_epoch_{epoch}\")#_val_acc_{val_acc:.2f}\")\n",
    "            print(f\"Saving off checkpoint {checkpoitn_filepath}\")\n",
    "            mdlu.save_checkpoint(model,filepath = checkpoitn_filepath,epoch = epoch,loss = loss)\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, loss = {loss}')\n",
    "        for type_name,metric_dict in zip([\"train\",\"val\"],[train_metric_dict,val_metric_dict]):\n",
    "            print_log = f\"   {type_name} metrics: \"\n",
    "            for k,v in metric_dict.items():\n",
    "                if log_to_tensorboard:\n",
    "                    writer.add_scalar(f'{type_name}_{k}',v,epoch)\n",
    "                print_log += f\" {k}: {v:4f},\"\n",
    "\n",
    "            print(print_log)\n",
    "        \n",
    "        if val_metric_dict[\"accuracy\"] < 0.0001:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /pytorch_tools/Applications/Cell_Types_GNN/tensorboard --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard_utils as tbu\n",
    "# df_board = df_tensorboard(\"./tensorboard/\",verbose = True)\n",
    "# df_board.query(\"(run=='DiffPooln_hidden_channels_32') and (name=='train_accuracy')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(checkpoint_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_name = \"GCNFlat_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True\"\n",
    "epoch = 95\n",
    "winning_dir = checkpoint_dir / Path(f\"{winning_name}_checkpoints\") \n",
    "winning_filepath = winning_dir / Path(f\"{winning_name}_epoch_{epoch}\")\n",
    "winning_filepath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True_epoch_95 #good one for seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Embedding for all cell types (Can Run in Batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_kwargs_curr = dict(n_hidden_channels = 64,global_pool_type = \"mean\",n_layers = 2)\n",
    "architecture_kwargs = gu.merge_dicts([architecture_kwargs_global,architecture_kwargs_curr])\n",
    "architecture_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(gm,model_name)(\n",
    "    dataset_num_node_features=dataset_num_node_features,\n",
    "    dataset_num_classes=dataset_num_classes,\n",
    "    **architecture_kwargs,\n",
    "    #use_bn=False\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(winning_filepath)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_loader = data_loader_mod(dataset, batch_size=batch_size,shuffle = False)\n",
    "all_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "embeddings = []\n",
    "labels = []\n",
    "for data in tqdm(all_data_loader):#train_loader:  # Iterate in batches over the training dataset.\n",
    "    data = data.to(device)\n",
    "    if model_name == \"DiffPoolGCN\":\n",
    "            out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "            #y_true = data.y.reshape(-1,3)\n",
    "    elif model_name == \"TreeLSTM\":\n",
    "        n = data.x.shape[0]\n",
    "        h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        out = model(\n",
    "            data,\n",
    "            h = h,\n",
    "            c = c,\n",
    "            embeddings = data.x\n",
    "            )\n",
    "    else:\n",
    "        out = model(data)\n",
    "\n",
    "    out_array = out.detach().cpu().numpy()\n",
    "    out_labels = data.y.numpy().reshape(-1)\n",
    "    #print(f\"out_array.shape = {out_array.shape}, out_labels.shape = {out_labels.shape}\")\n",
    "    \n",
    "#     if out_array.shape[0] != out_labels.shape[0]:\n",
    "#         raise Exception(\"\")\n",
    "    \n",
    "    embeddings.append(out_array)\n",
    "    labels.append(out_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "embedding_df[\"cell_type\"] = labels\n",
    "\n",
    "import general_utils as gu\n",
    "decoder_map = dict([(v,k) if k is not None else (v,\"Unknown\") for k,v in cell_type_map.items()])\n",
    "\n",
    "import pandas_utils as pu\n",
    "embedding_df[\"cell_type\"] = pu.new_column_from_dict_mapping(embedding_df,decoder_map,column_name = \"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(embedding_df[\"cell_type\"].to_numpy(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_idx = embedding_df.query(\"cell_type != 'Unknown'\").index.to_numpy()\n",
    "labeled_mask_plotting = np.zeros(len(embedding_df))\n",
    "labeled_mask_plotting[labeled_idx] = 1\n",
    "labeled_mask_plotting = labeled_mask_plotting.astype(\"bool\")\n",
    "labeled_mask_plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Plotting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import visualizations_ml as vml\n",
    "n_components = 3\n",
    "import dimensionality_reduction_ml as dru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ml as pdml\n",
    "X_data,y_labels = pdml.X_y(embedding_df,\"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data[labeled_mask_plotting].to_numpy().astype(\"float\")\n",
    "y = y_labels[labeled_mask_plotting].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep) PCA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_components_test=10\n",
    "pca_data = dru.pca_analysis(\n",
    "    X,\n",
    "    n_components=n_components_test,\n",
    "    plot_sqrt_eigvals=False,\n",
    "    plot_perc_variance_explained=True\n",
    ")\n",
    "\n",
    "X_pca = pca_data[\"data_proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/meshAfterParty/meshAfterParty/')\n",
    "import datajoint_utils as du\n",
    "import cell_type_utils as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "import cell_type_utils as ctu\n",
    "vml.plot_dim_red_analysis(\n",
    "    X=X_input,\n",
    "    y=y_input,\n",
    "    method = \"pca\",\n",
    "    color_mapppings = [\n",
    "        ctu.cell_type_fine_color_map,\n",
    "        ctu.e_i_color_dict()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input=X[y!= \"Unknown\"]\n",
    "y_input = y[y != \"Unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) UMAP (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vml.plot_dim_red_analysis(\n",
    "    X=X_input,\n",
    "    y=y_input,\n",
    "    method = \"umap\",\n",
    "    color_mapppings = [\n",
    "        ctu.cell_type_fine_color_map,\n",
    "        ctu.e_i_color_dict()],\n",
    "    min_dist = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) UMAP (0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/meshAfterParty/meshAfterParty/\")\n",
    "import datajoint_utils as du\n",
    "import cell_type_utils as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vml.plot_dim_red_analysis(\n",
    "    X=X_input,\n",
    "    y=y_input,\n",
    "    method = \"umap\",\n",
    "    color_mapppings = [\n",
    "        ctu.cell_type_fine_color_map,\n",
    "        ctu.e_i_color_dict()],\n",
    "    min_dist = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vml.plot_dim_red_analysis(\n",
    "    X=X_input,\n",
    "    y=y_input,\n",
    "    method = \"tsne\",\n",
    "    color_mapppings = [\n",
    "        ctu.cell_type_fine_color_map,\n",
    "        ctu.e_i_color_dict()],\n",
    "    #min_dist = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
