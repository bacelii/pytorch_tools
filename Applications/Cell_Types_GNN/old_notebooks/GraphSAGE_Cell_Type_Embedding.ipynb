{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Implementation fo DiffPool\n",
    "graph coarsening manner\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import trimesh\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from os import sys\n",
    "#sys.path.append(\"/meshAfterParty/meshAfterParty\")\n",
    "sys.path.append(\"/python_tools/python_tools\")\n",
    "sys.path.append(\"/machine_learning_tools/machine_learning_tools/\")\n",
    "sys.path.append(\"/pytorch_tools/pytorch_tools/\")\n",
    "sys.path.append(\"/neuron_morphology_tools/neuron_morphology_tools/\")\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/m65_full/df_cell_type_fine_limbs_dist_thresh_exc.pbz2'),\n",
       " PosixPath('data/m65_full/df_morphometrics.pbz2'),\n",
       " PosixPath('data/m65_full/df_cell_type_fine_limbs.pbz2'),\n",
       " PosixPath('data/m65_full/cell_type_fine_with_skeleton_no_dense'),\n",
       " PosixPath('data/m65_full/cell_type_fine_no_dense'),\n",
       " PosixPath('data/m65_full/df_cell_type_fine_limbs_dist_thresh_inh.pbz2'),\n",
       " PosixPath('data/m65_full/df_cell_type_fine.pbz2')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"./data/m65_full/\")\n",
    "list(data_path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python_tools modules\n",
    "import system_utils as su\n",
    "import pandas_utils as pu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_utils as nu\n",
    "import networkx_utils as xu\n",
    "from tqdm_utils import tqdm\n",
    "\n",
    "#neuron_morphology_tools modules\n",
    "import neuron_nx_io as nxio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric import transforms\n",
    "\n",
    "# for the dataset object\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import DenseDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_tools modules\n",
    "import preprocessing_utils as pret\n",
    "import geometric_models as gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Choosing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}\")\n",
    "\n",
    "with_skeleton = True\n",
    "\n",
    "features_to_delete = [\n",
    "    \"mesh_volume\",\n",
    "    \"apical_label\",\n",
    "    \"basal_label\",\n",
    "]\n",
    "\n",
    "if not with_skeleton:\n",
    "    features_to_delete +=[\n",
    "        \"skeleton_vector_downstream_phi\",      \n",
    "        \"skeleton_vector_downstream_theta\",    \n",
    "        \"skeleton_vector_upstream_phi\",        \n",
    "        \"skeleton_vector_upstream_theta\",  \n",
    "    ]\n",
    "\n",
    "features_to_keep = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading the Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_adj= True, directed = False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_adj= True, directed = False\n"
     ]
    }
   ],
   "source": [
    "model_name = \"GraphSAGE\"\n",
    "model_class = getattr(gm,model_name)\n",
    "dense_adj = getattr(model_class,\"dense_adj\",False)\n",
    "directed = getattr(model_class,\"directed\",False)\n",
    "print(f\"dense_adj= {dense_adj}, directed = {directed}\")\n",
    "\n",
    "gnn_task = \"cell_type_fine\"\n",
    "label_name = None\n",
    "graph_label = \"cell_type_fine_label\"\n",
    "data_file = \"df_cell_type_fine.pbz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>split_index</th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>external_layer</th>\n",
       "      <th>external_visual_area</th>\n",
       "      <th>cell_type_fine</th>\n",
       "      <th>cell_type_fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691134277239760</td>\n",
       "      <td>0</td>\n",
       "      <td>89719</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864691134339067925</td>\n",
       "      <td>0</td>\n",
       "      <td>624899</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>AL</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864691134366116139</td>\n",
       "      <td>0</td>\n",
       "      <td>476756</td>\n",
       "      <td>WHITE_MATTER</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_6'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>864691134378215335</td>\n",
       "      <td>0</td>\n",
       "      <td>3799</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864691134527727930</td>\n",
       "      <td>0</td>\n",
       "      <td>631380</td>\n",
       "      <td>WHITE_MATTER</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_0'], 'features': ['mesh_vol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60448</th>\n",
       "      <td>864691137197334593</td>\n",
       "      <td>0</td>\n",
       "      <td>376218</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_5',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60449</th>\n",
       "      <td>864691137197344065</td>\n",
       "      <td>0</td>\n",
       "      <td>191436</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_4', 'L0_6',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60450</th>\n",
       "      <td>864691137197345345</td>\n",
       "      <td>0</td>\n",
       "      <td>584463</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_13', 'L4_5', 'L0_8', 'L2_5'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60451</th>\n",
       "      <td>864691137197353281</td>\n",
       "      <td>0</td>\n",
       "      <td>591241</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L4_6', 'L1_10', 'L3_4', 'L0_10...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60452</th>\n",
       "      <td>864691137197364801</td>\n",
       "      <td>0</td>\n",
       "      <td>488097</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_9', 'L0_8', 'L0_10', 'L0_6'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60453 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               segment_id  split_index  nucleus_id external_layer  \\\n",
       "0      864691134277239760            0       89719        LAYER_6   \n",
       "1      864691134339067925            0      624899        LAYER_6   \n",
       "2      864691134366116139            0      476756   WHITE_MATTER   \n",
       "3      864691134378215335            0        3799      LAYER_2/3   \n",
       "4      864691134527727930            0      631380   WHITE_MATTER   \n",
       "...                   ...          ...         ...            ...   \n",
       "60448  864691137197334593            0      376218        LAYER_6   \n",
       "60449  864691137197344065            0      191436      LAYER_2/3   \n",
       "60450  864691137197345345            0      584463      LAYER_2/3   \n",
       "60451  864691137197353281            0      591241        LAYER_5   \n",
       "60452  864691137197364801            0      488097      LAYER_2/3   \n",
       "\n",
       "      external_visual_area                                     cell_type_fine  \\\n",
       "0                       V1  [{'nodelist': ['L0_0'], 'features': ['mesh_vol...   \n",
       "1                       AL  [{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...   \n",
       "2                       RL  [{'nodelist': ['L0_6'], 'features': ['mesh_vol...   \n",
       "3                       V1  [{'nodelist': ['L0_1', 'L0_0', 'L0_2'], 'featu...   \n",
       "4                       RL  [{'nodelist': ['L0_0'], 'features': ['mesh_vol...   \n",
       "...                    ...                                                ...   \n",
       "60448                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_5',...   \n",
       "60449                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_4', 'L0_6',...   \n",
       "60450                   RL  [{'nodelist': ['L0_13', 'L4_5', 'L0_8', 'L2_5'...   \n",
       "60451                   RL  [{'nodelist': ['L4_6', 'L1_10', 'L3_4', 'L0_10...   \n",
       "60452                   RL  [{'nodelist': ['L0_9', 'L0_8', 'L0_10', 'L0_6'...   \n",
       "\n",
       "      cell_type_fine_label  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "...                    ...  \n",
       "60448                  NaN  \n",
       "60449                  NaN  \n",
       "60450                  NaN  \n",
       "60451                  NaN  \n",
       "60452                  NaN  \n",
       "\n",
       "[60453 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filepath = Path(data_path) / Path(data_file)\n",
    "\n",
    "data_df = su.decompress_pickle(data_filepath)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote the cell_type_fine is the column\\nthat has all of the graph data stored\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note the cell_type_fine is the column\n",
    "that has all of the graph data stored\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>split_index</th>\n",
       "      <th>nucleus_id</th>\n",
       "      <th>external_layer</th>\n",
       "      <th>external_visual_area</th>\n",
       "      <th>cell_type_fine</th>\n",
       "      <th>cell_type_fine_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>864691134884748026</td>\n",
       "      <td>0</td>\n",
       "      <td>366181</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_10', 'L0_11', 'L0_12', 'L0_...</td>\n",
       "      <td>4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>864691134884761338</td>\n",
       "      <td>0</td>\n",
       "      <td>458241</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>23P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>864691134884769786</td>\n",
       "      <td>0</td>\n",
       "      <td>592718</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_18', 'L0_11', 'L0_17', 'L0_...</td>\n",
       "      <td>5P_IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>864691134884879610</td>\n",
       "      <td>0</td>\n",
       "      <td>304873</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L2_4', 'L1_1', 'L0_3', 'L1_6',...</td>\n",
       "      <td>IT_short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>864691134884945146</td>\n",
       "      <td>0</td>\n",
       "      <td>63499</td>\n",
       "      <td>LAYER_2/3</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>23P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60416</th>\n",
       "      <td>864691137197239105</td>\n",
       "      <td>0</td>\n",
       "      <td>262000</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...</td>\n",
       "      <td>4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60417</th>\n",
       "      <td>864691137197241665</td>\n",
       "      <td>0</td>\n",
       "      <td>308938</td>\n",
       "      <td>LAYER_6</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L1_4', 'L0_18', 'L4_1', 'L0_16...</td>\n",
       "      <td>Unsure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60433</th>\n",
       "      <td>864691137197306177</td>\n",
       "      <td>0</td>\n",
       "      <td>304611</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_1', 'L0_3', 'L0_6', 'L0_0',...</td>\n",
       "      <td>5P_NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60442</th>\n",
       "      <td>864691137197321281</td>\n",
       "      <td>0</td>\n",
       "      <td>434601</td>\n",
       "      <td>LAYER_5</td>\n",
       "      <td>RL</td>\n",
       "      <td>[{'nodelist': ['L0_3', 'L3_3', 'L2_2', 'L2_3',...</td>\n",
       "      <td>6P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60445</th>\n",
       "      <td>864691137197329985</td>\n",
       "      <td>0</td>\n",
       "      <td>260468</td>\n",
       "      <td>LAYER_4</td>\n",
       "      <td>V1</td>\n",
       "      <td>[{'nodelist': ['L0_3', 'L1_79', 'L1_82', 'L0_1...</td>\n",
       "      <td>BPC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3338 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               segment_id  split_index  nucleus_id external_layer  \\\n",
       "115    864691134884748026            0      366181        LAYER_5   \n",
       "147    864691134884761338            0      458241        LAYER_4   \n",
       "170    864691134884769786            0      592718        LAYER_5   \n",
       "205    864691134884879610            0      304873        LAYER_5   \n",
       "213    864691134884945146            0       63499      LAYER_2/3   \n",
       "...                   ...          ...         ...            ...   \n",
       "60416  864691137197239105            0      262000        LAYER_4   \n",
       "60417  864691137197241665            0      308938        LAYER_6   \n",
       "60433  864691137197306177            0      304611        LAYER_5   \n",
       "60442  864691137197321281            0      434601        LAYER_5   \n",
       "60445  864691137197329985            0      260468        LAYER_4   \n",
       "\n",
       "      external_visual_area                                     cell_type_fine  \\\n",
       "115                     V1  [{'nodelist': ['L0_10', 'L0_11', 'L0_12', 'L0_...   \n",
       "147                     RL  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "170                     RL  [{'nodelist': ['L0_18', 'L0_11', 'L0_17', 'L0_...   \n",
       "205                     V1  [{'nodelist': ['L2_4', 'L1_1', 'L0_3', 'L1_6',...   \n",
       "213                     V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "...                    ...                                                ...   \n",
       "60416                   V1  [{'nodelist': ['L0_0', 'L0_1', 'L0_2', 'L0_3',...   \n",
       "60417                   V1  [{'nodelist': ['L1_4', 'L0_18', 'L4_1', 'L0_16...   \n",
       "60433                   V1  [{'nodelist': ['L0_1', 'L0_3', 'L0_6', 'L0_0',...   \n",
       "60442                   RL  [{'nodelist': ['L0_3', 'L3_3', 'L2_2', 'L2_3',...   \n",
       "60445                   V1  [{'nodelist': ['L0_3', 'L1_79', 'L1_82', 'L0_1...   \n",
       "\n",
       "      cell_type_fine_label  \n",
       "115                     4P  \n",
       "147                    23P  \n",
       "170                  5P_IT  \n",
       "205               IT_short  \n",
       "213                    23P  \n",
       "...                    ...  \n",
       "60416                   4P  \n",
       "60417               Unsure  \n",
       "60433                5P_NP  \n",
       "60442                   6P  \n",
       "60445                  BPC  \n",
       "\n",
       "[3338 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.query(\"cell_type_fine_label == cell_type_fine_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodelist': array(['L0_1', 'L0_0', 'L0_2'], dtype=object),\n",
       " 'features': array(['mesh_volume', 'n_spines', 'n_synapses_head', 'n_synapses_neck',\n",
       "        'n_synapses_post', 'n_synapses_pre', 'skeletal_length',\n",
       "        'total_spine_volume', 'width_upstream', 'width_downstream',\n",
       "        'apical_label', 'basal_label', 'skeleton_vector_downstream_phi',\n",
       "        'skeleton_vector_downstream_theta', 'skeleton_vector_upstream_phi',\n",
       "        'skeleton_vector_upstream_theta', 'width_no_spine'], dtype=object),\n",
       " 'adjacency': array([[0, 1, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]]),\n",
       " 'feature_matrix': array([[ 3.71170715e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.43345470e+03,  0.00000000e+00,  5.91837642e+01,\n",
       "          5.91837642e+01,  0.00000000e+00,  1.00000000e+00,\n",
       "         -6.22637047e-01,  2.00297982e+00, -6.22637047e-01,\n",
       "          2.00297982e+00,  5.91837642e+01],\n",
       "        [ 2.99289539e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          6.76722467e+03,  0.00000000e+00,  4.71136497e+02,\n",
       "          8.69454592e+02,  0.00000000e+00,  1.00000000e+00,\n",
       "          3.01881171e+00,  1.61006762e+00, -2.79826200e+00,\n",
       "          2.06957198e+00,  7.32983276e+02],\n",
       "        [ 8.07215086e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          4.79074407e+03,  0.00000000e+00,  1.97902300e+02,\n",
       "          4.48326427e+02,  0.00000000e+00,  1.00000000e+00,\n",
       "          1.19580679e+00,  7.15696534e-01,  1.01784622e+00,\n",
       "          7.42017670e-01,  3.82383945e+02]]),\n",
       " 'label_name': None,\n",
       " 'graph_label': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = data_df[[\"cell_type_fine\"]].iloc[1].to_list()[0][0]\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating the Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- a) Getting Means and Std Dev for Normalization --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mesh_volume                         1.0\n",
       "n_spines                            1.0\n",
       "n_synapses_head                     1.0\n",
       "n_synapses_neck                     1.0\n",
       "n_synapses_post                     1.0\n",
       "n_synapses_pre                      1.0\n",
       "skeletal_length                     1.0\n",
       "total_spine_volume                  1.0\n",
       "width_upstream                      1.0\n",
       "width_downstream                    1.0\n",
       "apical_label                        1.0\n",
       "basal_label                         1.0\n",
       "skeleton_vector_downstream_phi      1.0\n",
       "skeleton_vector_downstream_theta    1.0\n",
       "skeleton_vector_upstream_phi        1.0\n",
       "skeleton_vector_upstream_theta      1.0\n",
       "width_no_spine                      1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_batch_df = pd.concat([nxio.feature_df_from_gnn_info(\n",
    "    k[0],\n",
    "    return_data_labels_split = False) for k in data_df[gnn_task].to_list()])\n",
    "\n",
    "if label_name is not None:\n",
    "    all_batch_df = all_batch_df[[k for k in \n",
    "            all_batch_df.columns if k not in nu.convert_to_array_like(label_name)]]\n",
    "else:\n",
    "    all_batch_df = all_batch_df\n",
    "    \n",
    "# will use these to normalize the data\n",
    "col_means = all_batch_df.mean(axis=0).to_numpy()\n",
    "col_stds = all_batch_df.std(axis=0).to_numpy()\n",
    "\n",
    "all_batch_df_norm = pu.normalize_df(all_batch_df,column_means=col_means,\n",
    "                                 column_stds = col_stds)\n",
    "all_batch_df_norm.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- b) Creating the Dataset Class --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1P': 1,\n",
       " '23P': 2,\n",
       " '4P': 3,\n",
       " '5P_IT': 4,\n",
       " '5P_NP': 5,\n",
       " '5P_PT': 6,\n",
       " '6CT': 7,\n",
       " '6P': 8,\n",
       " '6P_CT': 9,\n",
       " '6P_IT': 10,\n",
       " '6P_U': 11,\n",
       " 'BC': 12,\n",
       " 'BPC': 13,\n",
       " 'I targeting non_bpc': 14,\n",
       " 'IT_big_tuft': 15,\n",
       " 'IT_short': 16,\n",
       " 'IT_small_tuft': 17,\n",
       " 'Martinotti': 18,\n",
       " 'NGC': 19,\n",
       " 'Pvalb': 20,\n",
       " 'SST': 21,\n",
       " 'Unsure': 22,\n",
       " 'VIP': 23,\n",
       " 'WM_P': 24,\n",
       " 'cb1 basket': 25,\n",
       " 'chandelier': 26,\n",
       " 'l1vip': 27,\n",
       " 'ndnf+npy_': 28,\n",
       " 'ngfc': 29,\n",
       " 'prox targeting': 30,\n",
       " 'small basket': 31,\n",
       " None: 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- mapping of the labels to integers --\n",
    "total_labels,label_counts = np.unique((data_df.query(f\"{graph_label}=={graph_label}\")[\n",
    "    graph_label]).to_numpy(),return_counts = True)\n",
    "cell_type_map = {k:i+1 for i,k in enumerate(total_labels)}\n",
    "cell_type_map[None] = 0\n",
    "cell_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 0.25, 0.3 , 0.5 , 1.  , 0.8 , 1.  , 0.8 , 1.  , 0.8 ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cell_type_map\n",
    "cell_type_fine_classifier_weights = {\n",
    "'23P': 0.25,#1294\n",
    "'4P': 0.3,#890\n",
    "'5P_IT': 0.5,#465\n",
    "'6P': 0.8,#342\n",
    "'6P_IT': 0.8,#263\n",
    "'5P_PT': 0.8,#224\n",
    "}\n",
    "\n",
    "\n",
    "class_idx = np.array(list(cell_type_map.values()) )\n",
    "class_labels = np.array(list(cell_type_map.keys()) )\n",
    "weights = np.array([cell_type_fine_classifier_weights.get(k,1) for k in class_labels])\n",
    "weights = weights[np.argsort(class_idx)]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_data_from_gnn_info(\n",
    "    gnn_info,\n",
    "    y = None,\n",
    "    verbose = False,\n",
    "    normalize = True,\n",
    "    features_to_delete=None,\n",
    "    features_to_keep = None\n",
    "    ): \n",
    "    \"\"\"\n",
    "    Purpose: To convert our data format into pytorch Data object\n",
    "\n",
    "    Pseudocode: \n",
    "    1) Create the edgelist (turn into tensor)\n",
    "    2) Get the \n",
    "    \"\"\"\n",
    "    edgelist = torch.tensor(xu.edgelist_from_adjacency_matrix(\n",
    "        array = gnn_info[\"adjacency\"],\n",
    "        verbose = False,\n",
    "    ).T,dtype=torch.long)\n",
    "\n",
    "    x,y_raw = nxio.feature_df_from_gnn_info(\n",
    "        gnn_info,\n",
    "        return_data_labels_split = True)\n",
    "    if y is None:\n",
    "        y = y_raw\n",
    "        \n",
    "    if not type(y) == str:\n",
    "        y = None\n",
    "        \n",
    "    y_int = np.array(cell_type_map[y] ).reshape(1,-1)\n",
    "    \n",
    "    if normalize:\n",
    "        x = (x-col_means)/col_stds\n",
    "    \n",
    "    # --- keeping or not keeping sertain features\n",
    "    gnn_features = gnn_info[\"features\"]\n",
    "\n",
    "    keep_idx = np.arange(len(gnn_features))\n",
    "    if features_to_delete is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k not in features_to_delete])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER DELETE= {keep_idx}\")\n",
    "    if features_to_keep is not None:\n",
    "        curr_idx = np.array([i for i,k in enumerate(gnn_features)\n",
    "                       if k in features_to_keep])\n",
    "        keep_idx = np.intersect1d(keep_idx,curr_idx)\n",
    "        if verbose:\n",
    "            print(f\"keep_idx AFTER KEEP = {keep_idx}\")\n",
    "\n",
    "    x = x[:,keep_idx]\n",
    "\n",
    "    x = torch.tensor(x,dtype=torch.float)\n",
    "    y = torch.tensor(y_int,dtype=torch.long)\n",
    "    \n",
    "    if len(y) > 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "        \n",
    "    if y.shape[0] != 1 or y.shape[1] != 1:\n",
    "        raise Exception(f\"y = {y}\")\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"x.shape = {x.shape},y.shape ={y.shape}\")\n",
    "\n",
    "    data = Data(x=x,y=y,edge_index=edgelist)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellTypeDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        #return ['some_file_1', 'some_file_2', ...]\n",
    "        return [str(data_filepath.absolute())]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    # def download(self):\n",
    "    #     # Download to `self.raw_dir`.\n",
    "    #     download_url(url, self.raw_dir)\n",
    "    #     ...\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        #data_list = [...]\n",
    "\n",
    "#         if data_df is None:\n",
    "#             data_df = su.decompress_pickle(self.raw_file_names[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        data_list = []\n",
    "        for k,y in tqdm(zip(\n",
    "            data_df[gnn_task].to_list(),\n",
    "            data_df[graph_label].to_list())):\n",
    "            \n",
    "            data_list.append(pytorch_data_from_gnn_info(\n",
    "                k[0],\n",
    "                y=y,\n",
    "                features_to_delete=features_to_delete,\n",
    "                features_to_keep = features_to_keep,\n",
    "                verbose = False))\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list_final = []\n",
    "            for data in data_list:\n",
    "                try:\n",
    "                    if self.pre_filter(data):\n",
    "                        data_list_final.append(data)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}\")\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list_final = []\n",
    "            for j,data in enumerate(data_list):\n",
    "                try:\n",
    "                    curr_t = self.pre_transform(data)\n",
    "                    if curr_t.y.shape[0] != 1 or curr_t.y.shape[1] != 1:\n",
    "                        raise Exception(f\"{j}, data = {curr_t}\")\n",
    "                    data_list_final.append(curr_t)\n",
    "                except:\n",
    "                    continue\n",
    "            data_list = data_list_final\n",
    "            \n",
    "        for j,d in enumerate(data_list):\n",
    "            if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "                raise Exception(f\"{j}, data = {d}\")\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_skeleton:\n",
    "    gnn_task_name = f\"{gnn_task}_with_skeleton\"\n",
    "else:\n",
    "    gnn_task_name = f\"{gnn_task}\"\n",
    "\n",
    "if dense_adj:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}\")#_processed_dense\")\n",
    "elif directed:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_directed\")#_processed_dense\")\n",
    "else:\n",
    "    processed_data_folder = data_path / Path(f\"{gnn_task_name}_no_dense\")#_processed_dense\")\n",
    "    \n",
    "# try:\n",
    "#     su.rm_dir(processed_data_folder)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "processed_data_folder.mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1f457bfded4c06ae7c37890823fc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_nodes = np.max(all_batch_df_norm.index.to_numpy()) + 1\n",
    "\n",
    "class MyFilter(object):\n",
    "    def __call__(self, data):\n",
    "        return data.num_nodes <= max_nodes\n",
    "    \n",
    "if dense_adj:\n",
    "    #gets the maximum number of nodes in any of the graphs\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "        T.ToDense(max_nodes),\n",
    "        #transforms.NormalizeFeatures(),\n",
    "    ]\n",
    "    pre_filter = MyFilter()\n",
    "elif directed:\n",
    "    transform_list = []\n",
    "    pre_filter = None\n",
    "else:\n",
    "    transform_list = [\n",
    "        transforms.ToUndirected(),\n",
    "    ]\n",
    "    \n",
    "    pre_filter = None\n",
    "    \n",
    "\n",
    "transform_norm = transforms.Compose(transform_list)\n",
    "dataset = CellTypeDataset(\n",
    "        processed_data_folder.absolute(),\n",
    "        pre_transform = transform_norm,\n",
    "        pre_filter = pre_filter,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,d in enumerate(dataset):\n",
    "    if d.y.shape[0] != 1 or d.y.shape[1] != 1:\n",
    "        raise Exception(f\"{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num_node_features = dataset.num_node_features\n",
    "dataset_num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the dataset\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset_num_node_features}')\n",
    "print(f'Number of classes: {dataset_num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "# print(f'Has self-loops: {data.has_self_loops()}')\n",
    "# print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- c) Splitting the Data into Labeled and unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_mask = np.array(\n",
    "    [True if k.y[0][0] > 0 else False for k in dataset]\n",
    ").astype('int')\n",
    "dataset_labeled = dataset[np.where(labeled_mask)[0]]\n",
    "len(dataset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_curr = dataset_labeled\n",
    "torch.manual_seed(12345)\n",
    "dataset_curr = dataset_curr.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- d) Split Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: To turn percentages into raw lengths\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "(train_dataset,\n",
    "val_dataset,\n",
    "test_dataset,) = pret.train_val_test_split(\n",
    "    dataset_curr,\n",
    "    return_dict=False,\n",
    "    verbose = True)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "print(f'Number of val graphs: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if wanted to try and add weights\n",
    "# y_train = np.array([int(data.y[0][0].numpy()) for data in train_dataset])\n",
    "# y_train_classes,y_train_count = np.unique(y_train,return_counts = True)\n",
    "# y_train_classes,y_train_count\n",
    "\n",
    "# sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "# weight = {i:1/}\n",
    "# samples_weight = np.array([weight[t] for t in y_train])\n",
    "# samples_weight = torch.from_numpy(samples_weight)\n",
    "# samples_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dense_adj:\n",
    "    data_loader_mod = DenseDataLoader\n",
    "else:\n",
    "    data_loader_mod = DataLoader\n",
    "\n",
    "\n",
    "train_loader = data_loader_mod(train_dataset, batch_size=batch_size,shuffle = True)\n",
    "test_loader = data_loader_mod(test_dataset, batch_size=batch_size,shuffle=False)\n",
    "val_loader = data_loader_mod(val_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3a; Picking the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GAT\"\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "import general_utils as gu\n",
    "architecture_kwargs_global = dict(\n",
    "    n_hidden_channels = 32, \n",
    "    #n_hidden_channels=64, \n",
    "    #first_heads=8, \n",
    "    #output_heads=1, \n",
    "    #dropout=0.6,\n",
    "    global_pool_type=\"mean\",\n",
    "    n_layers = 2,\n",
    "    heads = 3\n",
    ")\n",
    "\n",
    "optimizer_kwargs_global = dict(\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "all_run_kwargs = [\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 8))\n",
    "    dict(),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 64),),\n",
    "    dict(architecture_kwargs = dict(n_hidden_channels = 128),),\n",
    "    \n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 32),),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 64),),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 128),),\n",
    "    dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 8))\n",
    "    \n",
    "    \n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 32,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 16,global_pool_type=\"add\"),),\n",
    "#     dict(architecture_kwargs = dict(n_layers = 1,n_hidden_channels = 8,global_pool_type=\"add\"),),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_utils as evu\n",
    "import torch.nn.functional as F\n",
    "import model_utils as mdlu\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j,config_dict in enumerate(all_run_kwargs):\n",
    "    \n",
    "    architecture_kwargs = config_dict.get(\"architecture_kwargs\",dict())\n",
    "    optimizer_kwargs = config_dict.get(\"optimizer_kwargs\",dict())\n",
    "    \n",
    "    architecture_kwargs = gu.merge_dicts([architecture_kwargs_global.copy(),architecture_kwargs])\n",
    "    optimizer_kwargs = gu.merge_dicts([optimizer_kwargs_global.copy(),optimizer_kwargs])\n",
    "    \n",
    "    run_kwargs = gu.merge_dicts([architecture_kwargs,optimizer_kwargs])\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\\n\\n****------ Running Model Config {j} with following parameters ------****\\n{run_kwargs}\")\n",
    "\n",
    "    model = getattr(gm,model_name)(\n",
    "        dataset_num_node_features=dataset_num_node_features,\n",
    "        dataset_num_classes=dataset_num_classes,\n",
    "        **architecture_kwargs\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **optimizer_kwargs)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    \n",
    "    # ---------------- Configuring the Tensorboard and Checkpoinns--------------------\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    tensorboard_dir = Path(\"./tensorboard\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "    tensorboard_dir = tensorboard_dir / Path(f\"{model_name}\")\n",
    "    tensorboard_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    tensorboard_file_name = (f'{model_name}_' \n",
    "                             + \"_\".join([f\"{k}_{v}\" for k,v in run_kwargs.items()]))\n",
    "    tensorboard_file_name += f\"_with_skeleton_{with_skeleton}\"\n",
    "    print(f\"tensorboard_file_name = {tensorboard_file_name}\")\n",
    "    tensorboard_file = tensorboard_dir / Path(f'{tensorboard_file_name}')\n",
    "    try:\n",
    "        su.rm_dir(tensorboard_file)\n",
    "    except:\n",
    "        pass\n",
    "    tensorboard_file.mkdir(exist_ok = True)\n",
    "\n",
    "\n",
    "    #-- when to save a checkpoint of the model\n",
    "    checkpoint_dir = Path(\"./model_checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_dir = checkpoint_dir / Path(f\"{model_name}\")\n",
    "    checkpoint_dir.mkdir(exist_ok = True)\n",
    "    checkpoint_path = checkpoint_dir / Path(f\"./{tensorboard_file_name}_checkpoints\")\n",
    "\n",
    "    try:\n",
    "        su.rm_dir(checkpoint_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    checkpoint_path.mkdir(exist_ok = True)\n",
    "    n_epoch_for_checkpoint = 5\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_file)\n",
    "    \n",
    "\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    # weights = weight#[0.1,0.5,,1,0.7,1,1,1]\n",
    "    # class_weights = None\n",
    "    class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "    tensor_map = None\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:#train_loader:  # Iterate in batches over the training dataset.\n",
    "            #print(f\"data = {data}\")\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPool\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "            y_true = data.y.squeeze_()\n",
    "            #print(f\"out.shape = {out.shape}, data.y.shape = {data.y.shape}\")\n",
    "            loss = F.nll_loss(\n",
    "                torch.log(out), y_true,\n",
    "                weight = class_weights,\n",
    "            )  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            return loss\n",
    "\n",
    "\n",
    "    def test(loader,verbose = False):\n",
    "        model.eval()\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            if model_name == \"DiffPool\":\n",
    "                out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "                #y_true = data.y.reshape(-1,3)\n",
    "            elif model_name == \"TreeLSTM\":\n",
    "                n = data.x.shape[0]\n",
    "                h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "                out = model(\n",
    "                    data,\n",
    "                    h = h,\n",
    "                    c = c,\n",
    "                    embeddings = data.x\n",
    "                    )\n",
    "            else:\n",
    "                out = model(data)\n",
    "\n",
    "            y_pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            y_true = data.y.squeeze_()\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y_true)\n",
    "    #         error_idx = np.where(pred > 0)[0]\n",
    "    #         if len(error_idx) > 0:\n",
    "    #             print(f\"error_idx = {error_idx}\")\n",
    "        y_pred = torch.cat(y_pred_list)\n",
    "        y_true = torch.cat(y_true_list)\n",
    "\n",
    "        return evu.metric_dict(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            tensor_map=tensor_map,\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "    log_to_tensorboard = True\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss = train()\n",
    "        writer.add_scalar('loss',loss,epoch) # new line\n",
    "        train_metric_dict = test(train_loader)#train_loader)\n",
    "        val_metric_dict = test(val_loader)#test_loader)\n",
    "\n",
    "        if epoch % n_epoch_for_checkpoint == 0 and epoch != 0:\n",
    "            val_acc = val_metric_dict['accuracy'].numpy()\n",
    "            checkpoitn_filepath = checkpoint_path / Path(f\"{tensorboard_file_name}_epoch_{epoch}\")#_val_acc_{val_acc:.2f}\")\n",
    "            print(f\"Saving off checkpoint {checkpoitn_filepath}\")\n",
    "            mdlu.save_checkpoint(model,filepath = checkpoitn_filepath,epoch = epoch,loss = loss)\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, loss = {loss}')\n",
    "        for type_name,metric_dict in zip([\"train\",\"val\"],[train_metric_dict,val_metric_dict]):\n",
    "            print_log = f\"   {type_name} metrics: \"\n",
    "            for k,v in metric_dict.items():\n",
    "                if log_to_tensorboard:\n",
    "                    writer.add_scalar(f'{type_name}_{k}',v,epoch)\n",
    "                print_log += f\" {k}: {v:4f},\"\n",
    "\n",
    "            print(print_log)\n",
    "        \n",
    "        if val_metric_dict[\"accuracy\"] < 0.0001:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /pytorch_tools/Applications/Cell_Types_GNN/tensorboard --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard_utils as tbu\n",
    "# df_board = df_tensorboard(\"./tensorboard/\",verbose = True)\n",
    "# df_board.query(\"(run=='DiffPooln_hidden_channels_32') and (name=='train_accuracy')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(checkpoint_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winning_name = \"GCNFlat_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True\"\n",
    "epoch = 95\n",
    "winning_dir = checkpoint_dir / Path(f\"{winning_name}_checkpoints\") \n",
    "winning_filepath = winning_dir / Path(f\"{winning_name}_epoch_{epoch}\")\n",
    "winning_filepath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN_n_hidden_channels_64_global_pool_type_mean_n_layers_2_lr_0.01_with_skeleton_True_epoch_95 #good one for seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Embedding for all cell types (Can Run in Batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_kwargs_curr = dict(n_hidden_channels = 64,global_pool_type = \"mean\",n_layers = 2)\n",
    "architecture_kwargs = gu.merge_dicts([architecture_kwargs_global,architecture_kwargs_curr])\n",
    "architecture_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(gm,model_name)(\n",
    "    dataset_num_node_features=dataset_num_node_features,\n",
    "    dataset_num_classes=dataset_num_classes,\n",
    "    **architecture_kwargs,\n",
    "    #use_bn=False\n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(winning_filepath)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_loader = data_loader_mod(dataset, batch_size=batch_size,shuffle = False)\n",
    "all_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "embeddings = []\n",
    "labels = []\n",
    "for data in tqdm(all_data_loader):#train_loader:  # Iterate in batches over the training dataset.\n",
    "    data = data.to(device)\n",
    "    if model_name == \"DiffPool\":\n",
    "            out,gnn_loss, cluster_loss = model(data)  # Perform a single forward pass.\n",
    "            #y_true = data.y.reshape(-1,3)\n",
    "    elif model_name == \"TreeLSTM\":\n",
    "        n = data.x.shape[0]\n",
    "        h = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        c = torch.zeros((n, architecture_kwargs[\"n_hidden_channels\"]))\n",
    "        out = model(\n",
    "            data,\n",
    "            h = h,\n",
    "            c = c,\n",
    "            embeddings = data.x\n",
    "            )\n",
    "    else:\n",
    "        out = model(data)\n",
    "\n",
    "    out_array = out.detach().cpu().numpy()\n",
    "    out_labels = data.y.numpy().reshape(-1)\n",
    "    #print(f\"out_array.shape = {out_array.shape}, out_labels.shape = {out_labels.shape}\")\n",
    "    \n",
    "#     if out_array.shape[0] != out_labels.shape[0]:\n",
    "#         raise Exception(\"\")\n",
    "    \n",
    "    embeddings.append(out_array)\n",
    "    labels.append(out_labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "embedding_df[\"cell_type\"] = labels\n",
    "\n",
    "import general_utils as gu\n",
    "decoder_map = dict([(v,k) if k is not None else (v,\"Unknown\") for k,v in cell_type_map.items()])\n",
    "\n",
    "import pandas_utils as pu\n",
    "embedding_df[\"cell_type\"] = pu.new_column_from_dict_mapping(embedding_df,decoder_map,column_name = \"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(embedding_df[\"cell_type\"].to_numpy(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_idx = embedding_df.query(\"cell_type != 'Unknown'\").index.to_numpy()\n",
    "labeled_mask_plotting = np.zeros(len(embedding_df))\n",
    "labeled_mask_plotting[labeled_idx] = 1\n",
    "labeled_mask_plotting = labeled_mask_plotting.astype(\"bool\")\n",
    "labeled_mask_plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Plotting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import visualizations_ml as vml\n",
    "n_components = 3\n",
    "import dimensionality_reduction_ml as dru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ml as pdml\n",
    "X_data,y_labels = pdml.X_y(embedding_df,\"cell_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data[labeled_mask_plotting].to_numpy().astype(\"float\")\n",
    "y = y_labels[labeled_mask_plotting].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep) PCA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_components_test=10\n",
    "pca_data = dru.pca_analysis(\n",
    "    X,\n",
    "    n_components=n_components_test,\n",
    "    plot_sqrt_eigvals=False,\n",
    "    plot_perc_variance_explained=True\n",
    ")\n",
    "\n",
    "X_pca = pca_data[\"data_proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/meshAfterParty/meshAfterParty/')\n",
    "import datajoint_utils as du\n",
    "import cell_type_utils as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "import cell_type_utils as ctu\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"pca\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"umap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"umap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"isomap\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =3,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimensionality_reduction_ml as dru\n",
    "dru.dimensionality_reduction_by_method(\n",
    "    method=\"tsne\",\n",
    "    X=X_pca[y!= \"Unknown\"],\n",
    "    y = y[y != \"Unknown\"],\n",
    "    n_components =2,\n",
    "    plot=True,\n",
    "    plot_kwargs=dict(\n",
    "    target_to_color = ctu.cell_type_fine_color_map,\n",
    "        ndim = 3,\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
