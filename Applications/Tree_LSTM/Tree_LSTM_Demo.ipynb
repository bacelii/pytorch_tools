{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPurpose: To Practice implementation of \\nDGL Tree LSTM models\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: To Practice implementation of \n",
    "DGL Tree LSTM models\n",
    "\n",
    "Documentation: https://docs.dgl.ai/tutorials/models/2_small_graph/3_tree-lstm.html?highlight=treelstm\n",
    "Github: https://github.com/dmlc/dgl/tree/master/examples/pytorch/tree_lstm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dgl.data.tree import SSTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\"sst\", num_graphs=8544, save_path=/root/.dgl/sst)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_data = SSTDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vocabs = 19536\n",
      "num_classes = 5\n"
     ]
    }
   ],
   "source": [
    "# Each sample in the dataset is a constituency tree. The leaf nodes\n",
    "# represent words. The word is an int value stored in the \"x\" field.\n",
    "# The non-leaf nodes have a special word PAD_WORD. The sentiment\n",
    "# label is stored in the \"y\" feature field.\n",
    "trainset = SSTDataset(mode='tiny')  # the \"tiny\" set has only five trees\n",
    "tiny_sst = trainset.trees\n",
    "num_vocabs = trainset.num_vocabs\n",
    "num_classes = trainset.num_classes\n",
    "\n",
    "if verbose:\n",
    "    print(f\"num_vocabs = {num_vocabs}\")\n",
    "    print(f\"num_classes = {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . "
     ]
    }
   ],
   "source": [
    "vocab = trainset.vocab # vocabulary dict: key -> id\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # inverted vocabulary dict: id -> word\n",
    "\n",
    "a_tree = tiny_sst[0]\n",
    "for token in a_tree.ndata['x'].tolist():\n",
    "    if token != trainset.PAD_WORD:\n",
    "        print(inv_vocab[token], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1,  0,  1, -1, -1,  2, -1,  3, -1, -1, -1, -1,  4, -1,  5, -1,  0,\n",
       "        -1,  6, -1, -1,  7,  8, -1,  9, -1, 10, 11, 12, 13, -1, 14, -1, 15, -1,\n",
       "         8, -1, 16, -1,  4, -1, -1, 17, -1, -1, 18, 19, -1, 20, 21, -1, 22, -1,\n",
       "        -1, -1, -1, -1, 23, 24, 25, -1, 26, -1, 27, 28, 29, -1, 30, 31, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tree.ndata[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mask': tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]), 'x': tensor([-1, -1,  0,  1, -1, -1,  2, -1,  3, -1, -1, -1, -1,  4, -1,  5, -1,  0,\n",
       "        -1,  6, -1, -1,  7,  8, -1,  9, -1, 10, 11, 12, 13, -1, 14, -1, 15, -1,\n",
       "         8, -1, 16, -1,  4, -1, -1, 17, -1, -1, 18, 19, -1, 20, 21, -1, 22, -1,\n",
       "        -1, -1, -1, -1, 23, 24, 25, -1, 26, -1, 27, 28, 29, -1, 30, 31, 32]), 'y': tensor([3, 2, 2, 2, 4, 3, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 4, 3, 2, 3, 3, 2, 3,\n",
       "        2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_sst[0].ndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Batching is done by just combining the disconnected trees into one tree and doing message passing on that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "import dgl\n",
    "\n",
    "class TreeLSTMCell(nn.Module):\n",
    "    def __init__(self, x_size, h_size):\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "        self.W_iou = nn.Linear(x_size, 3 * h_size, bias=False)\n",
    "        self.U_iou = nn.Linear(2 * h_size, 3 * h_size, bias=False)\n",
    "        self.b_iou = nn.Parameter(th.zeros(1, 3 * h_size))\n",
    "        self.U_f = nn.Linear(2 * h_size, 2 * h_size)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'h': edges.src['h'], 'c': edges.src['c']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # concatenate h_jl for equation (1), (2), (3), (4)\n",
    "        h_cat = nodes.mailbox['h'].view(nodes.mailbox['h'].size(0), -1)\n",
    "        # equation (2)\n",
    "        f = th.sigmoid(self.U_f(h_cat)).view(*nodes.mailbox['h'].size())\n",
    "        # second term of equation (5)\n",
    "        c = th.sum(f * nodes.mailbox['c'], 1)\n",
    "        return {'iou': self.U_iou(h_cat), 'c': c}\n",
    "\n",
    "    def apply_node_func(self, nodes):\n",
    "        # equation (1), (3), (4)\n",
    "        iou = nodes.data['iou'] + self.b_iou\n",
    "        i, o, u = th.chunk(iou, 3, 1)\n",
    "        i, o, u = th.sigmoid(i), th.sigmoid(o), th.tanh(u)\n",
    "        # equation (5)\n",
    "        c = i * u + nodes.data['c']\n",
    "        # equation (6)\n",
    "        h = o * th.tanh(c)\n",
    "        return {'h' : h, 'c' : c}\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "\n",
    "class TreeLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        #num_vocabs,\n",
    "        dataset_num_node_features,\n",
    "        dataset_num_classes,\n",
    "        #h_size,\n",
    "        n_hidden_channels=64,\n",
    "        dropout=0.5,\n",
    "        ):\n",
    "        \n",
    "        super(TreeLSTM, self).__init__()\n",
    "        #self.x_size = x_size\n",
    "        #self.embedding = nn.Embedding(num_vocabs, x_size)\n",
    "#         if pretrained_emb is not None:\n",
    "#             print('Using glove')\n",
    "#             self.embedding.weight.data.copy_(pretrained_emb)\n",
    "#             self.embedding.weight.requires_grad = True\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(\n",
    "            n_hidden_channels, \n",
    "            dataset_num_classes)\n",
    "        self.cell = TreeLSTMCell(\n",
    "            dataset_num_node_features,\n",
    "            n_hidden_channels)\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        batch,\n",
    "        h,\n",
    "        c,\n",
    "        embeddings):\n",
    "        \n",
    "        \"\"\"Compute tree-lstm prediction given a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : dgl.data.SSTBatch\n",
    "            The data batch.\n",
    "        h : Tensor\n",
    "            Initial hidden state.\n",
    "        c : Tensor\n",
    "            Initial cell state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : Tensor\n",
    "            The prediction of each node.\n",
    "        \"\"\"\n",
    "        g = batch.graph\n",
    "        # to heterogenous graph\n",
    "        g = dgl.graph(g.edges())\n",
    "        # feed embedding\n",
    "        #embeds = self.embedding(batch.wordid * batch.mask)\n",
    "        embeds = embeddings\n",
    "        g.ndata['iou'] = self.cell.W_iou(self.dropout(embeds))# * batch.mask.float().unsqueeze(-1)\n",
    "        g.ndata['h'] = h\n",
    "        g.ndata['c'] = c\n",
    "        # propagate\n",
    "        dgl.prop_nodes_topo(g,\n",
    "                            message_func=self.cell.message_func,\n",
    "                            reduce_func=self.cell.reduce_func,\n",
    "                            apply_node_func=self.cell.apply_node_func)\n",
    "        # compute logits\n",
    "        h = self.dropout(g.ndata.pop('h'))\n",
    "        return h\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        batch,\n",
    "        h,\n",
    "        c,\n",
    "        embeddings):\n",
    "        \n",
    "        h = self.encode(self,\n",
    "        batch=batch,\n",
    "        h=h,\n",
    "        c=c,\n",
    "        embeddings=embeddings)\n",
    "        \n",
    "        logits = self.linear(h)\n",
    "        return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vocabs,\n",
    "x_size,\n",
    "h_size,\n",
    "num_classes,\n",
    "dropout,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTM(\n",
      "  (embedding): Embedding(19536, 256)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (cell): TreeLSTMCell(\n",
      "    (W_iou): Linear(in_features=256, out_features=768, bias=False)\n",
      "    (U_iou): Linear(in_features=512, out_features=768, bias=False)\n",
      "    (U_f): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/dgl/core.py:79: DGLWarning: The input graph for the user-defined edge function does not contain valid edges\n",
      "  dgl_warning('The input graph for the user-defined edge function ' \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Step 00000 | Loss 443.8172 | Acc 0.1575 |\n",
      "Epoch 00001 | Step 00000 | Loss 274.1146 | Acc 0.7216 |\n",
      "Epoch 00002 | Step 00000 | Loss 327.7868 | Acc 0.6081 |\n",
      "Epoch 00003 | Step 00000 | Loss 493.5061 | Acc 0.7839 |\n",
      "Epoch 00004 | Step 00000 | Loss 427.5103 | Acc 0.6300 |\n",
      "Epoch 00005 | Step 00000 | Loss 205.8990 | Acc 0.8242 |\n",
      "Epoch 00006 | Step 00000 | Loss 105.0090 | Acc 0.8864 |\n",
      "Epoch 00007 | Step 00000 | Loss 93.6178 | Acc 0.8938 |\n",
      "Epoch 00008 | Step 00000 | Loss 62.3927 | Acc 0.9414 |\n",
      "Epoch 00009 | Step 00000 | Loss 54.9089 | Acc 0.9341 |\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SSTBatch = namedtuple('SSTBatch', ['graph', 'mask', 'wordid', 'label'])\n",
    "\n",
    "device = th.device('cpu')\n",
    "# hyper parameters\n",
    "x_size = 256\n",
    "h_size = 256\n",
    "dropout = 0.5\n",
    "lr = 0.05\n",
    "weight_decay = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "# create the model\n",
    "model = TreeLSTM(trainset.num_vocabs,\n",
    "                 x_size,\n",
    "                 h_size,\n",
    "                 trainset.num_classes,\n",
    "                 dropout)\n",
    "print(model)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = th.optim.Adagrad(model.parameters(),\n",
    "                          lr=lr,\n",
    "                          weight_decay=weight_decay)\n",
    "\n",
    "def batcher(dev):\n",
    "    def batcher_dev(batch):\n",
    "        batch_trees = dgl.batch(batch)\n",
    "        return SSTBatch(graph=batch_trees,\n",
    "                        mask=batch_trees.ndata['mask'].to(device),\n",
    "                        wordid=batch_trees.ndata['x'].to(device),\n",
    "                        label=batch_trees.ndata['y'].to(device))\n",
    "    return batcher_dev\n",
    "\n",
    "train_loader = DataLoader(dataset=tiny_sst,\n",
    "                          batch_size=5,\n",
    "                          collate_fn=batcher(device),\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        g = batch.graph\n",
    "        n = g.number_of_nodes()\n",
    "        h = th.zeros((n, h_size))\n",
    "        c = th.zeros((n, h_size))\n",
    "        logits = model(batch, h, c)\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        loss = F.nll_loss(logp, batch.label, reduction='sum')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = th.argmax(logits, 1)\n",
    "        acc = float(th.sum(th.eq(batch.label, pred))) / len(batch.label)\n",
    "        print(\"Epoch {:05d} | Step {:05d} | Loss {:.4f} | Acc {:.4f} |\".format(\n",
    "            epoch, step, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([273])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-96e89ba817df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
