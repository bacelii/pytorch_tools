1) For dealing with large datasets that have small number of training sets: Can do following process
    a. train on the small datasets
    b. get the predicted labels for all datapoints
    c. Train again using the datapoints that are high confidence
    
2) Rerunning a training with random initialization can really help with overfitting

    a. Monitoring the train/validation split early could help to monitor whether the overfitting will 
    happend soon or not
    
3) If you have more data then you could potentially increase the batch size and thus
increase the learning rate

- too small of a batch size on a large dataset may make the learner jump around too much 

batch rate observations: 
1) Increasing the batch size helped with overfitting (train accuracy running away from validation)
2) As got more data decreasing the batch size seemed to help the loss function from not fluctuating too much


The way the proces goes:
- runs batch
- calculates loss
- updates weights
- next batch
.....  can calculate the average loss over the whole training process

At end of the epoch calculates the loss, accuracy,
so the greater learning rate or the smaller the batch, the farther
the average loss of the training process is farther from the loss of the end of epoch


ignore above:
The big problem was that i was using a standardization method from previous dataset and
some data in the new dataset had values that were much larger (and probably dominating the loss):
-- ALWAYS CHECK THAT THE NORMALZIATION DATAFRAMES ARE SIMILAR ---
